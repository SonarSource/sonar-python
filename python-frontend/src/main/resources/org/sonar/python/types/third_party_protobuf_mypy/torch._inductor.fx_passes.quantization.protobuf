
&torch._inductor.fx_passes.quantization®
UnaryAttr@7104torch._inductor.fx_passes.quantization.UnaryAttr@710"builtins.object*²
__init__=torch._inductor.fx_passes.quantization.UnaryAttr@710.__init__"
None*v
selfl
4torch._inductor.fx_passes.quantization.UnaryAttr@710"4torch._inductor.fx_passes.quantization.UnaryAttr@710*)
op_name
builtins.str"builtins.str*
scalars_attr
Any *
algorithm_attr
Any re
op_name<torch._inductor.fx_passes.quantization.UnaryAttr@710.op_name
builtins.str"builtins.strrZ
scalars_attrAtorch._inductor.fx_passes.quantization.UnaryAttr@710.scalars_attr
Anyr^
algorithm_attrCtorch._inductor.fx_passes.quantization.UnaryAttr@710.algorithm_attr
AnyŠ
BinaryUnaryAttr@899:torch._inductor.fx_passes.quantization.BinaryUnaryAttr@899"builtins.object*•
__init__Ctorch._inductor.fx_passes.quantization.BinaryUnaryAttr@899.__init__"
None*‚
selfx
:torch._inductor.fx_passes.quantization.BinaryUnaryAttr@899":torch._inductor.fx_passes.quantization.BinaryUnaryAttr@899*0
binary_op_name
builtins.str"builtins.str*
alpha
Any *1
unary_op_name
builtins.str"builtins.str *
scalars_attr
Any *
algorithm_attr
Any ry
binary_op_nameItorch._inductor.fx_passes.quantization.BinaryUnaryAttr@899.binary_op_name
builtins.str"builtins.strrR
alpha@torch._inductor.fx_passes.quantization.BinaryUnaryAttr@899.alpha
Anyrw
unary_op_nameHtorch._inductor.fx_passes.quantization.BinaryUnaryAttr@899.unary_op_name
builtins.str"builtins.strr`
scalars_attrGtorch._inductor.fx_passes.quantization.BinaryUnaryAttr@899.scalars_attr
Anyrd
algorithm_attrItorch._inductor.fx_passes.quantization.BinaryUnaryAttr@899.algorithm_attr
AnyÁ
_get_pattern_output_dtype@torch._inductor.fx_passes.quantization._get_pattern_output_dtype"
Any*Y
matchN
%torch._inductor.pattern_matcher.Match"%torch._inductor.pattern_matcher.Match¼
(_may_generate_pattern_with_dtype_convertOtorch._inductor.fx_passes.quantization._may_generate_pattern_with_dtype_convert*
pattern*
dtype *
with_dtype_convert *
users ¤
"_may_generate_pattern_with_reshapeItorch._inductor.fx_passes.quantization._may_generate_pattern_with_reshape*
pattern*
reshape_size *
with_reshape Œ
_generate_linear_t_patternAtorch._inductor.fx_passes.quantization._generate_linear_t_pattern* 
_dequant_per_channel_pattern*	
dtypeŒ
_unary_fusion_pattern<torch._inductor.fx_passes.quantization._unary_fusion_pattern*
unary_fusion*
call_fn*	
users*
is_bf16
,get_dequantize_per_tensor_activation_patternStorch._inductor.fx_passes.quantization.get_dequantize_per_tensor_activation_pattern*
is_tensor_overload z
!get_dequantize_qconv_pt2e_patternHtorch._inductor.fx_passes.quantization.get_dequantize_qconv_pt2e_pattern*
users „
get_qlinear_pt2e_pattern?torch._inductor.fx_passes.quantization.get_qlinear_pt2e_pattern*
x_scale_zp_are_tensors*
users Î
generate_pattern_with_binaryCtorch._inductor.fx_passes.quantization.generate_pattern_with_binary*
binary_post_op*
computation_call*
extra_input_pattern*
dtype_convert *
swap_inputs Š
generate_pattern_with_unaryBtorch._inductor.fx_passes.quantization.generate_pattern_with_unary*
computation_call*
unary_post_opŸ
"generate_pattern_with_output_quantItorch._inductor.fx_passes.quantization.generate_pattern_with_output_quant*
computation_call*
with_dtype_convert ¥
_check_node_kwarg_arg_valueBtorch._inductor.fx_passes.quantization._check_node_kwarg_arg_value*

check_node*

kwarg_name*

args_index*
expected_value‰
/_is_valid_quantized_conv2d_optimization_patternVtorch._inductor.fx_passes.quantization._is_valid_quantized_conv2d_optimization_pattern¯
!_register_quantized_conv_loweringHtorch._inductor.fx_passes.quantization._register_quantized_conv_lowering*
pattern*
pass_number*
computation_op*

unary_attr‰
/_is_valid_quantized_linear_optimization_patternVtorch._inductor.fx_passes.quantization._is_valid_quantized_linear_optimization_pattern³
#_register_quantized_linear_loweringJtorch._inductor.fx_passes.quantization._register_quantized_linear_lowering*
pattern*
pass_number*
computation_op*

unary_attrÈ
*_register_quantized_linear_binary_loweringQtorch._inductor.fx_passes.quantization._register_quantized_linear_binary_lowering*
pattern*
pass_number*
computation_op*
binary_unary_attr
+_is_valid_qconv_binary_optimization_patternRtorch._inductor.fx_passes.quantization._is_valid_qconv_binary_optimization_pattern…
-_is_valid_qlinear_binary_optimization_patternTtorch._inductor.fx_passes.quantization._is_valid_qlinear_binary_optimization_pattern¸
2_is_valid_quantized_op_binary_optimization_patternYtorch._inductor.fx_passes.quantization._is_valid_quantized_op_binary_optimization_pattern*
qop*
extra_input_from_dequant Ä
(_register_quantized_conv_binary_loweringOtorch._inductor.fx_passes.quantization._register_quantized_conv_binary_lowering*
pattern*
pass_number*
computation_op*
binary_unary_attrq
#_register_quantization_unary_fusionJtorch._inductor.fx_passes.quantization._register_quantization_unary_fusions
$_register_quantization_binary_fusionKtorch._inductor.fx_passes.quantization._register_quantization_binary_fusion
2_is_valid_quantized_maxpool2d_optimization_patternYtorch._inductor.fx_passes.quantization._is_valid_quantized_maxpool2d_optimization_pattern˜
&_register_quantized_maxpool2d_loweringMtorch._inductor.fx_passes.quantization._register_quantized_maxpool2d_lowering*
pattern*
computation_opk
 _register_quantization_maxpool2dGtorch._inductor.fx_passes.quantization._register_quantization_maxpool2dw
_is_input_output_same_scale_zpEtorch._inductor.fx_passes.quantization._is_input_output_same_scale_zp*

check_nodeŒ
 _register_quantized_cat_loweringGtorch._inductor.fx_passes.quantization._register_quantized_cat_lowering*
pattern*
computation_op_
_register_quantization_catAtorch._inductor.fx_passes.quantization._register_quantization_cat”
$_register_quantized_reshape_loweringKtorch._inductor.fx_passes.quantization._register_quantized_reshape_lowering*
pattern*
computation_opg
_register_quantization_reshapeEtorch._inductor.fx_passes.quantization._register_quantization_reshapeo
"_is_valid_woq_optimization_patternItorch._inductor.fx_passes.quantization._is_valid_woq_optimization_pattern’
_register_woq_lowering=torch._inductor.fx_passes.quantization._register_woq_lowering*
pattern*
computation_woq*
computation_reshapeg
_register_woq_mm_int8_pattern1Etorch._inductor.fx_passes.quantization._register_woq_mm_int8_pattern1g
_register_woq_mm_int8_pattern2Etorch._inductor.fx_passes.quantization._register_woq_mm_int8_pattern2g
_register_woq_mm_int8_pattern3Etorch._inductor.fx_passes.quantization._register_woq_mm_int8_pattern3k
 _register_quantization_loweringsGtorch._inductor.fx_passes.quantization._register_quantization_loweringsY
_register_woq_lowerings>torch._inductor.fx_passes.quantization._register_woq_lowerings~
#_is_valid_dequant_promotion_patternJtorch._inductor.fx_passes.quantization._is_valid_dequant_promotion_pattern*
dtype –
 _register_dequant_promotion_passGtorch._inductor.fx_passes.quantization._register_dequant_promotion_pass*
pattern*
pass_number*
dtype v
 _is_valid_dequant_conv2d_patternGtorch._inductor.fx_passes.quantization._is_valid_dequant_conv2d_pattern*	
dtypeœ
#_register_qconv_weight_prepack_passJtorch._inductor.fx_passes.quantization._register_qconv_weight_prepack_pass*
pattern*
pass_number*
dtype ®
*_generate_dequant_convolution_node_patternQtorch._inductor.fx_passes.quantization._generate_dequant_convolution_node_pattern* 
_dequant_per_channel_pattern*
dtype †
'_generate_qconv_weight_prepack_patternsNtorch._inductor.fx_passes.quantization._generate_qconv_weight_prepack_patterns*
dtype ‡
_get_linear_node7torch._inductor.fx_passes.quantization._get_linear_node*	
match*
input_dim_exceeds_two*
input_contiguous¯
_get_linear_dq_node:torch._inductor.fx_passes.quantization._get_linear_dq_node*
linear_node*
input_index*	
dtype*
input_dim_exceeds_two*
input_contiguous§
 _is_valid_dequant_linear_patternGtorch._inductor.fx_passes.quantization._is_valid_dequant_linear_pattern*	
dtype*
input_dim_exceeds_two*
input_contiguousÕ
%_register_qlinear_weight_prepack_passLtorch._inductor.fx_passes.quantization._register_qlinear_weight_prepack_pass*
pattern*
pass_number*
dtype *
input_dim_exceeds_two *
input_contiguous Û
%_generate_dequant_linear_node_patternLtorch._inductor.fx_passes.quantization._generate_dequant_linear_node_pattern* 
_dequant_per_channel_pattern*
dtype *
input_dim_exceeds_two *
is_tensor_overload É
"_generate_dequant_bmm_node_patternItorch._inductor.fx_passes.quantization._generate_dequant_bmm_node_pattern* 
_dequant_per_channel_pattern*
dtype *
	with_bias *
is_tensor_overload ê
)_generate_qlinear_weight_prepack_patternsPtorch._inductor.fx_passes.quantization._generate_qlinear_weight_prepack_patterns*
dtype *
input_dim_exceeds_two *
input_contiguous *
	with_bias *
is_tensor_overload a
_register_dequant_promotionBtorch._inductor.fx_passes.quantization._register_dequant_promotiong
_register_qconv_weight_prepackEtorch._inductor.fx_passes.quantization._register_qconv_weight_prepackk
 _register_qlinear_weight_prepackGtorch._inductor.fx_passes.quantization._register_qlinear_weight_prepack¨
quant_lift_up4torch._inductor.fx_passes.quantization.quant_lift_up"
Any*X
graph_moduleF
!torch.fx.graph_module.GraphModule"!torch.fx.graph_module.GraphModule*¢
__annotations__6torch._inductor.fx_passes.quantization.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*ü
L"torch._inductor.lowering.loweringsÒ
Dbuiltins.dict[torch._ops.OpOverload,CallableType[builtins.function]].
torch._ops.OpOverload"torch._ops.OpOverloadK
CallableType[builtins.function]&
builtins.function"builtins.function"builtins.dict*<
aten+torch._inductor.fx_passes.quantization.aten
Any*>
prims,torch._inductor.fx_passes.quantization.prims
Any*\
quantized_decomposed;torch._inductor.fx_passes.quantization.quantized_decomposed
Any*F
	quantized0torch._inductor.fx_passes.quantization.quantized
Any*‰
_PER_TENSOR_QUANTIZE_OPS?torch._inductor.fx_passes.quantization._PER_TENSOR_QUANTIZE_OPS,
builtins.list[Any]
Any"builtins.list*k
	_VIEW_OPS0torch._inductor.fx_passes.quantization._VIEW_OPS,
builtins.list[Any]
Any"builtins.list*Ó
%dequantize_per_channel_weight_patternLtorch._inductor.fx_passes.quantization.dequantize_per_channel_weight_pattern\
,torch._inductor.pattern_matcher.CallFunction",torch._inductor.pattern_matcher.CallFunction*Ž
-dequantize_per_channel_to_bf16_weight_patternTtorch._inductor.fx_passes.quantization.dequantize_per_channel_to_bf16_weight_pattern
Any*ß
+dequantize_per_channel_clone_weight_patternRtorch._inductor.fx_passes.quantization.dequantize_per_channel_clone_weight_pattern\
,torch._inductor.pattern_matcher.CallFunction",torch._inductor.pattern_matcher.CallFunction*ï
3dequantize_per_channel_to_bf16_clone_weight_patternZtorch._inductor.fx_passes.quantization.dequantize_per_channel_to_bf16_clone_weight_pattern\
,torch._inductor.pattern_matcher.CallFunction",torch._inductor.pattern_matcher.CallFunction*¹
dequantize_accum_pattern?torch._inductor.fx_passes.quantization.dequantize_accum_pattern\
,torch._inductor.pattern_matcher.CallFunction",torch._inductor.pattern_matcher.CallFunction*ã
-_raw_dequantize_per_tensor_activation_patternTtorch._inductor.fx_passes.quantization._raw_dequantize_per_tensor_activation_pattern\
,torch._inductor.pattern_matcher.CallFunction",torch._inductor.pattern_matcher.CallFunction