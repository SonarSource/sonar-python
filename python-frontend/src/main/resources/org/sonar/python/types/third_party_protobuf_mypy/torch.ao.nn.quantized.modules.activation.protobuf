
(torch.ao.nn.quantized.modules.activationç
ReLU6.torch.ao.nn.quantized.modules.activation.ReLU6" torch.nn.modules.activation.ReLU*\
__init__7torch.ao.nn.quantized.modules.activation.ReLU6.__init__*
self*
inplace *V
forward6torch.ao.nn.quantized.modules.activation.ReLU6.forward*
self*	
input*O
	_get_name8torch.ao.nn.quantized.modules.activation.ReLU6._get_name*
self*„

from_float9torch.ao.nn.quantized.modules.activation.ReLU6.from_float*
mod* 
use_precomputed_fake_quant 0:staticmethodhÀ
	Hardswish2torch.ao.nn.quantized.modules.activation.Hardswish"%torch.nn.modules.activation.Hardswish*‡
__init__;torch.ao.nn.quantized.modules.activation.Hardswish.__init__*
self*	
scale*

zero_point*
device *
dtype *Z
forward:torch.ao.nn.quantized.modules.activation.Hardswish.forward*
self*	
input*S
	_get_name<torch.ao.nn.quantized.modules.activation.Hardswish._get_name*
self*ˆ

from_float=torch.ao.nn.quantized.modules.activation.Hardswish.from_float*
mod* 
use_precomputed_fake_quant 0:staticmethodh*‘
from_referenceAtorch.ao.nn.quantized.modules.activation.Hardswish.from_reference*
cls*
mod*	
scale*

zero_point0:classmethodp—
ELU,torch.ao.nn.quantized.modules.activation.ELU"torch.nn.modules.activation.ELU*s
__init__5torch.ao.nn.quantized.modules.activation.ELU.__init__*
self*	
scale*

zero_point*
alpha *T
forward4torch.ao.nn.quantized.modules.activation.ELU.forward*
self*	
input*M
	_get_name6torch.ao.nn.quantized.modules.activation.ELU._get_name*
self*‚

from_float7torch.ao.nn.quantized.modules.activation.ELU.from_float*
mod* 
use_precomputed_fake_quant 0:staticmethodh*‹
from_reference;torch.ao.nn.quantized.modules.activation.ELU.from_reference*
cls*
mod*	
scale*

zero_point0:classmethodprD
scale2torch.ao.nn.quantized.modules.activation.ELU.scale
AnyrN

zero_point7torch.ao.nn.quantized.modules.activation.ELU.zero_point
Anyõ
	LeakyReLU2torch.ao.nn.quantized.modules.activation.LeakyReLU"%torch.nn.modules.activation.LeakyReLU*´
__init__;torch.ao.nn.quantized.modules.activation.LeakyReLU.__init__"
None*r
selfh
2torch.ao.nn.quantized.modules.activation.LeakyReLU"2torch.ao.nn.quantized.modules.activation.LeakyReLU*+
scale 
builtins.float"builtins.float*,

zero_point
builtins.int"builtins.int*6
negative_slope 
builtins.float"builtins.float *-
inplace
builtins.bool"builtins.bool *
device
Any *
dtype
Any *Z
forward:torch.ao.nn.quantized.modules.activation.LeakyReLU.forward*
self*	
input*S
	_get_name<torch.ao.nn.quantized.modules.activation.LeakyReLU._get_name*
self*

from_float=torch.ao.nn.quantized.modules.activation.LeakyReLU.from_float*
cls*
mod* 
use_precomputed_fake_quant 0:classmethodp*‘
from_referenceAtorch.ao.nn.quantized.modules.activation.LeakyReLU.from_reference*
cls*
mod*	
scale*

zero_point0:classmethodpÞ
Sigmoid0torch.ao.nn.quantized.modules.activation.Sigmoid"#torch.nn.modules.activation.Sigmoid*¨
__init__9torch.ao.nn.quantized.modules.activation.Sigmoid.__init__"
None*n
selfd
0torch.ao.nn.quantized.modules.activation.Sigmoid"0torch.ao.nn.quantized.modules.activation.Sigmoid*2
output_scale 
builtins.float"builtins.float*3
output_zero_point
builtins.int"builtins.int*X
forward8torch.ao.nn.quantized.modules.activation.Sigmoid.forward*
self*	
input*Ž

from_float;torch.ao.nn.quantized.modules.activation.Sigmoid.from_float*
cls*
mod* 
use_precomputed_fake_quant 0:classmethodpro
output_scale=torch.ao.nn.quantized.modules.activation.Sigmoid.output_scale 
builtins.float"builtins.floatru
output_zero_pointBtorch.ao.nn.quantized.modules.activation.Sigmoid.output_zero_point
builtins.int"builtins.intÁ
Softmax0torch.ao.nn.quantized.modules.activation.Softmax"#torch.nn.modules.activation.Softmax*y
__init__9torch.ao.nn.quantized.modules.activation.Softmax.__init__*
self*	
dim *
scale *

zero_point *X
forward8torch.ao.nn.quantized.modules.activation.Softmax.forward*
self*	
input*Q
	_get_name:torch.ao.nn.quantized.modules.activation.Softmax._get_name*
self*†

from_float;torch.ao.nn.quantized.modules.activation.Softmax.from_float*
mod* 
use_precomputed_fake_quant 0:staticmethodh*
from_reference?torch.ao.nn.quantized.modules.activation.Softmax.from_reference*
cls*
mod*	
scale*

zero_point0:classmethodprH
scale6torch.ao.nn.quantized.modules.activation.Softmax.scale
AnyrR

zero_point;torch.ao.nn.quantized.modules.activation.Softmax.zero_point
Anyˆ
MultiheadAttention;torch.ao.nn.quantized.modules.activation.MultiheadAttention"=torch.ao.nn.quantizable.modules.activation.MultiheadAttention*\
	_get_nameEtorch.ao.nn.quantized.modules.activation.MultiheadAttention._get_name*
self*y

from_floatFtorch.ao.nn.quantized.modules.activation.MultiheadAttention.from_float*
cls*	
other0:classmethodp*
from_observedItorch.ao.nn.quantized.modules.activation.MultiheadAttention.from_observed*
cls*	
other0:classmethodpr›
_FLOAT_MODULEItorch.ao.nn.quantized.modules.activation.MultiheadAttention._FLOAT_MODULE?
CallableType[builtins.type]
builtins.type"builtins.typeÒ
PReLU.torch.ao.nn.quantized.modules.activation.PReLU"torch.nn.modules.module.Module*Ö
__init__7torch.ao.nn.quantized.modules.activation.PReLU.__init__"
None*j
self`
.torch.ao.nn.quantized.modules.activation.PReLU".torch.ao.nn.quantized.modules.activation.PReLU*2
output_scale 
builtins.float"builtins.float*3
output_zero_point
builtins.int"builtins.int*2
num_parameters
builtins.int"builtins.int *ò

set_weight9torch.ao.nn.quantized.modules.activation.PReLU.set_weight"
None*j
self`
.torch.ao.nn.quantized.modules.activation.PReLU".torch.ao.nn.quantized.modules.activation.PReLU*3
w,
torch._tensor.Tensor"torch._tensor.Tensor*”
forward6torch.ao.nn.quantized.modules.activation.PReLU.forward",
torch._tensor.Tensor"torch._tensor.Tensor*j
self`
.torch.ao.nn.quantized.modules.activation.PReLU".torch.ao.nn.quantized.modules.activation.PReLU*7
input,
torch._tensor.Tensor"torch._tensor.Tensor*O
	_get_name8torch.ao.nn.quantized.modules.activation.PReLU._get_name*
self*Œ

from_float9torch.ao.nn.quantized.modules.activation.PReLU.from_float*
cls*
mod* 
use_precomputed_fake_quant 0:classmethodp*
from_reference=torch.ao.nn.quantized.modules.activation.PReLU.from_reference*
cls*
mod*	
scale*

zero_point0:classmethodprm
num_parameters=torch.ao.nn.quantized.modules.activation.PReLU.num_parameters
builtins.int"builtins.intr_
scale4torch.ao.nn.quantized.modules.activation.PReLU.scale 
builtins.float"builtins.floatre

zero_point9torch.ao.nn.quantized.modules.activation.PReLU.zero_point
builtins.int"builtins.intrm
weight5torch.ao.nn.quantized.modules.activation.PReLU.weight,
torch._tensor.Tensor"torch._tensor.Tensor*¤
__annotations__8torch.ao.nn.quantized.modules.activation.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*‡
__all__0torch.ao.nn.quantized.modules.activation.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list