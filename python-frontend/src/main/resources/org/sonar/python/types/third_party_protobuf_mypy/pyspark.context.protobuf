
pyspark.context”Ï
SparkContextpyspark.context.SparkContext"builtins.object*É
__init__%pyspark.context.SparkContext.__init__"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*R
masterD
Union[builtins.str,None]
builtins.str"builtins.str
None *S
appNameD
Union[builtins.str,None]
builtins.str"builtins.str
None *U
	sparkHomeD
Union[builtins.str,None]
builtins.str"builtins.str
None *‘
pyFiles
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None *¦
environment’
+Union[builtins.dict[builtins.str,Any],None]W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict
None *-
	batchSize
builtins.int"builtins.int *R

serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer *n
confb
"Union[pyspark.conf.SparkConf,None]0
pyspark.conf.SparkConf"pyspark.conf.SparkConf
None *5
gateway&
Union[Any,None]
Any
None *1
jsc&
Union[Any,None]
Any
None *„
profiler_clsp
$Type[pyspark.profiler.BasicProfiler]@
pyspark.profiler.BasicProfiler"pyspark.profiler.BasicProfiler"type *‘
udf_profiler_clsy
'Type[pyspark.profiler.UDFBasicProfiler]F
!pyspark.profiler.UDFBasicProfiler"!pyspark.profiler.UDFBasicProfiler"type *Ž
memory_profiler_clss
%Type[pyspark.profiler.MemoryProfiler]B
pyspark.profiler.MemoryProfiler"pyspark.profiler.MemoryProfiler"type *á

_do_init%pyspark.context.SparkContext._do_init"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*P
masterD
Union[builtins.str,None]
builtins.str"builtins.str
None*Q
appNameD
Union[builtins.str,None]
builtins.str"builtins.str
None*S
	sparkHomeD
Union[builtins.str,None]
builtins.str"builtins.str
None*
pyFiles
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None*¤
environment’
+Union[builtins.dict[builtins.str,Any],None]W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict
None*+
	batchSize
builtins.int"builtins.int*P

serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer*l
confb
"Union[pyspark.conf.SparkConf,None]0
pyspark.conf.SparkConf"pyspark.conf.SparkConf
None*
jsc
Any*„
profiler_clsp
$Type[pyspark.profiler.BasicProfiler]@
pyspark.profiler.BasicProfiler"pyspark.profiler.BasicProfiler"type *‘
udf_profiler_clsy
'Type[pyspark.profiler.UDFBasicProfiler]F
!pyspark.profiler.UDFBasicProfiler"!pyspark.profiler.UDFBasicProfiler"type *Ž
memory_profiler_clss
%Type[pyspark.profiler.MemoryProfiler]B
pyspark.profiler.MemoryProfiler"pyspark.profiler.MemoryProfiler"type *
__repr__%pyspark.context.SparkContext.__repr__"
builtins.str"builtins.str*><
pyspark.context.SparkContext"pyspark.context.SparkContext*
_repr_html_(pyspark.context.SparkContext._repr_html_"
builtins.str"builtins.str*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*¬
_initialize_context0pyspark.context.SparkContext._initialize_context"
Any*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*
jconf
Any*…
_ensure_initialized0pyspark.context.SparkContext._ensure_initialized"
None*s
clsj
"Type[pyspark.context.SparkContext]<
pyspark.context.SparkContext"pyspark.context.SparkContext"type*„
instancet
(Union[pyspark.context.SparkContext,None]<
pyspark.context.SparkContext"pyspark.context.SparkContext
None *5
gateway&
Union[Any,None]
Any
None *n
confb
"Union[pyspark.conf.SparkConf,None]0
pyspark.conf.SparkConf"pyspark.conf.SparkConf
None 0:classmethodp*“
__getnewargs__+pyspark.context.SparkContext.__getnewargs__"
NoReturn
*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*±
	__enter__&pyspark.context.SparkContext.__enter__"<
pyspark.context.SparkContext"pyspark.context.SparkContext*><
pyspark.context.SparkContext"pyspark.context.SparkContext*Ô
__exit__%pyspark.context.SparkContext.__exit__"
None*><
pyspark.context.SparkContext"pyspark.context.SparkContext*“
(Union[Type[builtins.BaseException],None]X
Type[builtins.BaseException]0
builtins.BaseException"builtins.BaseException"type
None*db
"Union[builtins.BaseException,None]0
builtins.BaseException"builtins.BaseException
None*[Y
Union[types.TracebackType,None]*
types.TracebackType"types.TracebackType
None*ë
getOrCreate(pyspark.context.SparkContext.getOrCreate"<
pyspark.context.SparkContext"pyspark.context.SparkContext*s
clsj
"Type[pyspark.context.SparkContext]<
pyspark.context.SparkContext"pyspark.context.SparkContext"type*n
confb
"Union[pyspark.conf.SparkConf,None]0
pyspark.conf.SparkConf"pyspark.conf.SparkConf
None 0:classmethodp*µ
setLogLevel(pyspark.context.SparkContext.setLogLevel"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext**
logLevel
builtins.str"builtins.str*£
setSystemProperty.pyspark.context.SparkContext.setSystemProperty"
None*s
clsj
"Type[pyspark.context.SparkContext]<
pyspark.context.SparkContext"pyspark.context.SparkContext"type*%
key
builtins.str"builtins.str*'
value
builtins.str"builtins.str0:classmethodp*£
version$pyspark.context.SparkContext.version"
builtins.str"builtins.str*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*¯
applicationId*pyspark.context.SparkContext.applicationId"
builtins.str"builtins.str*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*Í
uiWebUrl%pyspark.context.SparkContext.uiWebUrl"D
Union[builtins.str,None]
builtins.str"builtins.str
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*§
	startTime&pyspark.context.SparkContext.startTime"
builtins.int"builtins.int*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*¹
defaultParallelism/pyspark.context.SparkContext.defaultParallelism"
builtins.int"builtins.int*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*½
defaultMinPartitions1pyspark.context.SparkContext.defaultMinPartitions"
builtins.int"builtins.int*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*{
stop!pyspark.context.SparkContext.stop"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*«
emptyRDD%pyspark.context.SparkContext.emptyRDD"0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*¾
range"pyspark.context.SparkContext.range"N
pyspark.rdd.RDD[builtins.int]
builtins.int"builtins.int"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*'
start
builtins.int"builtins.int*O
endD
Union[builtins.int,None]
builtins.int"builtins.int
None *(
step
builtins.int"builtins.int *U
	numSlicesD
Union[builtins.int,None]
builtins.int"builtins.int
None *æ
parallelize(pyspark.context.SparkContext.parallelize"
"pyspark.rdd.RDD[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*‰
c
"typing.Iterable[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"typing.Iterable*U
	numSlicesD
Union[builtins.int,None]
builtins.int"builtins.int
None *±
_serialize_to_jvm.pyspark.context.SparkContext._serialize_to_jvm"
Any*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*Œ
data
"typing.Iterable[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"typing.Iterable*P

serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer*\
reader_funcK
CallableType[builtins.function]&
builtins.function"builtins.function*\
server_funcK
CallableType[builtins.function]&
builtins.function"builtins.function*²

pickleFile'pyspark.context.SparkContext.pickleFile"0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
name
builtins.str"builtins.str*Y
minPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *ÿ
textFile%pyspark.context.SparkContext.textFile"N
pyspark.rdd.RDD[builtins.str]
builtins.str"builtins.str"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
name
builtins.str"builtins.str*Y
minPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *1
use_unicode
builtins.bool"builtins.bool *ä
wholeTextFiles+pyspark.context.SparkContext.wholeTextFiles"¦
1pyspark.rdd.RDD[Tuple[builtins.str,builtins.str]]`
 Tuple[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*Y
minPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *1
use_unicode
builtins.bool"builtins.bool *³
binaryFiles(pyspark.context.SparkContext.binaryFiles"®
3pyspark.rdd.RDD[Tuple[builtins.str,builtins.bytes]]f
"Tuple[builtins.str,builtins.bytes]
builtins.str"builtins.str 
builtins.bytes"builtins.bytes"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*Y
minPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *±
binaryRecords*pyspark.context.SparkContext.binaryRecords"T
pyspark.rdd.RDD[builtins.bytes] 
builtins.bytes"builtins.bytes"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*.
recordLength
builtins.int"builtins.int*Ò
_dictToJavaMap+pyspark.context.SparkContext._dictToJavaMap"
Any*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*Á
d¹
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None*­
sequenceFile)pyspark.context.SparkContext.sequenceFile"—
;pyspark.rdd.RDD[Tuple[pyspark.context.T,pyspark.context.U]]Æ
*Tuple[pyspark.context.T,pyspark.context.U]J
pyspark.context.T"
builtins.object"builtins.object"builtins.objectJ
pyspark.context.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*T
keyClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *V

valueClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *U
	minSplitsD
Union[builtins.int,None]
builtins.int"builtins.int
None *-
	batchSize
builtins.int"builtins.int *‡
newAPIHadoopFile-pyspark.context.SparkContext.newAPIHadoopFile"—
;pyspark.rdd.RDD[Tuple[pyspark.context.T,pyspark.context.U]]Æ
*Tuple[pyspark.context.T,pyspark.context.U]J
pyspark.context.T"
builtins.object"builtins.object"builtins.objectJ
pyspark.context.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*2
inputFormatClass
builtins.str"builtins.str**
keyClass
builtins.str"builtins.str*,

valueClass
builtins.str"builtins.str*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Æ
conf¹
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *-
	batchSize
builtins.int"builtins.int *Ý
newAPIHadoopRDD,pyspark.context.SparkContext.newAPIHadoopRDD"—
;pyspark.rdd.RDD[Tuple[pyspark.context.T,pyspark.context.U]]Æ
*Tuple[pyspark.context.T,pyspark.context.U]J
pyspark.context.T"
builtins.object"builtins.object"builtins.objectJ
pyspark.context.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*2
inputFormatClass
builtins.str"builtins.str**
keyClass
builtins.str"builtins.str*,

valueClass
builtins.str"builtins.str*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Æ
conf¹
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *-
	batchSize
builtins.int"builtins.int *û

hadoopFile'pyspark.context.SparkContext.hadoopFile"—
;pyspark.rdd.RDD[Tuple[pyspark.context.T,pyspark.context.U]]Æ
*Tuple[pyspark.context.T,pyspark.context.U]J
pyspark.context.T"
builtins.object"builtins.object"builtins.objectJ
pyspark.context.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*2
inputFormatClass
builtins.str"builtins.str**
keyClass
builtins.str"builtins.str*,

valueClass
builtins.str"builtins.str*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Æ
conf¹
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *-
	batchSize
builtins.int"builtins.int *Ñ
	hadoopRDD&pyspark.context.SparkContext.hadoopRDD"—
;pyspark.rdd.RDD[Tuple[pyspark.context.T,pyspark.context.U]]Æ
*Tuple[pyspark.context.T,pyspark.context.U]J
pyspark.context.T"
builtins.object"builtins.object"builtins.objectJ
pyspark.context.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*2
inputFormatClass
builtins.str"builtins.str**
keyClass
builtins.str"builtins.str*,

valueClass
builtins.str"builtins.str*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Æ
conf¹
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *-
	batchSize
builtins.int"builtins.int *Ç
_checkpointFile,pyspark.context.SparkContext._checkpointFile"0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
name
builtins.str"builtins.str*d
input_deserializerL
$pyspark.serializers.PairDeserializer"$pyspark.serializers.PairDeserializer*Ë
union"pyspark.context.SparkContext.union"
"pyspark.rdd.RDD[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*Ñ
rddsÆ
1builtins.list[pyspark.rdd.RDD[pyspark.context.T]]
"pyspark.rdd.RDD[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD"builtins.list*î
	broadcast&pyspark.context.SparkContext.broadcast"™
.pyspark.broadcast.Broadcast[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"pyspark.broadcast.Broadcast*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*U
valueJ
pyspark.context.T"
builtins.object"builtins.object"builtins.object*•
accumulator(pyspark.context.SparkContext.accumulator"£
3pyspark.accumulators.Accumulator[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*U
valueJ
pyspark.context.T"
builtins.object"builtins.object"builtins.object*–
accum_param‚
DUnion[pyspark.accumulators.AccumulatorParam[pyspark.context.T],None]­
8pyspark.accumulators.AccumulatorParam[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"%pyspark.accumulators.AccumulatorParam
None *Ú
addFile$pyspark.context.SparkContext.addFile"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*/
	recursive
builtins.bool"builtins.bool *Õ
	listFiles&pyspark.context.SparkContext.listFiles"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*­
	addPyFile&pyspark.context.SparkContext.addPyFile"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*¯

addArchive'pyspark.context.SparkContext.addArchive"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*Û
listArchives)pyspark.context.SparkContext.listArchives"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*¾
setCheckpointDir-pyspark.context.SparkContext.setCheckpointDir"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*)
dirName
builtins.str"builtins.str*Ï
getCheckpointDir-pyspark.context.SparkContext.getCheckpointDir"D
Union[builtins.str,None]
builtins.str"builtins.str
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*ô
_getJavaStorageLevel1pyspark.context.SparkContext._getJavaStorageLevel"
Any*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*X
storageLevelF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel*œ
setJobGroup(pyspark.context.SparkContext.setJobGroup"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*)
groupId
builtins.str"builtins.str*-
description
builtins.str"builtins.str*7
interruptOnCancel
builtins.bool"builtins.bool *Ò
setInterruptOnCancel1pyspark.context.SparkContext.setInterruptOnCancel"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*5
interruptOnCancel
builtins.bool"builtins.bool*¬
	addJobTag&pyspark.context.SparkContext.addJobTag"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*%
tag
builtins.str"builtins.str*²
removeJobTag)pyspark.context.SparkContext.removeJobTag"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*%
tag
builtins.str"builtins.str*Ç

getJobTags'pyspark.context.SparkContext.getJobTags"H
builtins.set[builtins.str]
builtins.str"builtins.str"builtins.set*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*‹
clearJobTags)pyspark.context.SparkContext.clearJobTags"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*ã
setLocalProperty-pyspark.context.SparkContext.setLocalProperty"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*%
key
builtins.str"builtins.str*'
value
builtins.str"builtins.str*ö
getLocalProperty-pyspark.context.SparkContext.getLocalProperty"D
Union[builtins.str,None]
builtins.str"builtins.str
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*%
key
builtins.str"builtins.str*¾
setJobDescription.pyspark.context.SparkContext.setJobDescription"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*'
value
builtins.str"builtins.str*™
	sparkUser&pyspark.context.SparkContext.sparkUser"
builtins.str"builtins.str*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*º
cancelJobGroup+pyspark.context.SparkContext.cancelJobGroup"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*)
groupId
builtins.str"builtins.str*¼
cancelJobsWithTag.pyspark.context.SparkContext.cancelJobsWithTag"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*%
tag
builtins.str"builtins.str*
cancelAllJobs*pyspark.context.SparkContext.cancelAllJobs"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*Á
statusTracker*pyspark.context.SparkContext.statusTracker"<
pyspark.status.StatusTracker"pyspark.status.StatusTracker*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*±
runJob#pyspark.context.SparkContext.runJob"}
 builtins.list[pyspark.context.U]J
pyspark.context.U"
builtins.object"builtins.object"builtins.object"builtins.list*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*‹
rdd
"pyspark.rdd.RDD[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*^
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function*š

partitions‡
)Union[typing.Sequence[builtins.int],None]N
typing.Sequence[builtins.int]
builtins.int"builtins.int"typing.Sequence
None *0

allowLocal
builtins.bool"builtins.bool *
show_profiles*pyspark.context.SparkContext.show_profiles"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*µ
dump_profiles*pyspark.context.SparkContext.dump_profiles"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*©
getConf$pyspark.context.SparkContext.getConf"0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*í
	resources&pyspark.context.SparkContext.resources"á
Lbuiltins.dict[builtins.str,pyspark.resource.information.ResourceInformation]
builtins.str"builtins.strd
0pyspark.resource.information.ResourceInformation"0pyspark.resource.information.ResourceInformation"builtins.dict*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*_
_assert_on_driver.pyspark.context.SparkContext._assert_on_driver"
None0:staticmethodhrY
_gateway%pyspark.context.SparkContext._gateway&
Union[Any,None]
Any
NonerQ
_jvm!pyspark.context.SparkContext._jvm&
Union[Any,None]
Any
Noner[
_next_accum_id+pyspark.context.SparkContext._next_accum_id
builtins.int"builtins.intrÁ
_active_spark_context2pyspark.context.SparkContext._active_spark_contextt
(Union[pyspark.context.SparkContext,None]<
pyspark.context.SparkContext"pyspark.context.SparkContext
NonerQ
_lock"pyspark.context.SparkContext._lock$
threading._RLock"threading._RLockrÅ
_python_includes-pyspark.context.SparkContext._python_includes
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
Nonerw

serializer'pyspark.context.SparkContext.serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializerr
profiler_collector/pyspark.context.SparkContext.profiler_collectorH
"pyspark.profiler.ProfilerCollector""pyspark.profiler.ProfilerCollectorr•
PACKAGE_EXTENSIONS/pyspark.context.SparkContext.PACKAGE_EXTENSIONSN
typing.Iterable[builtins.str]
builtins.str"builtins.str"typing.Iterabler’
	_callsite&pyspark.context.SparkContext._callsite]
Union[Any,Tuple[Any,Any,Any]]
Any1
Tuple[Any,Any,Any]
Any
Any
Anyr
environment(pyspark.context.SparkContext.environmentW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictr]
_conf"pyspark.context.SparkContext._conf0
pyspark.conf.SparkConf"pyspark.conf.SparkConfrS

_batchSize'pyspark.context.SparkContext._batchSize
builtins.int"builtins.intr
_unbatched_serializer2pyspark.context.SparkContext._unbatched_serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializerrs
master#pyspark.context.SparkContext.masterD
Union[builtins.str,None]
builtins.str"builtins.str
Noneru
appName$pyspark.context.SparkContext.appNameD
Union[builtins.str,None]
builtins.str"builtins.str
Nonery
	sparkHome&pyspark.context.SparkContext.sparkHomeD
Union[builtins.str,None]
builtins.str"builtins.str
Noner2
_jsc!pyspark.context.SparkContext._jsc
Anyr—
_accumulatorServer/pyspark.context.SparkContext._accumulatorServerP
&pyspark.accumulators.AccumulatorServer"&pyspark.accumulators.AccumulatorServerrJ
_javaAccumulator-pyspark.context.SparkContext._javaAccumulator
AnyrP
_encryption_enabled0pyspark.context.SparkContext._encryption_enabled
AnyrS

pythonExec'pyspark.context.SparkContext.pythonExec
builtins.str"builtins.strrQ
	pythonVer&pyspark.context.SparkContext.pythonVer
builtins.str"builtins.strr§
_pickled_broadcast_vars4pyspark.context.SparkContext._pickled_broadcast_varsV
)pyspark.broadcast.BroadcastPickleRegistry")pyspark.broadcast.BroadcastPickleRegistryr<
	_temp_dir&pyspark.context.SparkContext._temp_dir
Any(
_testpyspark.context._test"
None*‹
__annotations__pyspark.context.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*+
JavaMappyspark.context.JavaMap
Any*/
	Py4JErrorpyspark.context.Py4JError
Any*9
is_instance_ofpyspark.context.is_instance_of
Any*3
JavaGatewaypyspark.context.JavaGateway
Any*1

JavaObjectpyspark.context.JavaObject
Any*+
JVMViewpyspark.context.JVMView
Any*n
__all__pyspark.context.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*‹
DEFAULT_CONFIGSpyspark.context.DEFAULT_CONFIGSW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict