
"torch.distributed.fsdp._flat_paramî
HandleShardingStrategy9torch.distributed.fsdp._flat_param.HandleShardingStrategy"	enum.EnumHrj

FULL_SHARDDtorch.distributed.fsdp._flat_param.HandleShardingStrategy.FULL_SHARD
	enum.auto"	enum.autorp
SHARD_GRAD_OPGtorch.distributed.fsdp._flat_param.HandleShardingStrategy.SHARD_GRAD_OP
	enum.auto"	enum.autorf
NO_SHARDBtorch.distributed.fsdp._flat_param.HandleShardingStrategy.NO_SHARD
	enum.auto"	enum.autorn
HYBRID_SHARDFtorch.distributed.fsdp._flat_param.HandleShardingStrategy.HYBRID_SHARD
	enum.auto"	enum.autor|
_HYBRID_SHARD_ZERO2Mtorch.distributed.fsdp._flat_param.HandleShardingStrategy._HYBRID_SHARD_ZERO2
	enum.auto"	enum.auto“!
	ParamInfo,torch.distributed.fsdp._flat_param.ParamInfo"builtins.tuple*Ù
_replace5torch.distributed.fsdp._flat_param.ParamInfo._replace"¯
0torch.distributed.fsdp._flat_param.ParamInfo._NT¡
?Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str*Ñ
_self¯
0torch.distributed.fsdp._flat_param.ParamInfo._NT¡
?Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str*.

param_name
builtins.str"builtins.str *N
module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module */
module_name
builtins.str"builtins.str *Æ
__new__4torch.distributed.fsdp._flat_param.ParamInfo.__new__"¯
0torch.distributed.fsdp._flat_param.ParamInfo._NT¡
?Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str*∆
_clsª
6Type[torch.distributed.fsdp._flat_param.ParamInfo._NT]¯
0torch.distributed.fsdp._flat_param.ParamInfo._NT¡
?Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str"type*,

param_name
builtins.str"builtins.str*L
module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module*-
module_name
builtins.str"builtins.str*ü
_asdict4torch.distributed.fsdp._flat_param.ParamInfo._asdict"W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*Ñ
_self¯
0torch.distributed.fsdp._flat_param.ParamInfo._NT¡
?Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str*¯
_make2torch.distributed.fsdp._flat_param.ParamInfo._make"¯
0torch.distributed.fsdp._flat_param.ParamInfo._NT¡
?Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str*∆
_clsª
6Type[torch.distributed.fsdp._flat_param.ParamInfo._NT]¯
0torch.distributed.fsdp._flat_param.ParamInfo._NT¡
?Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str"type*>
iterable0
typing.Iterable[Any]
Any"typing.Iterable*
new
Any *
len
Any 0:classmethodprc

param_name7torch.distributed.fsdp._flat_param.ParamInfo.param_name
builtins.str"builtins.strr
module3torch.distributed.fsdp._flat_param.ParamInfo.module@
torch.nn.modules.module.Module"torch.nn.modules.module.Modulere
module_name8torch.distributed.fsdp._flat_param.ParamInfo.module_name
builtins.str"builtins.strrc

param_name7torch.distributed.fsdp._flat_param.ParamInfo.param_name
builtins.str"builtins.strr
module3torch.distributed.fsdp._flat_param.ParamInfo.module@
torch.nn.modules.module.Module"torch.nn.modules.module.Modulere
module_name8torch.distributed.fsdp._flat_param.ParamInfo.module_name
builtins.str"builtins.strrÕ
_fields4torch.distributed.fsdp._flat_param.ParamInfo._fieldsã
-Tuple[builtins.str,builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.strr¢
_field_types9torch.distributed.fsdp._flat_param.ParamInfo._field_typesW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictr®
_field_defaults<torch.distributed.fsdp._flat_param.ParamInfo._field_defaultsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictr]
_source4torch.distributed.fsdp._flat_param.ParamInfo._source
builtins.str"builtins.strr®
__annotations__<torch.distributed.fsdp._flat_param.ParamInfo.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictË6
SharedParamInfo2torch.distributed.fsdp._flat_param.SharedParamInfo"builtins.tuple*¥

_replace;torch.distributed.fsdp._flat_param.SharedParamInfo._replace"µ
6torch.distributed.fsdp._flat_param.SharedParamInfo._NT¯
xTuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str*¡
_selfµ
6torch.distributed.fsdp._flat_param.SharedParamInfo._NT¯
xTuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str*.

param_name
builtins.str"builtins.str *N
module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module */
module_name
builtins.str"builtins.str *3
prim_param_name
builtins.str"builtins.str *S
prim_module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module *4
prim_module_name
builtins.str"builtins.str *Ó

__new__:torch.distributed.fsdp._flat_param.SharedParamInfo.__new__"µ
6torch.distributed.fsdp._flat_param.SharedParamInfo._NT¯
xTuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str*â
_cls˛
<Type[torch.distributed.fsdp._flat_param.SharedParamInfo._NT]µ
6torch.distributed.fsdp._flat_param.SharedParamInfo._NT¯
xTuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str"type*,

param_name
builtins.str"builtins.str*L
module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module*-
module_name
builtins.str"builtins.str*1
prim_param_name
builtins.str"builtins.str*Q
prim_module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module*2
prim_module_name
builtins.str"builtins.str*‚
_asdict:torch.distributed.fsdp._flat_param.SharedParamInfo._asdict"W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*¡
_selfµ
6torch.distributed.fsdp._flat_param.SharedParamInfo._NT¯
xTuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str*˛
_make8torch.distributed.fsdp._flat_param.SharedParamInfo._make"µ
6torch.distributed.fsdp._flat_param.SharedParamInfo._NT¯
xTuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str*â
_cls˛
<Type[torch.distributed.fsdp._flat_param.SharedParamInfo._NT]µ
6torch.distributed.fsdp._flat_param.SharedParamInfo._NT¯
xTuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str"type*>
iterable0
typing.Iterable[Any]
Any"typing.Iterable*
new
Any *
len
Any 0:classmethodpri

param_name=torch.distributed.fsdp._flat_param.SharedParamInfo.param_name
builtins.str"builtins.strrÖ
module9torch.distributed.fsdp._flat_param.SharedParamInfo.module@
torch.nn.modules.module.Module"torch.nn.modules.module.Modulerk
module_name>torch.distributed.fsdp._flat_param.SharedParamInfo.module_name
builtins.str"builtins.strrs
prim_param_nameBtorch.distributed.fsdp._flat_param.SharedParamInfo.prim_param_name
builtins.str"builtins.strrè
prim_module>torch.distributed.fsdp._flat_param.SharedParamInfo.prim_module@
torch.nn.modules.module.Module"torch.nn.modules.module.Moduleru
prim_module_nameCtorch.distributed.fsdp._flat_param.SharedParamInfo.prim_module_name
builtins.str"builtins.strri

param_name=torch.distributed.fsdp._flat_param.SharedParamInfo.param_name
builtins.str"builtins.strrÖ
module9torch.distributed.fsdp._flat_param.SharedParamInfo.module@
torch.nn.modules.module.Module"torch.nn.modules.module.Modulerk
module_name>torch.distributed.fsdp._flat_param.SharedParamInfo.module_name
builtins.str"builtins.strrs
prim_param_nameBtorch.distributed.fsdp._flat_param.SharedParamInfo.prim_param_name
builtins.str"builtins.strrè
prim_module>torch.distributed.fsdp._flat_param.SharedParamInfo.prim_module@
torch.nn.modules.module.Module"torch.nn.modules.module.Moduleru
prim_module_nameCtorch.distributed.fsdp._flat_param.SharedParamInfo.prim_module_name
builtins.str"builtins.strr‘
_fields:torch.distributed.fsdp._flat_param.SharedParamInfo._fieldså
TTuple[builtins.str,builtins.str,builtins.str,builtins.str,builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.strr®
_field_types?torch.distributed.fsdp._flat_param.SharedParamInfo._field_typesW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictrÆ
_field_defaultsBtorch.distributed.fsdp._flat_param.SharedParamInfo._field_defaultsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictrc
_source:torch.distributed.fsdp._flat_param.SharedParamInfo._source
builtins.str"builtins.strrÆ
__annotations__Btorch.distributed.fsdp._flat_param.SharedParamInfo.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictÖ;
_ShardParamInfo2torch.distributed.fsdp._flat_param._ShardParamInfo"builtins.tuple*Ë
_replace;torch.distributed.fsdp._flat_param._ShardParamInfo._replace"Ò
6torch.distributed.fsdp._flat_param._ShardParamInfo._NT¥
xTuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]
builtins.bool"builtins.boolD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
None*˝
_selfÒ
6torch.distributed.fsdp._flat_param._ShardParamInfo._NT¥
xTuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]
builtins.bool"builtins.boolD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
None*.
in_shard
builtins.bool"builtins.bool *[
offset_in_shardD
Union[builtins.int,None]
builtins.int"builtins.int
None *Z
numel_in_shardD
Union[builtins.int,None]
builtins.int"builtins.int
None *a
intra_param_start_idxD
Union[builtins.int,None]
builtins.int"builtins.int
None *_
intra_param_end_idxD
Union[builtins.int,None]
builtins.int"builtins.int
None *§
__new__:torch.distributed.fsdp._flat_param._ShardParamInfo.__new__"Ò
6torch.distributed.fsdp._flat_param._ShardParamInfo._NT¥
xTuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]
builtins.bool"builtins.boolD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
None*≈
_cls∫
<Type[torch.distributed.fsdp._flat_param._ShardParamInfo._NT]Ò
6torch.distributed.fsdp._flat_param._ShardParamInfo._NT¥
xTuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]
builtins.bool"builtins.boolD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
None"type*,
in_shard
builtins.bool"builtins.bool*Y
offset_in_shardD
Union[builtins.int,None]
builtins.int"builtins.int
None*X
numel_in_shardD
Union[builtins.int,None]
builtins.int"builtins.int
None*_
intra_param_start_idxD
Union[builtins.int,None]
builtins.int"builtins.int
None*]
intra_param_end_idxD
Union[builtins.int,None]
builtins.int"builtins.int
None*û
_asdict:torch.distributed.fsdp._flat_param._ShardParamInfo._asdict"W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*˝
_selfÒ
6torch.distributed.fsdp._flat_param._ShardParamInfo._NT¥
xTuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]
builtins.bool"builtins.boolD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
None*ˆ	
_make8torch.distributed.fsdp._flat_param._ShardParamInfo._make"Ò
6torch.distributed.fsdp._flat_param._ShardParamInfo._NT¥
xTuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]
builtins.bool"builtins.boolD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
None*≈
_cls∫
<Type[torch.distributed.fsdp._flat_param._ShardParamInfo._NT]Ò
6torch.distributed.fsdp._flat_param._ShardParamInfo._NT¥
xTuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]
builtins.bool"builtins.boolD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
None"type*>
iterable0
typing.Iterable[Any]
Any"typing.Iterable*
new
Any *
len
Any 0:classmethodprg
in_shard;torch.distributed.fsdp._flat_param._ShardParamInfo.in_shard
builtins.bool"builtins.boolrõ
offset_in_shardBtorch.distributed.fsdp._flat_param._ShardParamInfo.offset_in_shardD
Union[builtins.int,None]
builtins.int"builtins.int
Nonerô
numel_in_shardAtorch.distributed.fsdp._flat_param._ShardParamInfo.numel_in_shardD
Union[builtins.int,None]
builtins.int"builtins.int
Nonerß
intra_param_start_idxHtorch.distributed.fsdp._flat_param._ShardParamInfo.intra_param_start_idxD
Union[builtins.int,None]
builtins.int"builtins.int
Noner£
intra_param_end_idxFtorch.distributed.fsdp._flat_param._ShardParamInfo.intra_param_end_idxD
Union[builtins.int,None]
builtins.int"builtins.int
Nonerg
in_shard;torch.distributed.fsdp._flat_param._ShardParamInfo.in_shard
builtins.bool"builtins.boolrõ
offset_in_shardBtorch.distributed.fsdp._flat_param._ShardParamInfo.offset_in_shardD
Union[builtins.int,None]
builtins.int"builtins.int
Nonerô
numel_in_shardAtorch.distributed.fsdp._flat_param._ShardParamInfo.numel_in_shardD
Union[builtins.int,None]
builtins.int"builtins.int
Nonerß
intra_param_start_idxHtorch.distributed.fsdp._flat_param._ShardParamInfo.intra_param_start_idxD
Union[builtins.int,None]
builtins.int"builtins.int
Noner£
intra_param_end_idxFtorch.distributed.fsdp._flat_param._ShardParamInfo.intra_param_end_idxD
Union[builtins.int,None]
builtins.int"builtins.int
Noner©
_fields:torch.distributed.fsdp._flat_param._ShardParamInfo._fields·
GTuple[builtins.str,builtins.str,builtins.str,builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.strr®
_field_types?torch.distributed.fsdp._flat_param._ShardParamInfo._field_typesW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictrÆ
_field_defaultsBtorch.distributed.fsdp._flat_param._ShardParamInfo._field_defaultsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictrc
_source:torch.distributed.fsdp._flat_param._ShardParamInfo._source
builtins.str"builtins.strrÆ
__annotations__Btorch.distributed.fsdp._flat_param._ShardParamInfo.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictéC
FlatParamShardMetadata9torch.distributed.fsdp._flat_param.FlatParamShardMetadata"builtins.tuple*ù
_replaceBtorch.distributed.fsdp._flat_param.FlatParamShardMetadata._replace"Ï
=torch.distributed.fsdp._flat_param.FlatParamShardMetadata._NT®
èTuple[builtins.tuple[builtins.str],builtins.tuple[torch._C.Size],builtins.tuple[builtins.int],builtins.tuple[Tuple[builtins.int,builtins.int]]]L
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tupleO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tupleL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuple§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tuple*¯
_selfÏ
=torch.distributed.fsdp._flat_param.FlatParamShardMetadata._NT®
èTuple[builtins.tuple[builtins.str],builtins.tuple[torch._C.Size],builtins.tuple[builtins.int],builtins.tuple[Tuple[builtins.int,builtins.int]]]L
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tupleO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tupleL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuple§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tuple*_
param_namesL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuple *c
param_shapesO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tuple *`
param_numelsL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuple *∫
param_offsets§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tuple *‚
__new__Atorch.distributed.fsdp._flat_param.FlatParamShardMetadata.__new__"Ï
=torch.distributed.fsdp._flat_param.FlatParamShardMetadata._NT®
èTuple[builtins.tuple[builtins.str],builtins.tuple[torch._C.Size],builtins.tuple[builtins.int],builtins.tuple[Tuple[builtins.int,builtins.int]]]L
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tupleO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tupleL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuple§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tuple*«
_clsº
CType[torch.distributed.fsdp._flat_param.FlatParamShardMetadata._NT]Ï
=torch.distributed.fsdp._flat_param.FlatParamShardMetadata._NT®
èTuple[builtins.tuple[builtins.str],builtins.tuple[torch._C.Size],builtins.tuple[builtins.int],builtins.tuple[Tuple[builtins.int,builtins.int]]]L
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tupleO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tupleL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuple§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tuple"type*]
param_namesL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuple*a
param_shapesO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tuple*^
param_numelsL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuple*∏
param_offsets§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tuple*†
_asdictAtorch.distributed.fsdp._flat_param.FlatParamShardMetadata._asdict"W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*¯
_selfÏ
=torch.distributed.fsdp._flat_param.FlatParamShardMetadata._NT®
èTuple[builtins.tuple[builtins.str],builtins.tuple[torch._C.Size],builtins.tuple[builtins.int],builtins.tuple[Tuple[builtins.int,builtins.int]]]L
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tupleO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tupleL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuple§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tuple*˙
_make?torch.distributed.fsdp._flat_param.FlatParamShardMetadata._make"Ï
=torch.distributed.fsdp._flat_param.FlatParamShardMetadata._NT®
èTuple[builtins.tuple[builtins.str],builtins.tuple[torch._C.Size],builtins.tuple[builtins.int],builtins.tuple[Tuple[builtins.int,builtins.int]]]L
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tupleO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tupleL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuple§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tuple*«
_clsº
CType[torch.distributed.fsdp._flat_param.FlatParamShardMetadata._NT]Ï
=torch.distributed.fsdp._flat_param.FlatParamShardMetadata._NT®
èTuple[builtins.tuple[builtins.str],builtins.tuple[torch._C.Size],builtins.tuple[builtins.int],builtins.tuple[Tuple[builtins.int,builtins.int]]]L
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tupleO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tupleL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuple§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tuple"type*>
iterable0
typing.Iterable[Any]
Any"typing.Iterable*
new
Any *
len
Any 0:classmethodpr¢
param_namesEtorch.distributed.fsdp._flat_param.FlatParamShardMetadata.param_namesL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuplerß
param_shapesFtorch.distributed.fsdp._flat_param.FlatParamShardMetadata.param_shapesO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tupler§
param_numelsFtorch.distributed.fsdp._flat_param.FlatParamShardMetadata.param_numelsL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuplerˇ
param_offsetsGtorch.distributed.fsdp._flat_param.FlatParamShardMetadata.param_offsets§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tupler¢
param_namesEtorch.distributed.fsdp._flat_param.FlatParamShardMetadata.param_namesL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuplerß
param_shapesFtorch.distributed.fsdp._flat_param.FlatParamShardMetadata.param_shapesO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tupler§
param_numelsFtorch.distributed.fsdp._flat_param.FlatParamShardMetadata.param_numelsL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuplerˇ
param_offsetsGtorch.distributed.fsdp._flat_param.FlatParamShardMetadata.param_offsets§
0builtins.tuple[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.tuplerÖ
_fieldsAtorch.distributed.fsdp._flat_param.FlatParamShardMetadata._fields∂
:Tuple[builtins.str,builtins.str,builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.strrØ
_field_typesFtorch.distributed.fsdp._flat_param.FlatParamShardMetadata._field_typesW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictrµ
_field_defaultsItorch.distributed.fsdp._flat_param.FlatParamShardMetadata._field_defaultsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictrj
_sourceAtorch.distributed.fsdp._flat_param.FlatParamShardMetadata._source
builtins.str"builtins.strrµ
__annotations__Itorch.distributed.fsdp._flat_param.FlatParamShardMetadata.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict“
_FlatParameterMeta5torch.distributed.fsdp._flat_param._FlatParameterMeta"builtins.object*t
__instancecheck__Gtorch.distributed.fsdp._flat_param._FlatParameterMeta.__instancecheck__*
self*
instanceÖE
FlatParameter0torch.distributed.fsdp._flat_param.FlatParameter"torch.nn.parameter.Parameter*m
__new__8torch.distributed.fsdp._flat_param.FlatParameter.__new__*
cls*

data *
requires_grad *Û
_init_metadata?torch.distributed.fsdp._flat_param.FlatParameter._init_metadata"
None*∞
cls¶
6Type[torch.distributed.fsdp._flat_param.FlatParameter]d
0torch.distributed.fsdp._flat_param.FlatParameter"0torch.distributed.fsdp._flat_param.FlatParameter"type*
self
Any*ø
param_infos≠
Ybuiltins.list[TypeAlias[Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]]]¿
JTypeAlias[Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]]¡
?Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str",torch.distributed.fsdp._flat_param.ParamInfo"builtins.list*V
numelsJ
builtins.list[builtins.int]
builtins.int"builtins.int"builtins.list*Y
shapesM
builtins.list[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.list*T
fqnsJ
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*˜
shared_param_infosﬁ
íbuiltins.list[TypeAlias[Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]]]∑
ÉTypeAlias[Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]]¯
xTuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str"2torch.distributed.fsdp._flat_param.SharedParamInfo"builtins.list*m
param_extensionsW
builtins.list[Union[Any,None]]&
Union[Any,None]
Any
None"builtins.list*Œ
params¡
7Union[builtins.list[torch.nn.parameter.Parameter],None]z
+builtins.list[torch.nn.parameter.Parameter]<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter"builtins.list
None*’
shared_params¡
7Union[builtins.list[torch.nn.parameter.Parameter],None]z
+builtins.list[torch.nn.parameter.Parameter]<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter"builtins.list
None*b
is_padding_maskM
builtins.list[builtins.bool]
builtins.bool"builtins.bool"builtins.list0:classmethodp@b5torch.distributed.fsdp._flat_param._FlatParameterMetarÖ
_unpadded_unsharded_sizeItorch.distributed.fsdp._flat_param.FlatParameter._unpadded_unsharded_size
torch._C.Size"torch._C.SizerÅ
_padded_unsharded_sizeGtorch.distributed.fsdp._flat_param.FlatParameter._padded_unsharded_size
torch._C.Size"torch._C.Sizero
_sharded_size>torch.distributed.fsdp._flat_param.FlatParameter._sharded_size
torch._C.Size"torch._C.Sizeri
_num_params<torch.distributed.fsdp._flat_param.FlatParameter._num_params
builtins.int"builtins.intrˇ
_param_infos=torch.distributed.fsdp._flat_param.FlatParameter._param_infosØ
Zbuiltins.tuple[TypeAlias[Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]]]¿
JTypeAlias[Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]]¡
?Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str",torch.distributed.fsdp._flat_param.ParamInfo"builtins.tuplerî
_shapes8torch.distributed.fsdp._flat_param.FlatParameter._shapesO
builtins.tuple[torch._C.Size]
torch._C.Size"torch._C.Size"builtins.tuplerç
_fqns6torch.distributed.fsdp._flat_param.FlatParameter._fqnsL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tupler≤
_param_extensionsBtorch.distributed.fsdp._flat_param.FlatParameter._param_extensionsY
builtins.tuple[Union[Any,None]]&
Union[Any,None]
Any
None"builtins.tupler´
_numels_with_paddingEtorch.distributed.fsdp._flat_param.FlatParameter._numels_with_paddingL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tuplerë
_numels8torch.distributed.fsdp._flat_param.FlatParameter._numelsL
builtins.tuple[builtins.int]
builtins.int"builtins.int"builtins.tupler¯
_shard_param_infosCtorch.distributed.fsdp._flat_param.FlatParameter._shard_param_infosú
ìbuiltins.tuple[TypeAlias[Tuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]]]Û
ÉTypeAlias[Tuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]]¥
xTuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]
builtins.bool"builtins.boolD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
None"2torch.distributed.fsdp._flat_param._ShardParamInfo"builtins.tupleræ
_shared_param_infosDtorch.distributed.fsdp._flat_param.FlatParameter._shared_param_infos‡
ìbuiltins.tuple[TypeAlias[Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]]]∑
ÉTypeAlias[Tuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]]¯
xTuple[builtins.str,torch.nn.modules.module.Module,builtins.str,builtins.str,torch.nn.modules.module.Module,builtins.str]
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str
builtins.str"builtins.str@
torch.nn.modules.module.Module"torch.nn.modules.module.Module
builtins.str"builtins.str"2torch.distributed.fsdp._flat_param.SharedParamInfo"builtins.tupler≈
_modules9torch.distributed.fsdp._flat_param.FlatParameter._modules~
,builtins.set[torch.nn.modules.module.Module]@
torch.nn.modules.module.Module"torch.nn.modules.module.Module"builtins.setry
_shard_numel_paddedDtorch.distributed.fsdp._flat_param.FlatParameter._shard_numel_padded
builtins.int"builtins.intr{
_local_shard=torch.distributed.fsdp._flat_param.FlatParameter._local_shard,
torch._tensor.Tensor"torch._tensor.Tensorrá
_full_param_paddedCtorch.distributed.fsdp._flat_param.FlatParameter._full_param_padded,
torch._tensor.Tensor"torch._tensor.Tensorrõ
_full_prec_full_param_paddedMtorch.distributed.fsdp._flat_param.FlatParameter._full_prec_full_param_padded,
torch._tensor.Tensor"torch._tensor.Tensorrç
_post_backward_hook_stateJtorch.distributed.fsdp._flat_param.FlatParameter._post_backward_hook_state$
Tuple[Any,Any]
Any
Anyrr
_post_backward_hook_handleKtorch.distributed.fsdp._flat_param.FlatParameter._post_backward_hook_handle
Anyru
	_mp_shard:torch.distributed.fsdp._flat_param.FlatParameter._mp_shard,
torch._tensor.Tensor"torch._tensor.Tensorru
	_cpu_grad:torch.distributed.fsdp._flat_param.FlatParameter._cpu_grad,
torch._tensor.Tensor"torch._tensor.TensorrÖ
_saved_grad_shardBtorch.distributed.fsdp._flat_param.FlatParameter._saved_grad_shard,
torch._tensor.Tensor"torch._tensor.Tensorrá
_params8torch.distributed.fsdp._flat_param.FlatParameter._params¡
7Union[builtins.list[torch.nn.parameter.Parameter],None]z
+builtins.list[torch.nn.parameter.Parameter]<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter"builtins.list
Nonerï
_shared_params?torch.distributed.fsdp._flat_param.FlatParameter._shared_params¡
7Union[builtins.list[torch.nn.parameter.Parameter],None]z
+builtins.list[torch.nn.parameter.Parameter]<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter"builtins.list
Noner≤
_tensors9torch.distributed.fsdp._flat_param.FlatParameter._tensorsÍ
;Union[builtins.list[Union[torch._tensor.Tensor,None]],None]û
/builtins.list[Union[torch._tensor.Tensor,None]]\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None"builtins.list
Noner·
_is_grad_none_maskCtorch.distributed.fsdp._flat_param.FlatParameter._is_grad_none_maskÖ
(Union[builtins.list[builtins.bool],None]M
builtins.list[builtins.bool]
builtins.bool"builtins.bool"builtins.list
Noner§
_is_padding_maskAtorch.distributed.fsdp._flat_param.FlatParameter._is_padding_maskM
builtins.list[builtins.bool]
builtins.bool"builtins.bool"builtins.list”π
FlatParamHandle2torch.distributed.fsdp._flat_param.FlatParamHandle"builtins.object*£
__init__;torch.distributed.fsdp._flat_param.FlatParamHandle.__init__"
None*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*î
paramsá
Ityping.Sequence[Union[torch.nn.parameter.Parameter,torch._tensor.Tensor]]®
8Union[torch.nn.parameter.Parameter,torch._tensor.Tensor]<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter,
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence*Z
fully_sharded_module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module*.
device"
torch._C.device"torch._C.device*ç
sharding_strategyv
9torch.distributed.fsdp._flat_param.HandleShardingStrategy"9torch.distributed.fsdp._flat_param.HandleShardingStrategy*2
offload_params
builtins.bool"builtins.bool*^
mp_param_dtypeJ
Union[torch._C.dtype,None] 
torch._C.dtype"torch._C.dtype
None*_
mp_reduce_dtypeJ
Union[torch._C.dtype,None] 
torch._C.dtype"torch._C.dtype
None*<
keep_low_precision_grads
builtins.bool"builtins.bool*e
process_groupR
'torch._C._distributed_c10d.ProcessGroup"'torch._C._distributed_c10d.ProcessGroup*3
use_orig_params
builtins.bool"builtins.bool*Ÿ
fsdp_extension¬
BUnion[torch.distributed.fsdp._fsdp_extensions.FSDPExtensions,None]p
6torch.distributed.fsdp._fsdp_extensions.FSDPExtensions"6torch.distributed.fsdp._fsdp_extensions.FSDPExtensions
None *c
_init_setattr_fnsDtorch.distributed.fsdp._flat_param.FlatParamHandle._init_setattr_fns*
self*õ
_init_get_unflat_views_fnLtorch.distributed.fsdp._flat_param.FlatParamHandle._init_get_unflat_views_fn"
Any*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*3
align_addresses
builtins.bool"builtins.bool*∂
_init_flat_param_and_metadataPtorch.distributed.fsdp._flat_param.FlatParamHandle._init_flat_param_and_metadata"
None*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*ê
paramsÉ
Gbuiltins.list[Union[torch._tensor.Tensor,torch.nn.parameter.Parameter]]®
8Union[torch._tensor.Tensor,torch.nn.parameter.Parameter],
torch._tensor.Tensor"torch._tensor.Tensor<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter"builtins.list*L
module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module*/
aligned_numel
builtins.int"builtins.int*3
use_orig_params
builtins.bool"builtins.bool*ß
_validate_tensors_to_flattenOtorch.distributed.fsdp._flat_param.FlatParamHandle._validate_tensors_to_flatten".
builtins.tuple[Any]
Any"builtins.tuple*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*ë
tensorsÉ
Gbuiltins.list[Union[torch._tensor.Tensor,torch.nn.parameter.Parameter]]®
8Union[torch._tensor.Tensor,torch.nn.parameter.Parameter],
torch._tensor.Tensor"torch._tensor.Tensor<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter"builtins.list*ô
flatten_tensorsBtorch.distributed.fsdp._flat_param.FlatParamHandle.flatten_tensors",
torch._tensor.Tensor"torch._tensor.Tensor*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*o
tensorsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*/
aligned_numel
builtins.int"builtins.int*§
flatten_tensors_into_flat_paramRtorch.distributed.fsdp._flat_param.FlatParamHandle.flatten_tensors_into_flat_param"d
0torch.distributed.fsdp._flat_param.FlatParameter"0torch.distributed.fsdp._flat_param.FlatParameter*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*o
tensorsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list*/
aligned_numel
builtins.int"builtins.int*1
requires_grad
builtins.bool"builtins.bool*®
_init_param_reduce_dtypesLtorch.distributed.fsdp._flat_param.FlatParamHandle._init_param_reduce_dtypes"
None*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*^
mp_param_dtypeJ
Union[torch._C.dtype,None] 
torch._C.dtype"torch._C.dtype
None*_
mp_reduce_dtypeJ
Union[torch._C.dtype,None] 
torch._C.dtype"torch._C.dtype
None*M
shard8torch.distributed.fsdp._flat_param.FlatParamHandle.shard*
self0*˘
_init_shard_metadataGtorch.distributed.fsdp._flat_param.FlatParamHandle._init_shard_metadata"
None*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*.
numel_padded
builtins.int"builtins.int*5
unsharded_start_idx
builtins.int"builtins.int*3
unsharded_end_idx
builtins.int"builtins.int*‹
_get_shard_metadataFtorch.distributed.fsdp._flat_param.FlatParamHandle._get_shard_metadata"ú
ìbuiltins.tuple[TypeAlias[Tuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]]]Û
ÉTypeAlias[Tuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]]¥
xTuple[builtins.bool,Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None],Union[builtins.int,None]]
builtins.bool"builtins.boolD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
NoneD
Union[builtins.int,None]
builtins.int"builtins.int
None"2torch.distributed.fsdp._flat_param._ShardParamInfo"builtins.tuple*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*5
unsharded_start_idx
builtins.int"builtins.int*3
unsharded_end_idx
builtins.int"builtins.int*˘
_get_unpadded_shardFtorch.distributed.fsdp._flat_param.FlatParamHandle._get_unpadded_shard"x
(Tuple[torch._tensor.Tensor,builtins.int],
torch._tensor.Tensor"torch._tensor.Tensor
builtins.int"builtins.int*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor*&
rank
builtins.int"builtins.int*,

world_size
builtins.int"builtins.int0:staticmethodh*Á

_get_shard=torch.distributed.fsdp._flat_param.FlatParamHandle._get_shard"x
(Tuple[torch._tensor.Tensor,builtins.int],
torch._tensor.Tensor"torch._tensor.Tensor
builtins.int"builtins.int*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor*&
rank
builtins.int"builtins.int*,

world_size
builtins.int"builtins.int0:staticmethodh*õ
_get_sharded_sizeDtorch.distributed.fsdp._flat_param.FlatParamHandle._get_sharded_size"
torch._C.Size"torch._C.Size*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor*&
rank
builtins.int"builtins.int*,

world_size
builtins.int"builtins.int0:staticmethodh*˛
_get_flat_param_offsetsJtorch.distributed.fsdp._flat_param.FlatParamHandle._get_flat_param_offsets"¢
/builtins.list[Tuple[builtins.int,builtins.int]]`
 Tuple[builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int"builtins.list*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*n
shard_metadataAtorch.distributed.fsdp._flat_param.FlatParamHandle.shard_metadata*
self0:no_type_check*Ü
init_flat_param_attributesMtorch.distributed.fsdp._flat_param.FlatParamHandle.init_flat_param_attributes*
self0:no_type_check*·
pre_unshard>torch.distributed.fsdp._flat_param.FlatParamHandle.pre_unshard"
builtins.bool"builtins.bool*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*q
_use_low_precision_shardKtorch.distributed.fsdp._flat_param.FlatParamHandle._use_low_precision_shard*
self*O
unshard:torch.distributed.fsdp._flat_param.FlatParamHandle.unshard*
self*Â
needs_unshard@torch.distributed.fsdp._flat_param.FlatParamHandle.needs_unshard"
builtins.bool"builtins.bool*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*Ö
"_alloc_padded_unsharded_flat_paramUtorch.distributed.fsdp._flat_param.FlatParamHandle._alloc_padded_unsharded_flat_param*
self*ô
 _get_padded_unsharded_flat_paramStorch.distributed.fsdp._flat_param.FlatParamHandle._get_padded_unsharded_flat_param",
torch._tensor.Tensor"torch._tensor.Tensor*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*‘
_all_gather_flat_paramItorch.distributed.fsdp._flat_param.FlatParamHandle._all_gather_flat_param",
torch._tensor.Tensor"torch._tensor.Tensor*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*M
padded_unsharded_flat_param,
torch._tensor.Tensor"torch._tensor.Tensor*∂
_use_unsharded_flat_paramLtorch.distributed.fsdp._flat_param.FlatParamHandle._use_unsharded_flat_param"
None*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*M
padded_unsharded_flat_param,
torch._tensor.Tensor"torch._tensor.Tensor*Y
post_unshard?torch.distributed.fsdp._flat_param.FlatParamHandle.post_unshard*
self*É
!_free_low_precision_sharded_paramTtorch.distributed.fsdp._flat_param.FlatParamHandle._free_low_precision_sharded_param*
self*[
unshard_grad?torch.distributed.fsdp._flat_param.FlatParamHandle.unshard_grad*
self0*Y
reshard_grad?torch.distributed.fsdp._flat_param.FlatParamHandle.reshard_grad*
self*{
prepare_gradient_for_backwardPtorch.distributed.fsdp._flat_param.FlatParamHandle.prepare_gradient_for_backward*
self*u
prepare_gradient_for_optimMtorch.distributed.fsdp._flat_param.FlatParamHandle.prepare_gradient_for_optim*
self*j
to_cpu9torch.distributed.fsdp._flat_param.FlatParamHandle.to_cpu*
self0:contextlib.contextmanager*Å
reshard:torch.distributed.fsdp._flat_param.FlatParamHandle.reshard"
Any*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*=
free_unsharded_flat_param
builtins.bool"builtins.bool*Y
post_reshard?torch.distributed.fsdp._flat_param.FlatParamHandle.post_reshard*
self*u
_free_unsharded_flat_paramMtorch.distributed.fsdp._flat_param.FlatParamHandle._free_unsharded_flat_param*
self*„
_use_sharded_flat_paramJtorch.distributed.fsdp._flat_param.FlatParamHandle._use_sharded_flat_param"
None*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*ñ
_get_unflat_views_unalignedNtorch.distributed.fsdp._flat_param.FlatParamHandle._get_unflat_views_unaligned*
self*
tensor 0:no_type_check*í
_get_unflat_views_alignedLtorch.distributed.fsdp._flat_param.FlatParamHandle._get_unflat_views_aligned*
self*
tensor 0:no_type_check*â
_use_unsharded_viewsGtorch.distributed.fsdp._flat_param.FlatParamHandle._use_unsharded_views*
self*
	as_params0:no_type_check*Ñ
_use_unsharded_grad_viewsLtorch.distributed.fsdp._flat_param.FlatParamHandle._use_unsharded_grad_views*
self0:no_type_check*º
unflatten_as_paramsFtorch.distributed.fsdp._flat_param.FlatParamHandle.unflatten_as_params"L
typing.Generator[Any,Any,Any]
Any
Any
Any"typing.Generator*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle0:contextlib.contextmanager*v
_use_sharded_viewsEtorch.distributed.fsdp._flat_param.FlatParamHandle._use_sharded_views*
self0:no_type_check*Ä
_use_sharded_grad_viewsJtorch.distributed.fsdp._flat_param.FlatParamHandle._use_sharded_grad_views*
self0:no_type_check*~
_writeback_orig_paramsItorch.distributed.fsdp._flat_param.FlatParamHandle._writeback_orig_params*
self0:no_type_check*ø
_writeback_tensorDtorch.distributed.fsdp._flat_param.FlatParamHandle._writeback_tensor"
None*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*l

src_tensor\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None*<

dst_tensor,
torch._tensor.Tensor"torch._tensor.Tensor*.
tensor_index
builtins.int"builtins.int*2
expected_shape
torch._C.Size"torch._C.Size*(
offset
builtins.int"builtins.int*,
is_param
builtins.bool"builtins.bool*ã
%_reset_flat_param_grad_info_if_neededXtorch.distributed.fsdp._flat_param.FlatParamHandle._reset_flat_param_grad_info_if_needed*
self*o
_deregister_orig_paramsJtorch.distributed.fsdp._flat_param.FlatParamHandle._deregister_orig_params*
self*q
flat_param_to@torch.distributed.fsdp._flat_param.FlatParamHandle.flat_param_to*
self*
args*

kwargs*√
_get_modules?torch.distributed.fsdp._flat_param.FlatParamHandle._get_modules"~
,builtins.set[torch.nn.modules.module.Module]@
torch.nn.modules.module.Module"torch.nn.modules.module.Module"builtins.set*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*ô

is_sharded=torch.distributed.fsdp._flat_param.FlatParamHandle.is_sharded"
builtins.bool"builtins.bool*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor*¯
param_module_namesEtorch.distributed.fsdp._flat_param.FlatParamHandle.param_module_names"¶
1typing.Iterator[Tuple[builtins.str,builtins.str]]`
 Tuple[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"typing.Iterator*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*Ü
shared_param_module_namesLtorch.distributed.fsdp._flat_param.FlatParamHandle.shared_param_module_names"¶
1typing.Iterator[Tuple[builtins.str,builtins.str]]`
 Tuple[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"typing.Iterator*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*°
_fqns_in_shardAtorch.distributed.fsdp._flat_param.FlatParamHandle._fqns_in_shard"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle0:property`*Ø
sharded_grad?torch.distributed.fsdp._flat_param.FlatParamHandle.sharded_grad"\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle0:property`*€
_reset_is_grad_noneFtorch.distributed.fsdp._flat_param.FlatParamHandle._reset_is_grad_none"
None*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*o
_check_sharded_strategyJtorch.distributed.fsdp._flat_param.FlatParamHandle._check_sharded_strategy*
self*û
_check_on_compute_deviceKtorch.distributed.fsdp._flat_param.FlatParamHandle._check_on_compute_device"
Any*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor*à
_check_on_cpu@torch.distributed.fsdp._flat_param.FlatParamHandle._check_on_cpu"
Any*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor*¥
_check_storage_freedGtorch.distributed.fsdp._flat_param.FlatParamHandle._check_storage_freed"
Any*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor0:staticmethodh*º
_check_storage_allocatedKtorch.distributed.fsdp._flat_param.FlatParamHandle._check_storage_allocated"
Any*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor0:staticmethodh*u
_check_low_precision_shardMtorch.distributed.fsdp._flat_param.FlatParamHandle._check_low_precision_shard*
self*é
_check_unshardedCtorch.distributed.fsdp._flat_param.FlatParamHandle._check_unsharded"
Any*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor*ä
_check_shardedAtorch.distributed.fsdp._flat_param.FlatParamHandle._check_sharded"
Any*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor*É
uses_sharded_strategyHtorch.distributed.fsdp._flat_param.FlatParamHandle.uses_sharded_strategy"
builtins.bool"builtins.bool*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle0:property`*è
_uses_param_mixed_precisionNtorch.distributed.fsdp._flat_param.FlatParamHandle._uses_param_mixed_precision"
builtins.bool"builtins.bool*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle0:property`*ë
_uses_reduce_mixed_precisionOtorch.distributed.fsdp._flat_param.FlatParamHandle._uses_reduce_mixed_precision"
builtins.bool"builtins.bool*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle0:property`*É
_force_full_precisionHtorch.distributed.fsdp._flat_param.FlatParamHandle._force_full_precision"
builtins.bool"builtins.bool*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle0:property`*ç
_skipped_use_sharded_viewsMtorch.distributed.fsdp._flat_param.FlatParamHandle._skipped_use_sharded_views"
builtins.bool"builtins.bool*r
selfh
2torch.distributed.fsdp._flat_param.FlatParamHandle"2torch.distributed.fsdp._flat_param.FlatParamHandle0:property`rÅ
_skip_writeback_checkHtorch.distributed.fsdp._flat_param.FlatParamHandle._skip_writeback_check
builtins.bool"builtins.boolrÉ
_use_full_prec_in_evalItorch.distributed.fsdp._flat_param.FlatParamHandle._use_full_prec_in_eval
builtins.bool"builtins.boolr
_use_fake_all_gatherGtorch.distributed.fsdp._flat_param.FlatParamHandle._use_fake_all_gather
builtins.bool"builtins.boolrw
_use_fake_reduceCtorch.distributed.fsdp._flat_param.FlatParamHandle._use_fake_reduce
builtins.bool"builtins.boolrg
device9torch.distributed.fsdp._flat_param.FlatParamHandle.device"
torch._C.device"torch._C.devicer≈
_device_handleAtorch.distributed.fsdp._flat_param.FlatParamHandle._device_handlep
6torch.distributed.fsdp._common_utils._FSDPDeviceHandle"6torch.distributed.fsdp._common_utils._FSDPDeviceHandler•
process_group@torch.distributed.fsdp._flat_param.FlatParamHandle.process_groupR
'torch._C._distributed_c10d.ProcessGroup"'torch._C._distributed_c10d.ProcessGrouprf
_fake_process_groupFtorch.distributed.fsdp._flat_param.FlatParamHandle._fake_process_group
Anyr]
rank7torch.distributed.fsdp._flat_param.FlatParamHandle.rank
builtins.int"builtins.intri

world_size=torch.distributed.fsdp._flat_param.FlatParamHandle.world_size
builtins.int"builtins.intr”
_sharding_strategyEtorch.distributed.fsdp._flat_param.FlatParamHandle._sharding_strategyv
9torch.distributed.fsdp._flat_param.HandleShardingStrategy"9torch.distributed.fsdp._flat_param.HandleShardingStrategyru
_offload_paramsBtorch.distributed.fsdp._flat_param.FlatParamHandle._offload_params
builtins.bool"builtins.boolrw
_use_orig_paramsCtorch.distributed.fsdp._flat_param.FlatParamHandle._use_orig_params
builtins.bool"builtins.boolrâ
_keep_low_precision_gradsLtorch.distributed.fsdp._flat_param.FlatParamHandle._keep_low_precision_grads
builtins.bool"builtins.boolrÀ
_training_stateBtorch.distributed.fsdp._flat_param.FlatParamHandle._training_statet
8torch.distributed.fsdp._common_utils.HandleTrainingState"8torch.distributed.fsdp._common_utils.HandleTrainingStaterX
_debug_level?torch.distributed.fsdp._flat_param.FlatParamHandle._debug_level
Anyr£
_fully_sharded_moduleHtorch.distributed.fsdp._flat_param.FlatParamHandle._fully_sharded_module@
torch.nn.modules.module.Module"torch.nn.modules.module.Moduler„
'_unsharded_flat_param_for_skipped_viewsZtorch.distributed.fsdp._flat_param.FlatParamHandle._unsharded_flat_param_for_skipped_views\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
Noneró
_handle_index@torch.distributed.fsdp._flat_param.FlatParamHandle._handle_indexD
Union[builtins.int,None]
builtins.int"builtins.int
Noner≠
_pre_forward_order_indexKtorch.distributed.fsdp._flat_param.FlatParamHandle._pre_forward_order_indexD
Union[builtins.int,None]
builtins.int"builtins.int
Noner£
_post_forward_indexFtorch.distributed.fsdp._flat_param.FlatParamHandle._post_forward_indexD
Union[builtins.int,None]
builtins.int"builtins.int
Nonerã
_needs_pre_forward_unshardMtorch.distributed.fsdp._flat_param.FlatParamHandle._needs_pre_forward_unshard
builtins.bool"builtins.boolrç
_needs_pre_backward_unshardNtorch.distributed.fsdp._flat_param.FlatParamHandle._needs_pre_backward_unshard
builtins.bool"builtins.boolrm
_prefetched>torch.distributed.fsdp._flat_param.FlatParamHandle._prefetched
builtins.bool"builtins.boolr{
_orig_param_dtypeDtorch.distributed.fsdp._flat_param.FlatParamHandle._orig_param_dtype 
torch._C.dtype"torch._C.dtyper\
_aligned_numelAtorch.distributed.fsdp._flat_param.FlatParamHandle._aligned_numel
Anyrö
_fsdp_extensionBtorch.distributed.fsdp._flat_param.FlatParamHandle._fsdp_extension¬
BUnion[torch.distributed.fsdp._fsdp_extensions.FSDPExtensions,None]p
6torch.distributed.fsdp._fsdp_extensions.FSDPExtensions"6torch.distributed.fsdp._fsdp_extensions.FSDPExtensions
Noner¢
_setattr_tensorBtorch.distributed.fsdp._flat_param.FlatParamHandle._setattr_tensorK
CallableType[builtins.function]&
builtins.function"builtins.functionr†
_setattr_paramAtorch.distributed.fsdp._flat_param.FlatParamHandle._setattr_paramK
CallableType[builtins.function]&
builtins.function"builtins.functionr¶
_get_unflat_viewsDtorch.distributed.fsdp._flat_param.FlatParamHandle._get_unflat_viewsK
CallableType[builtins.function]&
builtins.function"builtins.functionr±

flat_param=torch.distributed.fsdp._flat_param.FlatParamHandle.flat_paramd
0torch.distributed.fsdp._flat_param.FlatParameter"0torch.distributed.fsdp._flat_param.FlatParameterrï
_low_prec_param_dtype_specifiedRtorch.distributed.fsdp._flat_param.FlatParamHandle._low_prec_param_dtype_specified
builtins.bool"builtins.boolró
 _low_prec_reduce_dtype_specifiedStorch.distributed.fsdp._flat_param.FlatParamHandle._low_prec_reduce_dtype_specified
builtins.bool"builtins.boolr´
_fwd_bwd_param_dtypeGtorch.distributed.fsdp._flat_param.FlatParamHandle._fwd_bwd_param_dtypeJ
Union[torch._C.dtype,None] 
torch._C.dtype"torch._C.dtype
Nonerù
_reduce_dtype@torch.distributed.fsdp._flat_param.FlatParamHandle._reduce_dtypeJ
Union[torch._C.dtype,None] 
torch._C.dtype"torch._C.dtype
None†
_unsafe_setattr_param8torch.distributed.fsdp._flat_param._unsafe_setattr_param"
None*L
module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module*,

param_name
builtins.str"builtins.str*G
param<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameterì
_unsafe_setattr_tensor9torch.distributed.fsdp._flat_param._unsafe_setattr_tensor"
None*L
module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module*,

param_name
builtins.str"builtins.str*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensorß
_safe_setattr_tensor_or_param@torch.distributed.fsdp._flat_param._safe_setattr_tensor_or_param"
Any*L
module@
torch.nn.modules.module.Module"torch.nn.modules.module.Module*,

param_name
builtins.str"builtins.str*æ
tensor_or_param®
8Union[torch._tensor.Tensor,torch.nn.parameter.Parameter],
torch._tensor.Tensor"torch._tensor.Tensor<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter€
_convert_to_params5torch.distributed.fsdp._flat_param._convert_to_params"z
+builtins.list[torch.nn.parameter.Parameter]<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter"builtins.list*ë
tensorsÉ
Gbuiltins.list[Union[torch._tensor.Tensor,torch.nn.parameter.Parameter]]®
8Union[torch._tensor.Tensor,torch.nn.parameter.Parameter],
torch._tensor.Tensor"torch._tensor.Tensor<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter"builtins.list∏
_detach_if_needed4torch.distributed.fsdp._flat_param._detach_if_needed",
torch._tensor.Tensor"torch._tensor.Tensor*æ
param_or_tensor®
8Union[torch.nn.parameter.Parameter,torch._tensor.Tensor]<
torch.nn.parameter.Parameter"torch.nn.parameter.Parameter,
torch._tensor.Tensor"torch._tensor.Tensorã
_get_aligned_numel5torch.distributed.fsdp._flat_param._get_aligned_numel"
Any*5
unsharded_dtype 
torch._C.dtype"torch._C.dtype£
_construct_padding_tensor<torch.distributed.fsdp._flat_param._construct_padding_tensor"
Any*/
padding_numel
builtins.int"builtins.int*+
dtype 
torch._C.dtype"torch._C.dtype*1
requires_grad
builtins.bool"builtins.bool*.
device"
torch._C.device"torch._C.deviceO
_same_storage0torch.distributed.fsdp._flat_param._same_storage*
a*
bÆ
_same_storage_size5torch.distributed.fsdp._flat_param._same_storage_size"
Any*3
a,
torch._tensor.Tensor"torch._tensor.Tensor*#
b
builtins.int"builtins.intò
_storage_size_allocated:torch.distributed.fsdp._flat_param._storage_size_allocated"
Any*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor*û
__annotations__2torch.distributed.fsdp._flat_param.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*
disttorch.distributed *
nntorch.nn *
Ftorch.nn.functional *L
_ParameterMeta1torch.distributed.fsdp._flat_param._ParameterMeta
Any*Å
__all__*torch.distributed.fsdp._flat_param.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*U
logger)torch.distributed.fsdp._flat_param.logger 
logging.Logger"logging.Logger*u
_FSDP_USE_UNSAFE_SETATTR;torch.distributed.fsdp._flat_param._FSDP_USE_UNSAFE_SETATTR
builtins.str"builtins.str*y
_FSDP_SKIP_WRITEBACK_CHECK=torch.distributed.fsdp._flat_param._FSDP_SKIP_WRITEBACK_CHECK
builtins.str"builtins.str*{
_FSDP_USE_FULL_PREC_IN_EVAL>torch.distributed.fsdp._flat_param._FSDP_USE_FULL_PREC_IN_EVAL
builtins.str"builtins.str*w
_FLAT_PARAM_PADDING_VALUE<torch.distributed.fsdp._flat_param._FLAT_PARAM_PADDING_VALUE
builtins.int"builtins.int*w
_FSDP_USE_FAKE_ALL_GATHER<torch.distributed.fsdp._flat_param._FSDP_USE_FAKE_ALL_GATHER
builtins.str"builtins.str*o
_FSDP_USE_FAKE_REDUCE8torch.distributed.fsdp._flat_param._FSDP_USE_FAKE_REDUCE
builtins.str"builtins.str*Ê
'RESHARD_AFTER_FORWARD_HANDLE_STRATEGIESJtorch.distributed.fsdp._flat_param.RESHARD_AFTER_FORWARD_HANDLE_STRATEGIESÓ
zTuple[torch.distributed.fsdp._flat_param.HandleShardingStrategy,torch.distributed.fsdp._flat_param.HandleShardingStrategy]v
9torch.distributed.fsdp._flat_param.HandleShardingStrategy"9torch.distributed.fsdp._flat_param.HandleShardingStrategyv
9torch.distributed.fsdp._flat_param.HandleShardingStrategy"9torch.distributed.fsdp._flat_param.HandleShardingStrategy*Ï
*NO_RESHARD_AFTER_FORWARD_HANDLE_STRATEGIESMtorch.distributed.fsdp._flat_param.NO_RESHARD_AFTER_FORWARD_HANDLE_STRATEGIESÓ
zTuple[torch.distributed.fsdp._flat_param.HandleShardingStrategy,torch.distributed.fsdp._flat_param.HandleShardingStrategy]v
9torch.distributed.fsdp._flat_param.HandleShardingStrategy"9torch.distributed.fsdp._flat_param.HandleShardingStrategyv
9torch.distributed.fsdp._flat_param.HandleShardingStrategy"9torch.distributed.fsdp._flat_param.HandleShardingStrategy