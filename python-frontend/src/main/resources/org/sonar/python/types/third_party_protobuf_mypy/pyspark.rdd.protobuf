
pyspark.rdd…

RDDSamplerpyspark.rddsampler.RDDSampler"!pyspark.rddsampler.RDDSamplerBase*k
__init__&pyspark.rddsampler.RDDSampler.__init__*
self*
withReplacement*
fraction*

seed *M
func"pyspark.rddsampler.RDDSampler.func*
self*	
split*
iteratorr=
	_fraction'pyspark.rddsampler.RDDSampler._fraction
Any´
RDDRangeSampler"pyspark.rddsampler.RDDRangeSampler"!pyspark.rddsampler.RDDSamplerBase*m
__init__+pyspark.rddsampler.RDDRangeSampler.__init__*
self*

lowerBound*

upperBound*

seed *R
func'pyspark.rddsampler.RDDRangeSampler.func*
self*	
split*
iteratorrF
_lowerBound.pyspark.rddsampler.RDDRangeSampler._lowerBound
AnyrF
_upperBound.pyspark.rddsampler.RDDRangeSampler._upperBound
Any˛
RDDStratifiedSampler'pyspark.rddsampler.RDDStratifiedSampler"!pyspark.rddsampler.RDDSamplerBase*v
__init__0pyspark.rddsampler.RDDStratifiedSampler.__init__*
self*
withReplacement*
	fractions*

seed *W
func,pyspark.rddsampler.RDDStratifiedSampler.func*
self*	
split*
iteratorrI

_fractions2pyspark.rddsampler.RDDStratifiedSampler._fractions
Any”
PythonEvalTypepyspark.rdd.PythonEvalType"builtins.objectr®
NON_UDF"pyspark.rdd.PythonEvalType.NON_UDFy
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"pyspark._typing.NonUDFTyperƒ
SQL_BATCHED_UDF*pyspark.rdd.PythonEvalType.SQL_BATCHED_UDFÑ
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"%pyspark.sql._typing.SQLBatchedUDFTyper’
SQL_ARROW_BATCHED_UDF0pyspark.rdd.PythonEvalType.SQL_ARROW_BATCHED_UDFâ
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"*pyspark.sql._typing.SQLArrowBatchedUDFTyperŸ
SQL_SCALAR_PANDAS_UDF0pyspark.rdd.PythonEvalType.SQL_SCALAR_PANDAS_UDFç
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int".pyspark.sql.pandas._typing.PandasScalarUDFTyperÁ
SQL_GROUPED_MAP_PANDAS_UDF5pyspark.rdd.PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDFë
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"2pyspark.sql.pandas._typing.PandasGroupedMapUDFTyperÁ
SQL_GROUPED_AGG_PANDAS_UDF5pyspark.rdd.PythonEvalType.SQL_GROUPED_AGG_PANDAS_UDFë
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"2pyspark.sql.pandas._typing.PandasGroupedAggUDFTyper‰
SQL_WINDOW_AGG_PANDAS_UDF4pyspark.rdd.PythonEvalType.SQL_WINDOW_AGG_PANDAS_UDFê
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"1pyspark.sql.pandas._typing.PandasWindowAggUDFTyperÁ
SQL_SCALAR_PANDAS_ITER_UDF5pyspark.rdd.PythonEvalType.SQL_SCALAR_PANDAS_ITER_UDFë
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"2pyspark.sql.pandas._typing.PandasScalarIterUDFTyperﬁ
SQL_MAP_PANDAS_ITER_UDF2pyspark.rdd.PythonEvalType.SQL_MAP_PANDAS_ITER_UDFé
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"/pyspark.sql.pandas._typing.PandasMapIterUDFTyperÌ
SQL_COGROUPED_MAP_PANDAS_UDF7pyspark.rdd.PythonEvalType.SQL_COGROUPED_MAP_PANDAS_UDFì
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"4pyspark.sql.pandas._typing.PandasCogroupedMapUDFTyper€
SQL_MAP_ARROW_ITER_UDF1pyspark.rdd.PythonEvalType.SQL_MAP_ARROW_ITER_UDFç
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int".pyspark.sql.pandas._typing.ArrowMapIterUDFTyperÜ
%SQL_GROUPED_MAP_PANDAS_UDF_WITH_STATE@pyspark.rdd.PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF_WITH_STATEö
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int";pyspark.sql.pandas._typing.PandasGroupedMapUDFWithStateTyperæ
SQL_TABLE_UDF(pyspark.rdd.PythonEvalType.SQL_TABLE_UDFÇ
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"#pyspark.sql._typing.SQLTableUDFTyperœ
SQL_ARROW_TABLE_UDF.pyspark.rdd.PythonEvalType.SQL_ARROW_TABLE_UDFá
 TypeAlias[Literal[builtins.int]]7
Literal[builtins.int]	
builtins.int"builtins.int"(pyspark.sql._typing.SQLArrowTableUDFTypeü
BoundedFloatpyspark.rdd.BoundedFloat"builtins.float*ˇ
__new__ pyspark.rdd.BoundedFloat.__new__"4
pyspark.rdd.BoundedFloat"pyspark.rdd.BoundedFloat*g
cls^
Type[pyspark.rdd.BoundedFloat]4
pyspark.rdd.BoundedFloat"pyspark.rdd.BoundedFloat"type**
mean 
builtins.float"builtins.float*0

confidence 
builtins.float"builtins.float*)
low 
builtins.float"builtins.float**
high 
builtins.float"builtins.floatrS

confidence#pyspark.rdd.BoundedFloat.confidence 
builtins.float"builtins.floatrE
lowpyspark.rdd.BoundedFloat.low 
builtins.float"builtins.floatrG
highpyspark.rdd.BoundedFloat.high 
builtins.float"builtins.float¬
Partitionerpyspark.rdd.Partitioner"builtins.object*Ö
__init__ pyspark.rdd.Partitioner.__init__"
None*<
self2
pyspark.rdd.Partitioner"pyspark.rdd.Partitioner*/
numPartitions
builtins.int"builtins.int*^
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function*â
__eq__pyspark.rdd.Partitioner.__eq__"
builtins.bool"builtins.bool*42
pyspark.rdd.Partitioner"pyspark.rdd.Partitioner*	
Any*ò
__call__ pyspark.rdd.Partitioner.__call__"
builtins.int"builtins.int*<
self2
pyspark.rdd.Partitioner"pyspark.rdd.Partitioner*
k
AnyrT
numPartitions%pyspark.rdd.Partitioner.numPartitions
builtins.int"builtins.intrÉ
partitionFunc%pyspark.rdd.Partitioner.partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function¿ö
RDDpyspark.rdd.RDD"builtins.object*Ô
__init__pyspark.rdd.RDD.__init__"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*
jrdd
Any*E
ctx<
pyspark.context.SparkContext"pyspark.context.SparkContext*Y
jrdd_deserializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer *•
_pickledpyspark.rdd.RDD._pickled"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*¬
idpyspark.rdd.RDD.id"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*∆
__repr__pyspark.rdd.RDD.__repr__"
builtins.str"builtins.str*Å
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD* 
__getnewargs__pyspark.rdd.RDD.__getnewargs__"
NoReturn
*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
contextpyspark.rdd.RDD.context"<
pyspark.context.SparkContext"pyspark.context.SparkContext*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD0:builtins.property`*ü
cachepyspark.rdd.RDD.cache"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ˇ
persistpyspark.rdd.RDD.persist"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Z
storageLevelF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel *◊
	unpersistpyspark.rdd.RDD.unpersist"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*.
blocking
builtins.bool"builtins.bool *æ

checkpointpyspark.rdd.RDD.checkpoint"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*‹
isCheckpointedpyspark.rdd.RDD.isCheckpointed"
builtins.bool"builtins.bool*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*»
localCheckpointpyspark.rdd.RDD.localCheckpoint"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Í
isLocallyCheckpointed%pyspark.rdd.RDD.isLocallyCheckpointed"
builtins.bool"builtins.bool*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*à
getCheckpointFile!pyspark.rdd.RDD.getCheckpointFile"D
Union[builtins.str,None]
builtins.str"builtins.str
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
cleanShuffleDependencies(pyspark.rdd.RDD.cleanShuffleDependencies"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*.
blocking
builtins.bool"builtins.bool *¨
mappyspark.rdd.RDD.map"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *¥
flatMappyspark.rdd.RDD.flatMap"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *¿
mapPartitionspyspark.rdd.RDD.mapPartitions"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *“
mapPartitionsWithIndex&pyspark.rdd.RDD.mapPartitionsWithIndex"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *“
mapPartitionsWithSplit&pyspark.rdd.RDD.mapPartitionsWithSplit"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *ﬁ
getNumPartitions pyspark.rdd.RDD.getNumPartitions"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ı
filterpyspark.rdd.RDD.filter"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*Ä
distinctpyspark.rdd.RDD.distinct"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *ÿ
samplepyspark.rdd.RDD.sample"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*3
withReplacement
builtins.bool"builtins.bool*.
fraction 
builtins.float"builtins.float*P
seedD
Union[builtins.int,None]
builtins.int"builtins.int
None *˝
randomSplitpyspark.rdd.RDD.randomSplit"π
-builtins.list[pyspark.rdd.RDD[pyspark.rdd.T]]y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*º
weightsÆ
3typing.Sequence[Union[builtins.int,builtins.float]]f
"Union[builtins.int,builtins.float]
builtins.int"builtins.int 
builtins.float"builtins.float"typing.Sequence*P
seedD
Union[builtins.int,None]
builtins.int"builtins.int
None *”

takeSamplepyspark.rdd.RDD.takeSample"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*3
withReplacement
builtins.bool"builtins.bool*%
num
builtins.int"builtins.int*P
seedD
Union[builtins.int,None]
builtins.int"builtins.int
None *°
_computeFractionForSampleSize-pyspark.rdd.RDD._computeFractionForSampleSize" 
builtins.float"builtins.float*6
sampleSizeLowerBound
builtins.int"builtins.int*'
total
builtins.int"builtins.int*3
withReplacement
builtins.bool"builtins.bool0:builtins.staticmethodh*≠
unionpyspark.rdd.RDD.union"ˇ
3pyspark.rdd.RDD[Union[pyspark.rdd.T,pyspark.rdd.U]]∂
"Union[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ñ
othery
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*¥
intersectionpyspark.rdd.RDD.intersection"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ñ
othery
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*º
_reserializepyspark.rdd.RDD._reserialize"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*å

serializerz
*Union[pyspark.serializers.Serializer,None]@
pyspark.serializers.Serializer"pyspark.serializers.Serializer
None *û
__add__pyspark.rdd.RDD.__add__"ˇ
3pyspark.rdd.RDD[Union[pyspark.rdd.T,pyspark.rdd.U]]∂
"Union[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*{y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*{y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*á
sortBypyspark.rdd.RDD.sortBy"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*X
keyfuncK
CallableType[builtins.function]&
builtins.function"builtins.function*/
	ascending
builtins.bool"builtins.bool *Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *‹
glompyspark.rdd.RDD.glom"∑
-pyspark.rdd.RDD[builtins.list[pyspark.rdd.T]]u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*µ
	cartesianpyspark.rdd.RDD.cartesian"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.T,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ñ
othery
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ê
groupBypyspark.rdd.RDD.groupBy"‘
Dpyspark.rdd.RDD[Tuple[pyspark.rdd.K,typing.Iterable[pyspark.rdd.T]]]˙
3Tuple[pyspark.rdd.K,typing.Iterable[pyspark.rdd.T]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashabley
typing.Iterable[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"typing.Iterable"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *ú
pipepyspark.rdd.RDD.pipe"N
pyspark.rdd.RDD[builtins.str]
builtins.str"builtins.str"pyspark.rdd.RDD*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*)
command
builtins.str"builtins.str*≈
envπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None */
	checkCode
builtins.bool"builtins.bool *Ü
foreachpyspark.rdd.RDD.foreach"
None*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*ò
foreachPartition pyspark.rdd.RDD.foreachPartition"
None*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*ü
collectpyspark.rdd.RDD.collect"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD* 
collectWithJobGroup#pyspark.rdd.RDD.collectWithJobGroup"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*)
groupId
builtins.str"builtins.str*-
description
builtins.str"builtins.str*7
interruptOnCancel
builtins.bool"builtins.bool *¬
reducepyspark.rdd.RDD.reduce"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*ı

treeReducepyspark.rdd.RDD.treeReduce"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*)
depth
builtins.int"builtins.int *ñ
foldpyspark.rdd.RDD.fold"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
	zeroValueF
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*S
opK
CallableType[builtins.function]&
builtins.function"builtins.function*¸
	aggregatepyspark.rdd.RDD.aggregate"F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
	zeroValueF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object*V
seqOpK
CallableType[builtins.function]&
builtins.function"builtins.function*W
combOpK
CallableType[builtins.function]&
builtins.function"builtins.function*Ø
treeAggregatepyspark.rdd.RDD.treeAggregate"F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
	zeroValueF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object*V
seqOpK
CallableType[builtins.function]&
builtins.function"builtins.function*W
combOpK
CallableType[builtins.function]&
builtins.function"builtins.function*)
depth
builtins.int"builtins.int *ô
sumpyspark.rdd.RDD.sum"V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*»
countpyspark.rdd.RDD.count"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*â
statspyspark.rdd.RDD.stats"B
pyspark.statcounter.StatCounter"pyspark.statcounter.StatCounter*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*®
	histogrampyspark.rdd.RDD.histogram"Ã
ETuple[typing.Sequence[pyspark._typing.S],builtins.list[builtins.int]]¥
"typing.Sequence[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"typing.SequenceJ
builtins.list[builtins.int]
builtins.int"builtins.int"builtins.list*ø
self¥
"pyspark.rdd.RDD[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"pyspark.rdd.RDD*Ó
buckets‡
VUnion[builtins.int,builtins.list[pyspark._typing.S],builtins.tuple[pyspark._typing.S]]
builtins.int"builtins.int∞
 builtins.list[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"builtins.list≤
!builtins.tuple[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"builtins.tuple*Â
meanpyspark.rdd.RDD.mean" 
builtins.float"builtins.float*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ì
variancepyspark.rdd.RDD.variance" 
builtins.float"builtins.float*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Á
stdevpyspark.rdd.RDD.stdev" 
builtins.float"builtins.float*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Û
sampleStdevpyspark.rdd.RDD.sampleStdev" 
builtins.float"builtins.float*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*˘
sampleVariancepyspark.rdd.RDD.sampleVariance" 
builtins.float"builtins.float*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*’
countByValuepyspark.rdd.RDD.countByValue"†
)builtins.dict[pyspark.rdd.K,builtins.int]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable
builtins.int"builtins.int"builtins.dict*É
selfy
pyspark.rdd.RDD[pyspark.rdd.K]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable"pyspark.rdd.RDD*¿
takepyspark.rdd.RDD.take"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*%
num
builtins.int"builtins.int*Ï
firstpyspark.rdd.RDD.first"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Œ
isEmptypyspark.rdd.RDD.isEmpty"
builtins.bool"builtins.bool*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*î
saveAsNewAPIHadoopDataset)pyspark.rdd.RDD.saveAsNewAPIHadoopDataset"
None*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*
confu
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *·
saveAsNewAPIHadoopFile&pyspark.rdd.RDD.saveAsNewAPIHadoopFile"
None*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
path
builtins.str"builtins.str*3
outputFormatClass
builtins.str"builtins.str*T
keyClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *V

valueClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *∆
confπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *à
saveAsHadoopDataset#pyspark.rdd.RDD.saveAsHadoopDataset"
None*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*
confu
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *∏
saveAsHadoopFile pyspark.rdd.RDD.saveAsHadoopFile"
None*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
path
builtins.str"builtins.str*3
outputFormatClass
builtins.str"builtins.str*T
keyClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *V

valueClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *∆
confπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *a
compressionCodecClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *⁄
saveAsSequenceFile"pyspark.rdd.RDD.saveAsSequenceFile"
None*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
path
builtins.str"builtins.str*a
compressionCodecClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *°
saveAsPickleFile pyspark.rdd.RDD.saveAsPickleFile"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
path
builtins.str"builtins.str*-
	batchSize
builtins.int"builtins.int *—
saveAsTextFilepyspark.rdd.RDD.saveAsTextFile"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
path
builtins.str"builtins.str*a
compressionCodecClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *á
collectAsMappyspark.rdd.RDD.collectAsMap"À
*builtins.dict[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"builtins.dict*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*§
keyspyspark.rdd.RDD.keys"y
pyspark.rdd.RDD[pyspark.rdd.K]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*®
valuespyspark.rdd.RDD.values"y
pyspark.rdd.RDD[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Õ
reduceByKeypyspark.rdd.RDD.reduceByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *Í
reduceByKeyLocally"pyspark.rdd.RDD.reduceByKeyLocally"À
*builtins.dict[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"builtins.dict*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*ÿ

countByKeypyspark.rdd.RDD.countByKey"†
)builtins.dict[pyspark.rdd.K,builtins.int]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable
builtins.int"builtins.int"builtins.dict*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ø
joinpyspark.rdd.RDD.join"ö
Hpyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.rdd.V,pyspark.rdd.U]]]º
7Tuple[pyspark.rdd.K,Tuple[pyspark.rdd.V,pyspark.rdd.U]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable∂
"Tuple[pyspark.rdd.V,pyspark.rdd.U]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ã
otherˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *é	
leftOuterJoinpyspark.rdd.RDD.leftOuterJoin"Á
Tpyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.rdd.V,Union[pyspark.rdd.U,None]]]]˝
CTuple[pyspark.rdd.K,Tuple[pyspark.rdd.V,Union[pyspark.rdd.U,None]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableÎ
.Tuple[pyspark.rdd.V,Union[pyspark.rdd.U,None]]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.objecto
Union[pyspark.rdd.U,None]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object
None"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ã
otherˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *ê	
rightOuterJoinpyspark.rdd.RDD.rightOuterJoin"Á
Tpyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[Union[pyspark.rdd.V,None],pyspark.rdd.U]]]˝
CTuple[pyspark.rdd.K,Tuple[Union[pyspark.rdd.V,None],pyspark.rdd.U]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableÎ
.Tuple[Union[pyspark.rdd.V,None],pyspark.rdd.U]o
Union[pyspark.rdd.V,None]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object
NoneF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ã
otherˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *€	
fullOuterJoinpyspark.rdd.RDD.fullOuterJoin"¥
`pyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[Union[pyspark.rdd.V,None],Union[pyspark.rdd.U,None]]]]æ
OTuple[pyspark.rdd.K,Tuple[Union[pyspark.rdd.V,None],Union[pyspark.rdd.U,None]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable†
:Tuple[Union[pyspark.rdd.V,None],Union[pyspark.rdd.U,None]]o
Union[pyspark.rdd.V,None]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object
Noneo
Union[pyspark.rdd.U,None]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object
None"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ã
otherˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *Ù
partitionBypyspark.rdd.RDD.partitionBy"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*W
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None*`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *ó
combineByKeypyspark.rdd.RDD.combineByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*_
createCombinerK
CallableType[builtins.function]&
builtins.function"builtins.function*[

mergeValueK
CallableType[builtins.function]&
builtins.function"builtins.function*_
mergeCombinersK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *à
aggregateByKeypyspark.rdd.RDD.aggregateByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
	zeroValueF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object*X
seqFuncK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
combFuncK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *†
	foldByKeypyspark.rdd.RDD.foldByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
	zeroValueF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *ÿ
_memory_limitpyspark.rdd.RDD._memory_limit"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*…

groupByKeypyspark.rdd.RDD.groupByKey"‘
Dpyspark.rdd.RDD[Tuple[pyspark.rdd.K,typing.Iterable[pyspark.rdd.V]]]˙
3Tuple[pyspark.rdd.K,typing.Iterable[pyspark.rdd.V]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashabley
typing.Iterable[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"typing.Iterable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *ë
flatMapValuespyspark.rdd.RDD.flatMapValues"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*â
	mapValuespyspark.rdd.RDD.mapValues"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*·
cogrouppyspark.rdd.RDD.cogroup"∆
ñpyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.U]]]]ô
ÖTuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.U]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashableƒ
pTuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.U]]•
4pyspark.resultiterable.ResultIterable[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterable•
4pyspark.resultiterable.ResultIterable[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ã
otherˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *”
sampleByKeypyspark.rdd.RDD.sampleByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*3
withReplacement
builtins.bool"builtins.bool*ê
	fractionsÄ
?builtins.dict[pyspark.rdd.K,Union[builtins.float,builtins.int]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashablef
"Union[builtins.float,builtins.int] 
builtins.float"builtins.float
builtins.int"builtins.int"builtins.dict*P
seedD
Union[builtins.int,None]
builtins.int"builtins.int
None *“
subtractByKeypyspark.rdd.RDD.subtractByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*∑
other´
)pyspark.rdd.RDD[Tuple[pyspark.rdd.K,Any]]m
Tuple[pyspark.rdd.K,Any]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable
Any"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *á
subtractpyspark.rdd.RDD.subtract"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ñ
othery
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *˙
keyBypyspark.rdd.RDD.keyBy"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.T]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.T]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*‹
repartitionpyspark.rdd.RDD.repartition"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*/
numPartitions
builtins.int"builtins.int*Ö
coalescepyspark.rdd.RDD.coalesce"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*/
numPartitions
builtins.int"builtins.int*-
shuffle
builtins.bool"builtins.bool *©
zippyspark.rdd.RDD.zip"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.T,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ñ
othery
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*à
zipWithIndexpyspark.rdd.RDD.zipWithIndex"”
2pyspark.rdd.RDD[Tuple[pyspark.rdd.T,builtins.int]]ã
!Tuple[pyspark.rdd.T,builtins.int]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object
builtins.int"builtins.int"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*é
zipWithUniqueIdpyspark.rdd.RDD.zipWithUniqueId"”
2pyspark.rdd.RDD[Tuple[pyspark.rdd.T,builtins.int]]ã
!Tuple[pyspark.rdd.T,builtins.int]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object
builtins.int"builtins.int"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ó
namepyspark.rdd.RDD.name"D
Union[builtins.str,None]
builtins.str"builtins.str
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*À
setNamepyspark.rdd.RDD.setName"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
name
builtins.str"builtins.str*Ü
toDebugStringpyspark.rdd.RDD.toDebugString"J
Union[builtins.bytes,None] 
builtins.bytes"builtins.bytes
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ü
getStorageLevelpyspark.rdd.RDD.getStorageLevel"F
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ó
_defaultReducePartitions(pyspark.rdd.RDD._defaultReducePartitions"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ı
lookuppyspark.rdd.RDD.lookup"u
builtins.list[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"builtins.list*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*O
keyF
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable*œ
_to_java_object_rdd#pyspark.rdd.RDD._to_java_object_rdd"
Any*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*≥
countApproxpyspark.rdd.RDD.countApprox"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*)
timeout
builtins.int"builtins.int*2

confidence 
builtins.float"builtins.float *˜
	sumApproxpyspark.rdd.RDD.sumApprox"4
pyspark.rdd.BoundedFloat"pyspark.rdd.BoundedFloat*π
selfÆ
3pyspark.rdd.RDD[Union[builtins.float,builtins.int]]f
"Union[builtins.float,builtins.int] 
builtins.float"builtins.float
builtins.int"builtins.int"pyspark.rdd.RDD*)
timeout
builtins.int"builtins.int*2

confidence 
builtins.float"builtins.float *˘

meanApproxpyspark.rdd.RDD.meanApprox"4
pyspark.rdd.BoundedFloat"pyspark.rdd.BoundedFloat*π
selfÆ
3pyspark.rdd.RDD[Union[builtins.float,builtins.int]]f
"Union[builtins.float,builtins.int] 
builtins.float"builtins.float
builtins.int"builtins.int"pyspark.rdd.RDD*)
timeout
builtins.int"builtins.int*2

confidence 
builtins.float"builtins.float *í
countApproxDistinct#pyspark.rdd.RDD.countApproxDistinct"
builtins.int"builtins.int*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*2

relativeSD 
builtins.float"builtins.float *Ì
toLocalIteratorpyspark.rdd.RDD.toLocalIterator"y
typing.Iterator[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"typing.Iterator*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*8
prefetchPartitions
builtins.bool"builtins.bool *≤
barrierpyspark.rdd.RDD.barrier"á
%pyspark.rdd.RDDBarrier[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDBarrier*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*÷
_is_barrierpyspark.rdd.RDD._is_barrier"
builtins.bool"builtins.bool*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*í
withResourcespyspark.rdd.RDD.withResources"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*a
profileT
(pyspark.resource.profile.ResourceProfile"(pyspark.resource.profile.ResourceProfile*ﬂ
getResourceProfile"pyspark.rdd.RDD.getResourceProfile"ò
4Union[pyspark.resource.profile.ResourceProfile,None]T
(pyspark.resource.profile.ResourceProfile"(pyspark.resource.profile.ResourceProfile
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD2À
"repartitionAndSortWithinPartitions2pyspark.rdd.RDD.repartitionAndSortWithinPartitionsË
"repartitionAndSortWithinPartitions2pyspark.rdd.RDD.repartitionAndSortWithinPartitions"æ
7pyspark.rdd.RDD[Tuple[pyspark._typing.S,pyspark.rdd.V]]Ò
&Tuple[pyspark._typing.S,pyspark.rdd.V]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrderingF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*…
selfæ
7pyspark.rdd.RDD[Tuple[pyspark._typing.S,pyspark.rdd.V]]Ò
&Tuple[pyspark._typing.S,pyspark.rdd.V]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrderingF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function */
	ascending
builtins.bool"builtins.bool 0:typing.overloadXæ
"repartitionAndSortWithinPartitions2pyspark.rdd.RDD.repartitionAndSortWithinPartitions"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*W
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None*^
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function*-
	ascending
builtins.bool"builtins.bool*X
keyfuncK
CallableType[builtins.function]&
builtins.function"builtins.function0:typing.overloadXƒ
"repartitionAndSortWithinPartitions2pyspark.rdd.RDD.repartitionAndSortWithinPartitions"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function */
	ascending
builtins.bool"builtins.bool *X
keyfuncK
CallableType[builtins.function]&
builtins.function"builtins.function0:typing.overloadX2¯
	sortByKeypyspark.rdd.RDD.sortByKeyï
	sortByKeypyspark.rdd.RDD.sortByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*…
selfæ
7pyspark.rdd.RDD[Tuple[pyspark._typing.S,pyspark.rdd.V]]Ò
&Tuple[pyspark._typing.S,pyspark.rdd.V]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrderingF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*/
	ascending
builtins.bool"builtins.bool *Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None 0:typing.overloadXÑ
	sortByKeypyspark.rdd.RDD.sortByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*-
	ascending
builtins.bool"builtins.bool*/
numPartitions
builtins.int"builtins.int*X
keyfuncK
CallableType[builtins.function]&
builtins.function"builtins.function0:typing.overloadX∞
	sortByKeypyspark.rdd.RDD.sortByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*/
	ascending
builtins.bool"builtins.bool *Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *X
keyfuncK
CallableType[builtins.function]&
builtins.function"builtins.function0:typing.overloadX2„
maxpyspark.rdd.RDD.max
maxpyspark.rdd.RDD.max"}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering*ø
self¥
"pyspark.rdd.RDD[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"pyspark.rdd.RDD0:typing.overloadX”
maxpyspark.rdd.RDD.max"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*T
keyK
CallableType[builtins.function]&
builtins.function"builtins.function0:typing.overloadX2„
minpyspark.rdd.RDD.min
minpyspark.rdd.RDD.min"}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering*ø
self¥
"pyspark.rdd.RDD[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"pyspark.rdd.RDD0:typing.overloadX”
minpyspark.rdd.RDD.min"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*T
keyK
CallableType[builtins.function]&
builtins.function"builtins.function0:typing.overloadX2î
toppyspark.rdd.RDD.topÀ
toppyspark.rdd.RDD.top"∞
 builtins.list[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"builtins.list*ø
self¥
"pyspark.rdd.RDD[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"pyspark.rdd.RDD*%
num
builtins.int"builtins.int0:typing.overloadX©
toppyspark.rdd.RDD.top"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*%
num
builtins.int"builtins.int*T
keyK
CallableType[builtins.function]&
builtins.function"builtins.function0:typing.overloadX2ƒ
takeOrderedpyspark.rdd.RDD.takeOrdered€
takeOrderedpyspark.rdd.RDD.takeOrdered"∞
 builtins.list[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"builtins.list*ø
self¥
"pyspark.rdd.RDD[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"pyspark.rdd.RDD*%
num
builtins.int"builtins.int0:typing.overloadXπ
takeOrderedpyspark.rdd.RDD.takeOrdered"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*%
num
builtins.int"builtins.int*T
keyK
CallableType[builtins.function]&
builtins.function"builtins.function0:typing.overloadX2∞0
	groupWithpyspark.rdd.RDD.groupWithß
	groupWithpyspark.rdd.RDD.groupWith"À
ópyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]]]]ù
ÜTuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable«
qTuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]]•
4pyspark.resultiterable.ResultIterable[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]G
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*é
otherÇ
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V1]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V1]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD0:typing.overloadX¸
	groupWithpyspark.rdd.RDD.groupWith"ò	
Õpyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2]]]]¥
ºTuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable®
ßTuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2]]•
4pyspark.resultiterable.ResultIterable[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]G
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V2]G
pyspark.rdd.V2"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*é
otherÇ
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V1]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V1]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ÖÇ
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V2]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V2]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V2"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD0:typing.overloadXﬁ
	groupWithpyspark.rdd.RDD.groupWith"‰
Épyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2],pyspark.resultiterable.ResultIterable[pyspark.rdd.V3]]]] 	
ÚTuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2],pyspark.resultiterable.ResultIterable[pyspark.rdd.V3]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashableà
›Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2],pyspark.resultiterable.ResultIterable[pyspark.rdd.V3]]•
4pyspark.resultiterable.ResultIterable[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]G
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V2]G
pyspark.rdd.V2"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V3]G
pyspark.rdd.V3"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*é
otherÇ
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V1]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V1]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*å
_o1Ç
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V2]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V2]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V2"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*å
_o2Ç
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V3]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V3]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V3"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD0:typing.overloadX2£
toDFpyspark.rdd.RDD.toDFı
toDFpyspark.rdd.RDD.toDF"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*†
selfï
,pyspark.rdd.RDD[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*˚
schemaÏ
DUnion[builtins.list[builtins.str],builtins.tuple[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuple
None *]
sampleRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None 0:typing.overloadX…
toDFpyspark.rdd.RDD.toDF"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*†
selfï
,pyspark.rdd.RDD[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Æ
schemaü
5Union[pyspark.sql.types.StructType,builtins.str,None]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
builtins.str"builtins.str
None 0:typing.overloadX¿
toDFpyspark.rdd.RDD.toDF"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*®
selfù
0pyspark.rdd.RDD[pyspark.sql._typing.AtomicValue]X
pyspark.sql._typing.AtomicValue"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ù
schemaê
0Union[pyspark.sql.types.AtomicType,builtins.str]<
pyspark.sql.types.AtomicType"pyspark.sql.types.AtomicType
builtins.str"builtins.str0:typing.overloadXPr'
_jrddpyspark.rdd.RDD._jrdd
AnyrF
	is_cachedpyspark.rdd.RDD.is_cached
builtins.bool"builtins.boolrR
is_checkpointedpyspark.rdd.RDD.is_checkpointed
builtins.bool"builtins.boolr\
has_resource_profile$pyspark.rdd.RDD.has_resource_profile
builtins.bool"builtins.boolrX
ctxpyspark.rdd.RDD.ctx<
pyspark.context.SparkContext"pyspark.context.SparkContextrz
_jrdd_deserializer"pyspark.rdd.RDD._jrdd_deserializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializerr#
_idpyspark.rdd.RDD._id
Anyrë
partitionerpyspark.rdd.RDD.partitionere
#Union[pyspark.rdd.Partitioner,None]2
pyspark.rdd.Partitioner"pyspark.rdd.Partitioner
NoneÏ

RDDBarrierpyspark.rdd.RDDBarrier"builtins.object*œ
__init__pyspark.rdd.RDDBarrier.__init__"
None*í
selfá
%pyspark.rdd.RDDBarrier[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDBarrier*Ç
rddy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*÷
mapPartitions$pyspark.rdd.RDDBarrier.mapPartitions"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*í
selfá
%pyspark.rdd.RDDBarrier[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDBarrier*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *Ë
mapPartitionsWithIndex-pyspark.rdd.RDDBarrier.mapPartitionsWithIndex"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*í
selfá
%pyspark.rdd.RDDBarrier[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDBarrier*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool Prú
rddpyspark.rdd.RDDBarrier.rddy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD◊
PipelinedRDDpyspark.rdd.PipelinedRDD"pyspark.rdd.RDD*ı
__init__!pyspark.rdd.PipelinedRDD.__init__"
None*Ï
self·
5pyspark.rdd.PipelinedRDD[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.PipelinedRDD*É
prevy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *3
isFromBarrier
builtins.bool"builtins.bool * 
getNumPartitions)pyspark.rdd.PipelinedRDD.getNumPartitions"
builtins.int"builtins.int*Ï
self·
5pyspark.rdd.PipelinedRDD[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.PipelinedRDD*∂
_jrddpyspark.rdd.PipelinedRDD._jrdd"
Any*Ï
self·
5pyspark.rdd.PipelinedRDD[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.PipelinedRDD0:builtins.property`*Æ
idpyspark.rdd.PipelinedRDD.id"
builtins.int"builtins.int*Ï
self·
5pyspark.rdd.PipelinedRDD[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.PipelinedRDD* 
_is_pipelinable(pyspark.rdd.PipelinedRDD._is_pipelinable"
builtins.bool"builtins.bool*Ï
self·
5pyspark.rdd.PipelinedRDD[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.PipelinedRDD*¬
_is_barrier$pyspark.rdd.PipelinedRDD._is_barrier"
builtins.bool"builtins.bool*Ï
self·
5pyspark.rdd.PipelinedRDD[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.PipelinedRDDPrr
funcpyspark.rdd.PipelinedRDD.funcK
CallableType[builtins.function]&
builtins.function"builtins.functionrg
preservesPartitioning.pyspark.rdd.PipelinedRDD.preservesPartitioning
builtins.bool"builtins.boolr:

_prev_jrdd#pyspark.rdd.PipelinedRDD._prev_jrdd
Anyrç
_prev_jrdd_deserializer0pyspark.rdd.PipelinedRDD._prev_jrdd_deserializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializerr†
prevpyspark.rdd.PipelinedRDD.prevy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDrW
	_jrdd_val"pyspark.rdd.PipelinedRDD._jrdd_val&
Union[Any,None]
Any
Nonera
_bypass_serializer+pyspark.rdd.PipelinedRDD._bypass_serializer
builtins.bool"builtins.boolrQ

is_barrier#pyspark.rdd.PipelinedRDD.is_barrier
builtins.bool"builtins.boolÒ
PyLocalIterable@271pyspark.rdd.PyLocalIterable@271"builtins.object*¯
__init__(pyspark.rdd.PyLocalIterable@271.__init__"
None*L
selfB
pyspark.rdd.PyLocalIterable@271"pyspark.rdd.PyLocalIterable@271*

_sock_info
Any*Q
_serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer*¨
__iter__(pyspark.rdd.PyLocalIterable@271.__iter__"0
typing.Iterator[Any]
Any"typing.Iterator*DB
pyspark.rdd.PyLocalIterable@271"pyspark.rdd.PyLocalIterable@271*Ç
__del__'pyspark.rdd.PyLocalIterable@271.__del__"
None*DB
pyspark.rdd.PyLocalIterable@271"pyspark.rdd.PyLocalIterable@271rS
jsocket_auth_server3pyspark.rdd.PyLocalIterable@271.jsocket_auth_server
Anyr^
	_sockfile)pyspark.rdd.PyLocalIterable@271._sockfile&
io.BufferedRWPair"io.BufferedRWPairr|
_serializer+pyspark.rdd.PyLocalIterable@271._serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializerrj

_read_iter*pyspark.rdd.PyLocalIterable@271._read_iter0
typing.Iterator[Any]
Any"typing.IteratorrZ
_read_status,pyspark.rdd.PyLocalIterable@271._read_status
builtins.int"builtins.ints
portable_hashpyspark.rdd.portable_hash"
builtins.int"builtins.int*)
x"
typing.Hashable"typing.Hashablex
_create_local_socket pyspark.rdd._create_local_socket"&
io.BufferedRWPair"io.BufferedRWPair*
	sock_info
AnyŒ
_load_from_socketpyspark.rdd._load_from_socket"0
typing.Iterator[Any]
Any"typing.Iterator*
	sock_info
Any*P

serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer‚
_local_iterator_from_socket'pyspark.rdd._local_iterator_from_socket"0
typing.Iterator[Any]
Any"typing.Iterator*
	sock_info
Any*P

serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer˛
_prepare_for_python_RDD#pyspark.rdd._prepare_for_python_RDD"b
!Tuple[builtins.bytes,Any,Any,Any] 
builtins.bytes"builtins.bytes
Any
Any
Any*D
sc<
pyspark.context.SparkContext"pyspark.context.SparkContext*
command
Anyü
_wrap_functionpyspark.rdd._wrap_function"
Any*D
sc<
pyspark.context.SparkContext"pyspark.context.SparkContext*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*
deserializer
Any*

serializer
Any*
profiler
Any $
_testpyspark.rdd._test"
None*á
__annotations__pyspark.rdd.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*-

JavaObjectpyspark.rdd.JavaObject
Any*+
	JavaArraypyspark.rdd.JavaArray
Any*j
__all__pyspark.rdd.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list