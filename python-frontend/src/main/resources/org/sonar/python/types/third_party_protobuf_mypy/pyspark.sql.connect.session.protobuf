
pyspark.sql.connect.session•©
PySparkSession pyspark.sql.session.SparkSession"2pyspark.sql.pandas.conversion.SparkConversionMixin*˝
builder(pyspark.sql.session.SparkSession.builder"T
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*M
clsD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:!pyspark.sql.session.classproperty*Ñ
__init__)pyspark.sql.session.SparkSession.__init__"
None*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*N
sparkContext<
pyspark.context.SparkContext"pyspark.context.SparkContext*;
jsparkSession&
Union[Any,None]
Any
None *f
optionsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict *©
_repr_html_,pyspark.sql.session.SparkSession._repr_html_"
builtins.str"builtins.str*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*°
_jconf'pyspark.sql.session.SparkSession._jconf"
Any*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*œ

newSession+pyspark.sql.session.SparkSession.newSession"D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*ï
getActiveSession1pyspark.sql.session.SparkSession.getActiveSession"Ä
,Union[pyspark.sql.session.SparkSession,None]D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession
None*
clsv
&Type[pyspark.sql.session.SparkSession]D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession"type0:builtins.classmethod:0pyspark.sql.utils.try_remote_session_classmethodp*ƒ
active'pyspark.sql.session.SparkSession.active"D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*
clsv
&Type[pyspark.sql.session.SparkSession]D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession"type0:builtins.classmethod:0pyspark.sql.utils.try_remote_session_classmethodp*‚
sparkContext-pyspark.sql.session.SparkSession.sparkContext"<
pyspark.context.SparkContext"pyspark.context.SparkContext*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*∏
version(pyspark.sql.session.SparkSession.version"
builtins.str"builtins.str*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*÷
conf%pyspark.sql.session.SparkSession.conf"@
pyspark.sql.conf.RuntimeConfig"pyspark.sql.conf.RuntimeConfig*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*÷
catalog(pyspark.sql.session.SparkSession.catalog":
pyspark.sql.catalog.Catalog"pyspark.sql.catalog.Catalog*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*÷
udf$pyspark.sql.session.SparkSession.udf"B
pyspark.sql.udf.UDFRegistration"pyspark.sql.udf.UDFRegistration*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*‹
udtf%pyspark.sql.session.SparkSession.udtf"F
!pyspark.sql.udtf.UDTFRegistration"!pyspark.sql.udtf.UDTFRegistration*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*¬
range&pyspark.sql.session.SparkSession.range"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*'
start
builtins.int"builtins.int*O
endD
Union[builtins.int,None]
builtins.int"builtins.int
None *(
step
builtins.int"builtins.int *Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *©
_inferSchemaFromList5pyspark.sql.session.SparkSession._inferSchemaFromList"<
pyspark.sql.types.StructType"pyspark.sql.types.StructType*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*:
data0
typing.Iterable[Any]
Any"typing.Iterable*è
namesÅ
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None *˘
_inferSchema-pyspark.sql.session.SparkSession._inferSchema"<
pyspark.sql.types.StructType"pyspark.sql.types.StructType*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*9
rdd0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD*_
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None *è
namesÅ
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None *á
_createFromRDD/pyspark.sql.session.SparkSession._createFromRDD"Û
HTuple[pyspark.rdd.RDD[builtins.tuple[Any]],pyspark.sql.types.StructType]g
$pyspark.rdd.RDD[builtins.tuple[Any]].
builtins.tuple[Any]
Any"builtins.tuple"pyspark.rdd.RDD<
pyspark.sql.types.StructType"pyspark.sql.types.StructType*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*9
rdd0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD*„
schema÷
BUnion[pyspark.sql.types.DataType,builtins.list[builtins.str],None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataTypeJ
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None*]
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None*≠
_createFromLocal1pyspark.sql.session.SparkSession._createFromLocal"Û
HTuple[pyspark.rdd.RDD[builtins.tuple[Any]],pyspark.sql.types.StructType]g
$pyspark.rdd.RDD[builtins.tuple[Any]].
builtins.tuple[Any]
Any"builtins.tuple"pyspark.rdd.RDD<
pyspark.sql.types.StructType"pyspark.sql.types.StructType*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*:
data0
typing.Iterable[Any]
Any"typing.Iterable*„
schema÷
BUnion[pyspark.sql.types.DataType,builtins.list[builtins.str],None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataTypeJ
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None*∞
_create_shell_session6pyspark.sql.session.SparkSession._create_shell_session"D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.staticmethodh*“
_getActiveSessionOrCreate:pyspark.sql.session.SparkSession._getActiveSessionOrCreate"D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*
static_conf
Any0:builtins.staticmethodh*¯
_create_dataframe2pyspark.sql.session.SparkSession._create_dataframe"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*£
dataò
0Union[pyspark.rdd.RDD[Any],typing.Iterable[Any]]0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD0
typing.Iterable[Any]
Any"typing.Iterable*„
schema÷
BUnion[pyspark.sql.types.DataType,builtins.list[builtins.str],None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataTypeJ
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None*]
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None*0
verifySchema
builtins.bool"builtins.bool*„
sql$pyspark.sql.session.SparkSession.sql"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession**
sqlQuery
builtins.str"builtins.str*‡
args”
>Union[builtins.dict[builtins.str,Any],builtins.list[Any],None]W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict,
builtins.list[Any]
Any"builtins.list
None *
kwargs
Any*
table&pyspark.sql.session.SparkSession.table"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*+
	tableName
builtins.str"builtins.str*Ê
read%pyspark.sql.session.SparkSession.read"P
&pyspark.sql.readwriter.DataFrameReader"&pyspark.sql.readwriter.DataFrameReader*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*à

readStream+pyspark.sql.session.SparkSession.readStream"f
1pyspark.sql.streaming.readwriter.DataStreamReader"1pyspark.sql.streaming.readwriter.DataStreamReader*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*Ç
streams(pyspark.sql.session.SparkSession.streams"f
1pyspark.sql.streaming.query.StreamingQueryManager"1pyspark.sql.streaming.query.StreamingQueryManager*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*á
stop%pyspark.sql.session.SparkSession.stop"
None*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*≈
	__enter__*pyspark.sql.session.SparkSession.__enter__"D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*FD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*‡
__exit__)pyspark.sql.session.SparkSession.__exit__"
None*FD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*ìê
(Union[Type[builtins.BaseException],None]X
Type[builtins.BaseException]0
builtins.BaseException"builtins.BaseException"type
None*db
"Union[builtins.BaseException,None]0
builtins.BaseException"builtins.BaseException
None*[Y
Union[types.TracebackType,None]*
types.TracebackType"types.TracebackType
None*Ç
client'pyspark.sql.session.SparkSession.client"h
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession0:builtins.property`*»
addArtifacts-pyspark.sql.session.SparkSession.addArtifacts"
None*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*&
path
builtins.str"builtins.str*,
pyfile
builtins.bool"builtins.bool *-
archive
builtins.bool"builtins.bool **
file
builtins.bool"builtins.bool *¸
copyFromLocalToFs2pyspark.sql.session.SparkSession.copyFromLocalToFs"
None*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*,

local_path
builtins.str"builtins.str*+
	dest_path
builtins.str"builtins.str*Ÿ
interruptAll-pyspark.sql.session.SparkSession.interruptAll"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*Ä
interruptTag-pyspark.sql.session.SparkSession.interruptTag"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*%
tag
builtins.str"builtins.str*é
interruptOperation3pyspark.sql.session.SparkSession.interruptOperation"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*'
op_id
builtins.str"builtins.str*≤
addTag'pyspark.sql.session.SparkSession.addTag"
None*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*%
tag
builtins.str"builtins.str*∏
	removeTag*pyspark.sql.session.SparkSession.removeTag"
None*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*%
tag
builtins.str"builtins.str*Õ
getTags(pyspark.sql.session.SparkSession.getTags"H
builtins.set[builtins.str]
builtins.str"builtins.str"builtins.set*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*ë
	clearTags*pyspark.sql.session.SparkSession.clearTags"
None*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession2Œ&
createDataFrame0pyspark.sql.session.SparkSession.createDataFrameﬂ
createDataFrame0pyspark.sql.session.SparkSession.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*†
dataï
,typing.Iterable[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"typing.Iterable*Ï
schema›
?Union[builtins.list[builtins.str],builtins.tuple[builtins.str]]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuple *_
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None 0:typing.overloadXﬂ
createDataFrame0pyspark.sql.session.SparkSession.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*†
dataï
,pyspark.rdd.RDD[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ï
schema›
?Union[builtins.list[builtins.str],builtins.tuple[builtins.str]]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuple *_
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None 0:typing.overloadX„
createDataFrame0pyspark.sql.session.SparkSession.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*†
dataï
,typing.Iterable[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"typing.Iterable*ù
schemaê
0Union[pyspark.sql.types.StructType,builtins.str]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:typing.overloadX„
createDataFrame0pyspark.sql.session.SparkSession.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*†
dataï
,pyspark.rdd.RDD[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ù
schemaê
0Union[pyspark.sql.types.StructType,builtins.str]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:typing.overloadXÎ
createDataFrame0pyspark.sql.session.SparkSession.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*®
dataù
0pyspark.rdd.RDD[pyspark.sql._typing.AtomicValue]X
pyspark.sql._typing.AtomicValue"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ù
schemaê
0Union[pyspark.sql.types.AtomicType,builtins.str]<
pyspark.sql.types.AtomicType"pyspark.sql.types.AtomicType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:typing.overloadXÎ
createDataFrame0pyspark.sql.session.SparkSession.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*®
dataù
0typing.Iterable[pyspark.sql._typing.AtomicValue]X
pyspark.sql._typing.AtomicValue"
builtins.object"builtins.object"builtins.object"typing.Iterable*ù
schemaê
0Union[pyspark.sql.types.AtomicType,builtins.str]<
pyspark.sql.types.AtomicType"pyspark.sql.types.AtomicType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:typing.overloadXì
createDataFrame0pyspark.sql.session.SparkSession.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*D
data:
pandas.core.frame.DataFrame"pandas.core.frame.DataFrame*_
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None 0:typing.overloadXÜ
createDataFrame0pyspark.sql.session.SparkSession.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*N
selfD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*D
data:
pandas.core.frame.DataFrame"pandas.core.frame.DataFrame*ù
schemaê
0Union[pyspark.sql.types.StructType,builtins.str]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:typing.overloadXr–
_instantiatedSession5pyspark.sql.session.SparkSession._instantiatedSessionÄ
,Union[pyspark.sql.session.SparkSession,None]D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession
Nonerƒ
_activeSession/pyspark.sql.session.SparkSession._activeSessionÄ
,Union[pyspark.sql.session.SparkSession,None]D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession
Nonerà
addArtifact,pyspark.sql.session.SparkSession.addArtifactK
CallableType[builtins.function]&
builtins.function"builtins.functionri
_sc$pyspark.sql.session.SparkSession._sc<
pyspark.context.SparkContext"pyspark.context.SparkContextr6
_jsc%pyspark.sql.session.SparkSession._jsc
AnyrU
_jvm%pyspark.sql.session.SparkSession._jvm&
Union[Any,None]
Any
Nonerq
_conf&pyspark.sql.session.SparkSession._conf@
pyspark.sql.conf.RuntimeConfig"pyspark.sql.conf.RuntimeConfigrq
_catalog)pyspark.sql.session.SparkSession._catalog:
pyspark.sql.catalog.Catalog"pyspark.sql.catalog.Catalogzê
Builder(pyspark.sql.session.SparkSession.Builder"builtins.object*ß
__init__1pyspark.sql.session.SparkSession.Builder.__init__"
None*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*√
_validate_startup_urls?pyspark.sql.session.SparkSession.Builder._validate_startup_urls"
None*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*ô
master/pyspark.sql.session.SparkSession.Builder.master"T
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*(
master
builtins.str"builtins.str*ñ
remote/pyspark.sql.session.SparkSession.Builder.remote"T
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*%
url
builtins.str"builtins.str*ô
appName0pyspark.sql.session.SparkSession.Builder.appName"T
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*&
name
builtins.str"builtins.str*Ö
enableHiveSupport:pyspark.sql.session.SparkSession.Builder.enableHiveSupport"T
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*È
getOrCreate4pyspark.sql.session.SparkSession.Builder.getOrCreate"D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*ﬂ
create/pyspark.sql.session.SparkSession.Builder.create"D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder2∫
config/pyspark.sql.session.SparkSession.Builder.config¿
config/pyspark.sql.session.SparkSession.Builder.config"T
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*:
conf0
pyspark.conf.SparkConf"pyspark.conf.SparkConf0:typing.overloadXø
config/pyspark.sql.session.SparkSession.Builder.config"T
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*%
key
builtins.str"builtins.str*
value
Any0:typing.overloadX˘
config/pyspark.sql.session.SparkSession.Builder.config"T
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*^
selfT
(pyspark.sql.session.SparkSession.Builder"(pyspark.sql.session.SparkSession.Builder*Ú
mapË
ibuiltins.dict[builtins.str,TypeAlias[Union[builtins.bool,builtins.float,builtins.int,builtins.str,None]]]
builtins.str"builtins.strÕ
MTypeAlias[Union[builtins.bool,builtins.float,builtins.int,builtins.str,None]]Œ
BUnion[builtins.bool,builtins.float,builtins.int,builtins.str,None]
builtins.bool"builtins.bool 
builtins.float"builtins.float
builtins.int"builtins.int
builtins.str"builtins.str
None")pyspark.sql._typing.OptionalPrimitiveType"builtins.dict0:typing.overloadXr]
_lock.pyspark.sql.session.SparkSession.Builder._lock$
threading._RLock"threading._RLockrñ
_options1pyspark.sql.session.SparkSession.Builder._optionsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict—É
SparkSession(pyspark.sql.connect.session.SparkSession"builtins.object*•
builder0pyspark.sql.connect.session.SparkSession.builder"d
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*]
clsT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:!pyspark.sql.session.classproperty*’
__init__1pyspark.sql.connect.session.SparkSession.__init__"
None*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*◊

connection∆
BUnion[builtins.str,pyspark.sql.connect.client.core.ChannelBuilder]
builtins.str"builtins.str`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder*R
userIdD
Union[builtins.str,None]
builtins.str"builtins.str
None *ç
_set_default_and_active_sessionHpyspark.sql.connect.session.SparkSession._set_default_and_active_session"
None*ò
clsé
.Type[pyspark.sql.connect.session.SparkSession]T
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession"type*a
sessionT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.classmethodp*ù
getActiveSession9pyspark.sql.connect.session.SparkSession.getActiveSession"ò
4Union[pyspark.sql.connect.session.SparkSession,None]T
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession
None*ò
clsé
.Type[pyspark.sql.connect.session.SparkSession]T
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession"type0:builtins.classmethodp*ƒ
active/pyspark.sql.connect.session.SparkSession.active"T
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*ò
clsé
.Type[pyspark.sql.connect.session.SparkSession]T
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession"type0:builtins.classmethodp*ò
table.pyspark.sql.connect.session.SparkSession.table"R
'pyspark.sql.connect.dataframe.DataFrame"'pyspark.sql.connect.dataframe.DataFrame*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*+
	tableName
builtins.str"builtins.str*é
read-pyspark.sql.connect.session.SparkSession.read"`
.pyspark.sql.connect.readwriter.DataFrameReader".pyspark.sql.connect.readwriter.DataFrameReader*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`*∞

readStream3pyspark.sql.connect.session.SparkSession.readStream"v
9pyspark.sql.connect.streaming.readwriter.DataStreamReader"9pyspark.sql.connect.streaming.readwriter.DataStreamReader*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`*¡
_inferSchemaFromList=pyspark.sql.connect.session.SparkSession._inferSchemaFromList"<
pyspark.sql.types.StructType"pyspark.sql.types.StructType*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*:
data0
typing.Iterable[Any]
Any"typing.Iterable*è
namesÅ
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None *Ë
createDataFrame8pyspark.sql.connect.session.SparkSession.createDataFrame"R
'pyspark.sql.connect.dataframe.DataFrame"'pyspark.sql.connect.dataframe.DataFrame*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*Ü
data˚
NUnion[pandas.core.frame.DataFrame,numpy.ndarray[Any,Any],typing.Iterable[Any]]:
pandas.core.frame.DataFrame"pandas.core.frame.DataFrame9
numpy.ndarray[Any,Any]
Any
Any"numpy.ndarray0
typing.Iterable[Any]
Any"typing.Iterable*›
schemaŒ
ãUnion[pyspark.sql.types.AtomicType,pyspark.sql.types.StructType,builtins.str,builtins.list[builtins.str],builtins.tuple[builtins.str],None]<
pyspark.sql.types.AtomicType"pyspark.sql.types.AtomicType<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
builtins.str"builtins.strJ
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuple
None *ˆ
sql,pyspark.sql.connect.session.SparkSession.sql"R
'pyspark.sql.connect.dataframe.DataFrame"'pyspark.sql.connect.dataframe.DataFrame*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession**
sqlQuery
builtins.str"builtins.str*‡
args”
>Union[builtins.dict[builtins.str,Any],builtins.list[Any],None]W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict,
builtins.list[Any]
Any"builtins.list
None *Í
range.pyspark.sql.connect.session.SparkSession.range"R
'pyspark.sql.connect.dataframe.DataFrame"'pyspark.sql.connect.dataframe.DataFrame*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*'
start
builtins.int"builtins.int*O
endD
Union[builtins.int,None]
builtins.int"builtins.int
None *(
step
builtins.int"builtins.int *Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *˛
catalog0pyspark.sql.connect.session.SparkSession.catalog"J
#pyspark.sql.connect.catalog.Catalog"#pyspark.sql.connect.catalog.Catalog*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`*ù
__del__0pyspark.sql.connect.session.SparkSession.__del__"
None*VT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*Ò
interruptAll5pyspark.sql.connect.session.SparkSession.interruptAll"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*ò
interruptTag5pyspark.sql.connect.session.SparkSession.interruptTag"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*%
tag
builtins.str"builtins.str*¶
interruptOperation;pyspark.sql.connect.session.SparkSession.interruptOperation"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*'
op_id
builtins.str"builtins.str* 
addTag/pyspark.sql.connect.session.SparkSession.addTag"
None*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*%
tag
builtins.str"builtins.str*–
	removeTag2pyspark.sql.connect.session.SparkSession.removeTag"
None*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*%
tag
builtins.str"builtins.str*Â
getTags0pyspark.sql.connect.session.SparkSession.getTags"H
builtins.set[builtins.str]
builtins.str"builtins.str"builtins.set*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*©
	clearTags2pyspark.sql.connect.session.SparkSession.clearTags"
None*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*ü
stop-pyspark.sql.connect.session.SparkSession.stop"
None*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*ÿ

is_stopped3pyspark.sql.connect.session.SparkSession.is_stopped"
builtins.bool"builtins.bool*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`*˙
conf-pyspark.sql.connect.session.SparkSession.conf"L
$pyspark.sql.connect.conf.RuntimeConf"$pyspark.sql.connect.conf.RuntimeConf*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`*™
streams0pyspark.sql.connect.session.SparkSession.streams"v
9pyspark.sql.connect.streaming.query.StreamingQueryManager"9pyspark.sql.connect.streaming.query.StreamingQueryManager*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`*ƒ
__getattr__4pyspark.sql.connect.session.SparkSession.__getattr__"
Any*VT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*
builtins.str"builtins.str*˛
udf,pyspark.sql.connect.session.SparkSession.udf"R
'pyspark.sql.connect.udf.UDFRegistration"'pyspark.sql.connect.udf.UDFRegistration*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`*Ñ
udtf-pyspark.sql.connect.session.SparkSession.udtf"V
)pyspark.sql.connect.udtf.UDTFRegistration")pyspark.sql.connect.udtf.UDTFRegistration*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`*–
version0pyspark.sql.connect.session.SparkSession.version"
builtins.str"builtins.str*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`*ö
client/pyspark.sql.connect.session.SparkSession.client"h
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`*‡
addArtifacts5pyspark.sql.connect.session.SparkSession.addArtifacts"
None*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*&
path
builtins.str"builtins.str*,
pyfile
builtins.bool"builtins.bool *-
archive
builtins.bool"builtins.bool **
file
builtins.bool"builtins.bool *ª
_cache_local_relation>pyspark.sql.connect.session.SparkSession._cache_local_relation"
builtins.str"builtins.str*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*d
local_relationP
&pyspark.sql.connect.plan.LocalRelation"&pyspark.sql.connect.plan.LocalRelation*î
copyFromLocalToFs:pyspark.sql.connect.session.SparkSession.copyFromLocalToFs"
None*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*,

local_path
builtins.str"builtins.str*+
	dest_path
builtins.str"builtins.str*æ
_create_remote_dataframeApyspark.sql.connect.session.SparkSession._create_remote_dataframe"R
'pyspark.sql.connect.dataframe.DataFrame"'pyspark.sql.connect.dataframe.DataFrame*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*+
	remote_id
builtins.str"builtins.str*â
_start_connect_server>pyspark.sql.connect.session.SparkSession._start_connect_server"
None*(
master
builtins.str"builtins.str*a
optsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict0:builtins.staticmethodh*÷

session_id3pyspark.sql.connect.session.SparkSession.session_id"
builtins.str"builtins.str*^
selfT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession0:builtins.property`ro
_active_session8pyspark.sql.connect.session.SparkSession._active_session"
threading.local"threading.localrË
_default_session9pyspark.sql.connect.session.SparkSession._default_sessionò
4Union[pyspark.sql.connect.session.SparkSession,None]T
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession
Noner]
_lock.pyspark.sql.connect.session.SparkSession._lock$
threading._RLock"threading._RLockr•
_client0pyspark.sql.connect.session.SparkSession._clienth
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClientrê
addArtifact4pyspark.sql.connect.session.SparkSession.addArtifactK
CallableType[builtins.function]&
builtins.function"builtins.functionra
_session_id4pyspark.sql.connect.session.SparkSession._session_id
builtins.str"builtins.strrâ
_catalog1pyspark.sql.connect.session.SparkSession._catalogJ
#pyspark.sql.connect.catalog.Catalog"#pyspark.sql.connect.catalog.Catalogz‰#
Builder0pyspark.sql.connect.session.SparkSession.Builder"builtins.object*ø
__init__9pyspark.sql.connect.session.SparkSession.Builder.__init__"
None*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*¡
master7pyspark.sql.connect.session.SparkSession.Builder.master"d
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*(
master
builtins.str"builtins.str*¡
appName8pyspark.sql.connect.session.SparkSession.Builder.appName"d
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*&
name
builtins.str"builtins.str*≈
remote7pyspark.sql.connect.session.SparkSession.Builder.remote"d
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*,
location
builtins.str"builtins.str *ù
channelBuilder?pyspark.sql.connect.session.SparkSession.Builder.channelBuilder"d
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*t
channelBuilder`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder*≠
enableHiveSupportBpyspark.sql.connect.session.SparkSession.Builder.enableHiveSupport"d
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*Æ
_apply_options?pyspark.sql.connect.session.SparkSession.Builder._apply_options"
None*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*a
sessionT
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*á
create7pyspark.sql.connect.session.SparkSession.Builder.create"T
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*ë
getOrCreate<pyspark.sql.connect.session.SparkSession.Builder.getOrCreate"T
(pyspark.sql.connect.session.SparkSession"(pyspark.sql.connect.session.SparkSession*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder2◊	
config7pyspark.sql.connect.session.SparkSession.Builder.configÁ
config7pyspark.sql.connect.session.SparkSession.Builder.config"d
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*%
key
builtins.str"builtins.str*
value
Any0:typing.overloadX©
config7pyspark.sql.connect.session.SparkSession.Builder.config"d
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*n
selfd
0pyspark.sql.connect.session.SparkSession.Builder"0pyspark.sql.connect.session.SparkSession.Builder*˙
map
ibuiltins.dict[builtins.str,TypeAlias[Union[builtins.bool,builtins.float,builtins.int,builtins.str,None]]]
builtins.str"builtins.str’
MTypeAlias[Union[builtins.bool,builtins.float,builtins.int,builtins.str,None]]Œ
BUnion[builtins.bool,builtins.float,builtins.int,builtins.str,None]
builtins.bool"builtins.bool 
builtins.float"builtins.float
builtins.int"builtins.int
builtins.str"builtins.str
None"1pyspark.sql.connect._typing.OptionalPrimitiveType"builtins.dict0:typing.overloadXre
_lock6pyspark.sql.connect.session.SparkSession.Builder._lock$
threading._RLock"threading._RLockrû
_options9pyspark.sql.connect.session.SparkSession.Builder._optionsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictrÇ
_channel_builderApyspark.sql.connect.session.SparkSession.Builder._channel_builder™
:Union[pyspark.sql.connect.client.core.ChannelBuilder,None]`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder
None4
_test!pyspark.sql.connect.session._test"
None*ó
__annotations__+pyspark.sql.connect.session.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*
npnumpy *
pdpandas *-
papyspark.sql.connect.session.pa
Any