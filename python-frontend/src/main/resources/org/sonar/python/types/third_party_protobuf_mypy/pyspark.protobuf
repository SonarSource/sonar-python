
pyspark≈
	SparkConfpyspark.conf.SparkConf"builtins.object*è
__init__pyspark.conf.SparkConf.__init__"
None*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*2
loadDefaults
builtins.bool"builtins.bool *2
_jvm&
Union[Any,None]
Any
None *4
_jconf&
Union[Any,None]
Any
None *ﬂ
setpyspark.conf.SparkConf.set"0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*%
key
builtins.str"builtins.str*'
value
builtins.str"builtins.str*Ò
setIfMissing#pyspark.conf.SparkConf.setIfMissing"0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*%
key
builtins.str"builtins.str*'
value
builtins.str"builtins.str*ƒ
	setMaster pyspark.conf.SparkConf.setMaster"0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*'
value
builtins.str"builtins.str*∆

setAppName!pyspark.conf.SparkConf.setAppName"0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*'
value
builtins.str"builtins.str* 
setSparkHome#pyspark.conf.SparkConf.setSparkHome"0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*'
value
builtins.str"builtins.str*∆
setAllpyspark.conf.SparkConf.setAll"0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*Æ
pairs¢
/builtins.list[Tuple[builtins.str,builtins.str]]`
 Tuple[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.list*à
getAllpyspark.conf.SparkConf.getAll"¢
/builtins.list[Tuple[builtins.str,builtins.str]]`
 Tuple[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.list*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*Æ
containspyspark.conf.SparkConf.contains"
builtins.bool"builtins.bool*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*%
key
builtins.str"builtins.str*è
toDebugString$pyspark.conf.SparkConf.toDebugString"
builtins.str"builtins.str*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf2§
setExecutorEnv%pyspark.conf.SparkConf.setExecutorEnvÉ
setExecutorEnv%pyspark.conf.SparkConf.setExecutorEnv"0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*%
key
builtins.str"builtins.str*'
value
builtins.str"builtins.str0:overloadX‰
setExecutorEnv%pyspark.conf.SparkConf.setExecutorEnv"0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*Æ
pairs¢
/builtins.list[Tuple[builtins.str,builtins.str]]`
 Tuple[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.list0:overloadX2÷
getpyspark.conf.SparkConf.getÿ
getpyspark.conf.SparkConf.get"D
Union[builtins.str,None]
builtins.str"builtins.str
None*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*%
key
builtins.str"builtins.str0:overloadXÙ
getpyspark.conf.SparkConf.get"D
Union[builtins.str,None]
builtins.str"builtins.str
None*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*%
key
builtins.str"builtins.str*
defaultValue
None0:overloadX‡
getpyspark.conf.SparkConf.get"
builtins.str"builtins.str*:
self0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*%
key
builtins.str"builtins.str*.
defaultValue
builtins.str"builtins.str0:overloadXrO
_jconfpyspark.conf.SparkConf._jconf&
Union[Any,None]
Any
Noner·
_confpyspark.conf.SparkConf._confπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None¢ô
RDDpyspark.rdd.RDD"builtins.object*Ô
__init__pyspark.rdd.RDD.__init__"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*
jrdd
Any*E
ctx<
pyspark.context.SparkContext"pyspark.context.SparkContext*Y
jrdd_deserializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer *•
_pickledpyspark.rdd.RDD._pickled"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*¬
idpyspark.rdd.RDD.id"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*∆
__repr__pyspark.rdd.RDD.__repr__"
builtins.str"builtins.str*Å
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD* 
__getnewargs__pyspark.rdd.RDD.__getnewargs__"
NoReturn
*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*˙
contextpyspark.rdd.RDD.context"<
pyspark.context.SparkContext"pyspark.context.SparkContext*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD0:property`*ü
cachepyspark.rdd.RDD.cache"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ˇ
persistpyspark.rdd.RDD.persist"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Z
storageLevelF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel *◊
	unpersistpyspark.rdd.RDD.unpersist"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*.
blocking
builtins.bool"builtins.bool *æ

checkpointpyspark.rdd.RDD.checkpoint"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*‹
isCheckpointedpyspark.rdd.RDD.isCheckpointed"
builtins.bool"builtins.bool*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*»
localCheckpointpyspark.rdd.RDD.localCheckpoint"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Í
isLocallyCheckpointed%pyspark.rdd.RDD.isLocallyCheckpointed"
builtins.bool"builtins.bool*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*à
getCheckpointFile!pyspark.rdd.RDD.getCheckpointFile"D
Union[builtins.str,None]
builtins.str"builtins.str
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
cleanShuffleDependencies(pyspark.rdd.RDD.cleanShuffleDependencies"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*.
blocking
builtins.bool"builtins.bool *¨
mappyspark.rdd.RDD.map"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *¥
flatMappyspark.rdd.RDD.flatMap"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *¿
mapPartitionspyspark.rdd.RDD.mapPartitions"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *“
mapPartitionsWithIndex&pyspark.rdd.RDD.mapPartitionsWithIndex"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *“
mapPartitionsWithSplit&pyspark.rdd.RDD.mapPartitionsWithSplit"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *ﬁ
getNumPartitions pyspark.rdd.RDD.getNumPartitions"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ı
filterpyspark.rdd.RDD.filter"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*Ä
distinctpyspark.rdd.RDD.distinct"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *ÿ
samplepyspark.rdd.RDD.sample"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*3
withReplacement
builtins.bool"builtins.bool*.
fraction 
builtins.float"builtins.float*P
seedD
Union[builtins.int,None]
builtins.int"builtins.int
None *˝
randomSplitpyspark.rdd.RDD.randomSplit"π
-builtins.list[pyspark.rdd.RDD[pyspark.rdd.T]]y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*º
weightsÆ
3typing.Sequence[Union[builtins.int,builtins.float]]f
"Union[builtins.int,builtins.float]
builtins.int"builtins.int 
builtins.float"builtins.float"typing.Sequence*P
seedD
Union[builtins.int,None]
builtins.int"builtins.int
None *”

takeSamplepyspark.rdd.RDD.takeSample"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*3
withReplacement
builtins.bool"builtins.bool*%
num
builtins.int"builtins.int*P
seedD
Union[builtins.int,None]
builtins.int"builtins.int
None *ò
_computeFractionForSampleSize-pyspark.rdd.RDD._computeFractionForSampleSize" 
builtins.float"builtins.float*6
sampleSizeLowerBound
builtins.int"builtins.int*'
total
builtins.int"builtins.int*3
withReplacement
builtins.bool"builtins.bool0:staticmethodh*≠
unionpyspark.rdd.RDD.union"ˇ
3pyspark.rdd.RDD[Union[pyspark.rdd.T,pyspark.rdd.U]]∂
"Union[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ñ
othery
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*¥
intersectionpyspark.rdd.RDD.intersection"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ñ
othery
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*º
_reserializepyspark.rdd.RDD._reserialize"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*å

serializerz
*Union[pyspark.serializers.Serializer,None]@
pyspark.serializers.Serializer"pyspark.serializers.Serializer
None *û
__add__pyspark.rdd.RDD.__add__"ˇ
3pyspark.rdd.RDD[Union[pyspark.rdd.T,pyspark.rdd.U]]∂
"Union[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*{y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*{y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*á
sortBypyspark.rdd.RDD.sortBy"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*X
keyfuncK
CallableType[builtins.function]&
builtins.function"builtins.function*/
	ascending
builtins.bool"builtins.bool *Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *‹
glompyspark.rdd.RDD.glom"∑
-pyspark.rdd.RDD[builtins.list[pyspark.rdd.T]]u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*µ
	cartesianpyspark.rdd.RDD.cartesian"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.T,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ñ
othery
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ê
groupBypyspark.rdd.RDD.groupBy"‘
Dpyspark.rdd.RDD[Tuple[pyspark.rdd.K,typing.Iterable[pyspark.rdd.T]]]˙
3Tuple[pyspark.rdd.K,typing.Iterable[pyspark.rdd.T]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashabley
typing.Iterable[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"typing.Iterable"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *ú
pipepyspark.rdd.RDD.pipe"N
pyspark.rdd.RDD[builtins.str]
builtins.str"builtins.str"pyspark.rdd.RDD*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*)
command
builtins.str"builtins.str*≈
envπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None */
	checkCode
builtins.bool"builtins.bool *Ü
foreachpyspark.rdd.RDD.foreach"
None*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*ò
foreachPartition pyspark.rdd.RDD.foreachPartition"
None*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*ü
collectpyspark.rdd.RDD.collect"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD* 
collectWithJobGroup#pyspark.rdd.RDD.collectWithJobGroup"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*)
groupId
builtins.str"builtins.str*-
description
builtins.str"builtins.str*7
interruptOnCancel
builtins.bool"builtins.bool *¬
reducepyspark.rdd.RDD.reduce"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*ı

treeReducepyspark.rdd.RDD.treeReduce"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*)
depth
builtins.int"builtins.int *ñ
foldpyspark.rdd.RDD.fold"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
	zeroValueF
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*S
opK
CallableType[builtins.function]&
builtins.function"builtins.function*¸
	aggregatepyspark.rdd.RDD.aggregate"F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
	zeroValueF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object*V
seqOpK
CallableType[builtins.function]&
builtins.function"builtins.function*W
combOpK
CallableType[builtins.function]&
builtins.function"builtins.function*Ø
treeAggregatepyspark.rdd.RDD.treeAggregate"F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
	zeroValueF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object*V
seqOpK
CallableType[builtins.function]&
builtins.function"builtins.function*W
combOpK
CallableType[builtins.function]&
builtins.function"builtins.function*)
depth
builtins.int"builtins.int *ô
sumpyspark.rdd.RDD.sum"V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*»
countpyspark.rdd.RDD.count"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*â
statspyspark.rdd.RDD.stats"B
pyspark.statcounter.StatCounter"pyspark.statcounter.StatCounter*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*®
	histogrampyspark.rdd.RDD.histogram"Ã
ETuple[typing.Sequence[pyspark._typing.S],builtins.list[builtins.int]]¥
"typing.Sequence[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"typing.SequenceJ
builtins.list[builtins.int]
builtins.int"builtins.int"builtins.list*ø
self¥
"pyspark.rdd.RDD[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"pyspark.rdd.RDD*Ó
buckets‡
VUnion[builtins.int,builtins.list[pyspark._typing.S],builtins.tuple[pyspark._typing.S]]
builtins.int"builtins.int∞
 builtins.list[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"builtins.list≤
!builtins.tuple[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"builtins.tuple*Â
meanpyspark.rdd.RDD.mean" 
builtins.float"builtins.float*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ì
variancepyspark.rdd.RDD.variance" 
builtins.float"builtins.float*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Á
stdevpyspark.rdd.RDD.stdev" 
builtins.float"builtins.float*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Û
sampleStdevpyspark.rdd.RDD.sampleStdev" 
builtins.float"builtins.float*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*˘
sampleVariancepyspark.rdd.RDD.sampleVariance" 
builtins.float"builtins.float*§
selfô
.pyspark.rdd.RDD[pyspark._typing.NumberOrArray]V
pyspark._typing.NumberOrArray"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*’
countByValuepyspark.rdd.RDD.countByValue"†
)builtins.dict[pyspark.rdd.K,builtins.int]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable
builtins.int"builtins.int"builtins.dict*É
selfy
pyspark.rdd.RDD[pyspark.rdd.K]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable"pyspark.rdd.RDD*¿
takepyspark.rdd.RDD.take"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*%
num
builtins.int"builtins.int*Ï
firstpyspark.rdd.RDD.first"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Œ
isEmptypyspark.rdd.RDD.isEmpty"
builtins.bool"builtins.bool*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*î
saveAsNewAPIHadoopDataset)pyspark.rdd.RDD.saveAsNewAPIHadoopDataset"
None*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*
confu
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *·
saveAsNewAPIHadoopFile&pyspark.rdd.RDD.saveAsNewAPIHadoopFile"
None*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
path
builtins.str"builtins.str*3
outputFormatClass
builtins.str"builtins.str*T
keyClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *V

valueClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *∆
confπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *à
saveAsHadoopDataset#pyspark.rdd.RDD.saveAsHadoopDataset"
None*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*
confu
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *∏
saveAsHadoopFile pyspark.rdd.RDD.saveAsHadoopFile"
None*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
path
builtins.str"builtins.str*3
outputFormatClass
builtins.str"builtins.str*T
keyClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *V

valueClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *∆
confπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *a
compressionCodecClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *⁄
saveAsSequenceFile"pyspark.rdd.RDD.saveAsSequenceFile"
None*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
path
builtins.str"builtins.str*a
compressionCodecClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *°
saveAsPickleFile pyspark.rdd.RDD.saveAsPickleFile"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
path
builtins.str"builtins.str*-
	batchSize
builtins.int"builtins.int *—
saveAsTextFilepyspark.rdd.RDD.saveAsTextFile"
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
path
builtins.str"builtins.str*a
compressionCodecClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *á
collectAsMappyspark.rdd.RDD.collectAsMap"À
*builtins.dict[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"builtins.dict*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*§
keyspyspark.rdd.RDD.keys"y
pyspark.rdd.RDD[pyspark.rdd.K]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*®
valuespyspark.rdd.RDD.values"y
pyspark.rdd.RDD[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Õ
reduceByKeypyspark.rdd.RDD.reduceByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *Í
reduceByKeyLocally"pyspark.rdd.RDD.reduceByKeyLocally"À
*builtins.dict[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"builtins.dict*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*ÿ

countByKeypyspark.rdd.RDD.countByKey"†
)builtins.dict[pyspark.rdd.K,builtins.int]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable
builtins.int"builtins.int"builtins.dict*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ø
joinpyspark.rdd.RDD.join"ö
Hpyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.rdd.V,pyspark.rdd.U]]]º
7Tuple[pyspark.rdd.K,Tuple[pyspark.rdd.V,pyspark.rdd.U]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable∂
"Tuple[pyspark.rdd.V,pyspark.rdd.U]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ã
otherˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *é	
leftOuterJoinpyspark.rdd.RDD.leftOuterJoin"Á
Tpyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.rdd.V,Union[pyspark.rdd.U,None]]]]˝
CTuple[pyspark.rdd.K,Tuple[pyspark.rdd.V,Union[pyspark.rdd.U,None]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableÎ
.Tuple[pyspark.rdd.V,Union[pyspark.rdd.U,None]]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.objecto
Union[pyspark.rdd.U,None]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object
None"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ã
otherˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *ê	
rightOuterJoinpyspark.rdd.RDD.rightOuterJoin"Á
Tpyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[Union[pyspark.rdd.V,None],pyspark.rdd.U]]]˝
CTuple[pyspark.rdd.K,Tuple[Union[pyspark.rdd.V,None],pyspark.rdd.U]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableÎ
.Tuple[Union[pyspark.rdd.V,None],pyspark.rdd.U]o
Union[pyspark.rdd.V,None]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object
NoneF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ã
otherˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *€	
fullOuterJoinpyspark.rdd.RDD.fullOuterJoin"¥
`pyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[Union[pyspark.rdd.V,None],Union[pyspark.rdd.U,None]]]]æ
OTuple[pyspark.rdd.K,Tuple[Union[pyspark.rdd.V,None],Union[pyspark.rdd.U,None]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable†
:Tuple[Union[pyspark.rdd.V,None],Union[pyspark.rdd.U,None]]o
Union[pyspark.rdd.V,None]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object
Noneo
Union[pyspark.rdd.U,None]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object
None"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ã
otherˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *Ù
partitionBypyspark.rdd.RDD.partitionBy"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*W
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None*`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *ó
combineByKeypyspark.rdd.RDD.combineByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*_
createCombinerK
CallableType[builtins.function]&
builtins.function"builtins.function*[

mergeValueK
CallableType[builtins.function]&
builtins.function"builtins.function*_
mergeCombinersK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *à
aggregateByKeypyspark.rdd.RDD.aggregateByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
	zeroValueF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object*X
seqFuncK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
combFuncK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *†
	foldByKeypyspark.rdd.RDD.foldByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*U
	zeroValueF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *ÿ
_memory_limitpyspark.rdd.RDD._memory_limit"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*…

groupByKeypyspark.rdd.RDD.groupByKey"‘
Dpyspark.rdd.RDD[Tuple[pyspark.rdd.K,typing.Iterable[pyspark.rdd.V]]]˙
3Tuple[pyspark.rdd.K,typing.Iterable[pyspark.rdd.V]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashabley
typing.Iterable[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"typing.Iterable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function *ë
flatMapValuespyspark.rdd.RDD.flatMapValues"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*â
	mapValuespyspark.rdd.RDD.mapValues"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*·
cogrouppyspark.rdd.RDD.cogroup"∆
ñpyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.U]]]]ô
ÖTuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.U]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashableƒ
pTuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.U]]•
4pyspark.resultiterable.ResultIterable[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterable•
4pyspark.resultiterable.ResultIterable[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ã
otherˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.U]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *”
sampleByKeypyspark.rdd.RDD.sampleByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*3
withReplacement
builtins.bool"builtins.bool*ê
	fractionsÄ
?builtins.dict[pyspark.rdd.K,Union[builtins.float,builtins.int]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashablef
"Union[builtins.float,builtins.int] 
builtins.float"builtins.float
builtins.int"builtins.int"builtins.dict*P
seedD
Union[builtins.int,None]
builtins.int"builtins.int
None *“
subtractByKeypyspark.rdd.RDD.subtractByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*∑
other´
)pyspark.rdd.RDD[Tuple[pyspark.rdd.K,Any]]m
Tuple[pyspark.rdd.K,Any]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable
Any"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *á
subtractpyspark.rdd.RDD.subtract"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ñ
othery
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *˙
keyBypyspark.rdd.RDD.keyBy"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.T]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.T]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*‹
repartitionpyspark.rdd.RDD.repartition"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*/
numPartitions
builtins.int"builtins.int*Ö
coalescepyspark.rdd.RDD.coalesce"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*/
numPartitions
builtins.int"builtins.int*-
shuffle
builtins.bool"builtins.bool *©
zippyspark.rdd.RDD.zip"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.T,pyspark.rdd.U]]∂
"Tuple[pyspark.rdd.T,pyspark.rdd.U]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.objectF
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ñ
othery
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*à
zipWithIndexpyspark.rdd.RDD.zipWithIndex"”
2pyspark.rdd.RDD[Tuple[pyspark.rdd.T,builtins.int]]ã
!Tuple[pyspark.rdd.T,builtins.int]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object
builtins.int"builtins.int"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*é
zipWithUniqueIdpyspark.rdd.RDD.zipWithUniqueId"”
2pyspark.rdd.RDD[Tuple[pyspark.rdd.T,builtins.int]]ã
!Tuple[pyspark.rdd.T,builtins.int]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object
builtins.int"builtins.int"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ó
namepyspark.rdd.RDD.name"D
Union[builtins.str,None]
builtins.str"builtins.str
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*À
setNamepyspark.rdd.RDD.setName"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*&
name
builtins.str"builtins.str*Ü
toDebugStringpyspark.rdd.RDD.toDebugString"J
Union[builtins.bytes,None] 
builtins.bytes"builtins.bytes
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ü
getStorageLevelpyspark.rdd.RDD.getStorageLevel"F
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Ó
_defaultReducePartitions(pyspark.rdd.RDD._defaultReducePartitions"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ı
lookuppyspark.rdd.RDD.lookup"u
builtins.list[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"builtins.list*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*O
keyF
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable*œ
_to_java_object_rdd#pyspark.rdd.RDD._to_java_object_rdd"
Any*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*≥
countApproxpyspark.rdd.RDD.countApprox"
builtins.int"builtins.int*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*)
timeout
builtins.int"builtins.int*2

confidence 
builtins.float"builtins.float *˜
	sumApproxpyspark.rdd.RDD.sumApprox"4
pyspark.rdd.BoundedFloat"pyspark.rdd.BoundedFloat*π
selfÆ
3pyspark.rdd.RDD[Union[builtins.float,builtins.int]]f
"Union[builtins.float,builtins.int] 
builtins.float"builtins.float
builtins.int"builtins.int"pyspark.rdd.RDD*)
timeout
builtins.int"builtins.int*2

confidence 
builtins.float"builtins.float *˘

meanApproxpyspark.rdd.RDD.meanApprox"4
pyspark.rdd.BoundedFloat"pyspark.rdd.BoundedFloat*π
selfÆ
3pyspark.rdd.RDD[Union[builtins.float,builtins.int]]f
"Union[builtins.float,builtins.int] 
builtins.float"builtins.float
builtins.int"builtins.int"pyspark.rdd.RDD*)
timeout
builtins.int"builtins.int*2

confidence 
builtins.float"builtins.float *í
countApproxDistinct#pyspark.rdd.RDD.countApproxDistinct"
builtins.int"builtins.int*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*2

relativeSD 
builtins.float"builtins.float *Ì
toLocalIteratorpyspark.rdd.RDD.toLocalIterator"y
typing.Iterator[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"typing.Iterator*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*8
prefetchPartitions
builtins.bool"builtins.bool *≤
barrierpyspark.rdd.RDD.barrier"á
%pyspark.rdd.RDDBarrier[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDBarrier*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*÷
_is_barrierpyspark.rdd.RDD._is_barrier"
builtins.bool"builtins.bool*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*í
withResourcespyspark.rdd.RDD.withResources"y
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*a
profileT
(pyspark.resource.profile.ResourceProfile"(pyspark.resource.profile.ResourceProfile*ﬂ
getResourceProfile"pyspark.rdd.RDD.getResourceProfile"ò
4Union[pyspark.resource.profile.ResourceProfile,None]T
(pyspark.resource.profile.ResourceProfile"(pyspark.resource.profile.ResourceProfile
None*â
self
!pyspark.rdd.RDD[pyspark.rdd.T_co]I
pyspark.rdd.T_co"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD2∂
"repartitionAndSortWithinPartitions2pyspark.rdd.RDD.repartitionAndSortWithinPartitions·
"repartitionAndSortWithinPartitions2pyspark.rdd.RDD.repartitionAndSortWithinPartitions"æ
7pyspark.rdd.RDD[Tuple[pyspark._typing.S,pyspark.rdd.V]]Ò
&Tuple[pyspark._typing.S,pyspark.rdd.V]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrderingF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*…
selfæ
7pyspark.rdd.RDD[Tuple[pyspark._typing.S,pyspark.rdd.V]]Ò
&Tuple[pyspark._typing.S,pyspark.rdd.V]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrderingF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function */
	ascending
builtins.bool"builtins.bool 0:overloadX∑
"repartitionAndSortWithinPartitions2pyspark.rdd.RDD.repartitionAndSortWithinPartitions"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*W
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None*^
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function*-
	ascending
builtins.bool"builtins.bool*X
keyfuncK
CallableType[builtins.function]&
builtins.function"builtins.function0:overloadXΩ
"repartitionAndSortWithinPartitions2pyspark.rdd.RDD.repartitionAndSortWithinPartitions"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *`
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function */
	ascending
builtins.bool"builtins.bool *X
keyfuncK
CallableType[builtins.function]&
builtins.function"builtins.function0:overloadX2„
	sortByKeypyspark.rdd.RDD.sortByKeyé
	sortByKeypyspark.rdd.RDD.sortByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*…
selfæ
7pyspark.rdd.RDD[Tuple[pyspark._typing.S,pyspark.rdd.V]]Ò
&Tuple[pyspark._typing.S,pyspark.rdd.V]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrderingF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*/
	ascending
builtins.bool"builtins.bool *Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None 0:overloadX˝
	sortByKeypyspark.rdd.RDD.sortByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*-
	ascending
builtins.bool"builtins.bool*/
numPartitions
builtins.int"builtins.int*X
keyfuncK
CallableType[builtins.function]&
builtins.function"builtins.function0:overloadX©
	sortByKeypyspark.rdd.RDD.sortByKey"ˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*/
	ascending
builtins.bool"builtins.bool *Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *X
keyfuncK
CallableType[builtins.function]&
builtins.function"builtins.function0:overloadX2’
maxpyspark.rdd.RDD.maxÈ
maxpyspark.rdd.RDD.max"}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering*ø
self¥
"pyspark.rdd.RDD[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"pyspark.rdd.RDD0:overloadXÃ
maxpyspark.rdd.RDD.max"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*T
keyK
CallableType[builtins.function]&
builtins.function"builtins.function0:overloadX2’
minpyspark.rdd.RDD.minÈ
minpyspark.rdd.RDD.min"}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering*ø
self¥
"pyspark.rdd.RDD[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"pyspark.rdd.RDD0:overloadXÃ
minpyspark.rdd.RDD.min"F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*T
keyK
CallableType[builtins.function]&
builtins.function"builtins.function0:overloadX2Ü
toppyspark.rdd.RDD.topƒ
toppyspark.rdd.RDD.top"∞
 builtins.list[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"builtins.list*ø
self¥
"pyspark.rdd.RDD[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"pyspark.rdd.RDD*%
num
builtins.int"builtins.int0:overloadX¢
toppyspark.rdd.RDD.top"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*%
num
builtins.int"builtins.int*T
keyK
CallableType[builtins.function]&
builtins.function"builtins.function0:overloadX2∂
takeOrderedpyspark.rdd.RDD.takeOrdered‘
takeOrderedpyspark.rdd.RDD.takeOrdered"∞
 builtins.list[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"builtins.list*ø
self¥
"pyspark.rdd.RDD[pyspark._typing.S]}
pyspark._typing.SD
 pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering" pyspark._typing.SupportsOrdering"pyspark.rdd.RDD*%
num
builtins.int"builtins.int0:overloadX≤
takeOrderedpyspark.rdd.RDD.takeOrdered"u
builtins.list[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"builtins.list*É
selfy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*%
num
builtins.int"builtins.int*T
keyK
CallableType[builtins.function]&
builtins.function"builtins.function0:overloadX2õ0
	groupWithpyspark.rdd.RDD.groupWith†
	groupWithpyspark.rdd.RDD.groupWith"À
ópyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]]]]ù
ÜTuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable«
qTuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]]•
4pyspark.resultiterable.ResultIterable[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]G
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*é
otherÇ
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V1]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V1]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD0:overloadXı
	groupWithpyspark.rdd.RDD.groupWith"ò	
Õpyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2]]]]¥
ºTuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashable®
ßTuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2]]•
4pyspark.resultiterable.ResultIterable[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]G
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V2]G
pyspark.rdd.V2"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*é
otherÇ
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V1]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V1]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ÖÇ
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V2]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V2]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V2"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD0:overloadX◊
	groupWithpyspark.rdd.RDD.groupWith"‰
Épyspark.rdd.RDD[Tuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2],pyspark.resultiterable.ResultIterable[pyspark.rdd.V3]]]] 	
ÚTuple[pyspark.rdd.K,Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2],pyspark.resultiterable.ResultIterable[pyspark.rdd.V3]]]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.Hashableà
›Tuple[pyspark.resultiterable.ResultIterable[pyspark.rdd.V],pyspark.resultiterable.ResultIterable[pyspark.rdd.V1],pyspark.resultiterable.ResultIterable[pyspark.rdd.V2],pyspark.resultiterable.ResultIterable[pyspark.rdd.V3]]•
4pyspark.resultiterable.ResultIterable[pyspark.rdd.V]F
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V1]G
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V2]G
pyspark.rdd.V2"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterableß
5pyspark.resultiterable.ResultIterable[pyspark.rdd.V3]G
pyspark.rdd.V3"
builtins.object"builtins.object"builtins.object"%pyspark.resultiterable.ResultIterable"pyspark.rdd.RDD*ä
selfˇ
3pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V]]∂
"Tuple[pyspark.rdd.K,pyspark.rdd.V]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableF
pyspark.rdd.V"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*é
otherÇ
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V1]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V1]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V1"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*å
_o1Ç
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V2]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V2]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V2"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*å
_o2Ç
4pyspark.rdd.RDD[Tuple[pyspark.rdd.K,pyspark.rdd.V3]]∏
#Tuple[pyspark.rdd.K,pyspark.rdd.V3]F
pyspark.rdd.K"
typing.Hashable"typing.Hashable"typing.HashableG
pyspark.rdd.V3"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD0:overloadX2é
toDFpyspark.rdd.RDD.toDFÓ
toDFpyspark.rdd.RDD.toDF"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*†
selfï
,pyspark.rdd.RDD[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*˚
schemaÏ
DUnion[builtins.list[builtins.str],builtins.tuple[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuple
None *]
sampleRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None 0:overloadX¬
toDFpyspark.rdd.RDD.toDF"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*†
selfï
,pyspark.rdd.RDD[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*Æ
schemaü
5Union[pyspark.sql.types.StructType,builtins.str,None]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
builtins.str"builtins.str
None 0:overloadXπ
toDFpyspark.rdd.RDD.toDF"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*®
selfù
0pyspark.rdd.RDD[pyspark.sql._typing.AtomicValue]X
pyspark.sql._typing.AtomicValue"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*ù
schemaê
0Union[pyspark.sql.types.AtomicType,builtins.str]<
pyspark.sql.types.AtomicType"pyspark.sql.types.AtomicType
builtins.str"builtins.str0:overloadXPr'
_jrddpyspark.rdd.RDD._jrdd
AnyrF
	is_cachedpyspark.rdd.RDD.is_cached
builtins.bool"builtins.boolrR
is_checkpointedpyspark.rdd.RDD.is_checkpointed
builtins.bool"builtins.boolr\
has_resource_profile$pyspark.rdd.RDD.has_resource_profile
builtins.bool"builtins.boolrX
ctxpyspark.rdd.RDD.ctx<
pyspark.context.SparkContext"pyspark.context.SparkContextrz
_jrdd_deserializer"pyspark.rdd.RDD._jrdd_deserializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializerr#
_idpyspark.rdd.RDD._id
Anyrë
partitionerpyspark.rdd.RDD.partitionere
#Union[pyspark.rdd.Partitioner,None]2
pyspark.rdd.Partitioner"pyspark.rdd.Partitioner
NoneÏ

RDDBarrierpyspark.rdd.RDDBarrier"builtins.object*œ
__init__pyspark.rdd.RDDBarrier.__init__"
None*í
selfá
%pyspark.rdd.RDDBarrier[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDBarrier*Ç
rddy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*÷
mapPartitions$pyspark.rdd.RDDBarrier.mapPartitions"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*í
selfá
%pyspark.rdd.RDDBarrier[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDBarrier*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool *Ë
mapPartitionsWithIndex-pyspark.rdd.RDDBarrier.mapPartitionsWithIndex"y
pyspark.rdd.RDD[pyspark.rdd.U]F
pyspark.rdd.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*í
selfá
%pyspark.rdd.RDDBarrier[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDBarrier*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*;
preservesPartitioning
builtins.bool"builtins.bool Prú
rddpyspark.rdd.RDDBarrier.rddy
pyspark.rdd.RDD[pyspark.rdd.T]F
pyspark.rdd.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD˚

SparkFilespyspark.files.SparkFiles"builtins.object*w
__init__!pyspark.files.SparkFiles.__init__"
None*>
self4
pyspark.files.SparkFiles"pyspark.files.SparkFiles*Á
getpyspark.files.SparkFiles.get"
builtins.str"builtins.str*g
cls^
Type[pyspark.files.SparkFiles]4
pyspark.files.SparkFiles"pyspark.files.SparkFiles"type**
filename
builtins.str"builtins.str0:classmethodp*’
getRootDirectory)pyspark.files.SparkFiles.getRootDirectory"
builtins.str"builtins.str*g
cls^
Type[pyspark.files.SparkFiles]4
pyspark.files.SparkFiles"pyspark.files.SparkFiles"type0:classmethodprÅ
_root_directory(pyspark.files.SparkFiles._root_directoryD
Union[builtins.str,None]
builtins.str"builtins.str
Nonerg
_is_running_on_worker.pyspark.files.SparkFiles._is_running_on_worker
builtins.bool"builtins.boolrô
_scpyspark.files.SparkFiles._sct
(Union[pyspark.context.SparkContext,None]<
pyspark.context.SparkContext"pyspark.context.SparkContext
None∂
StatusTrackerpyspark.status.StatusTracker"builtins.object*ö
__init__%pyspark.status.StatusTracker.__init__"
None*F
self<
pyspark.status.StatusTracker"pyspark.status.StatusTracker*
jtracker
Any*≠
getJobIdsForGroup.pyspark.status.StatusTracker.getJobIdsForGroup"J
builtins.list[builtins.int]
builtins.int"builtins.int"builtins.list*F
self<
pyspark.status.StatusTracker"pyspark.status.StatusTracker*T
jobGroupD
Union[builtins.str,None]
builtins.str"builtins.str
None *◊
getActiveStageIds.pyspark.status.StatusTracker.getActiveStageIds"J
builtins.list[builtins.int]
builtins.int"builtins.int"builtins.list*F
self<
pyspark.status.StatusTracker"pyspark.status.StatusTracker*’
getActiveJobsIds-pyspark.status.StatusTracker.getActiveJobsIds"J
builtins.list[builtins.int]
builtins.int"builtins.int"builtins.list*F
self<
pyspark.status.StatusTracker"pyspark.status.StatusTracker*¥

getJobInfo'pyspark.status.StatusTracker.getJobInfo"ã
;Union[TypeAlias[Tuple[builtins.int,Any,builtins.str]],None]ø
/TypeAlias[Tuple[builtins.int,Any,builtins.str]]m
$Tuple[builtins.int,Any,builtins.str]
builtins.int"builtins.int
Any
builtins.str"builtins.str"pyspark.status.SparkJobInfo
None*F
self<
pyspark.status.StatusTracker"pyspark.status.StatusTracker*'
jobId
builtins.int"builtins.int*Å
getStageInfo)pyspark.status.StatusTracker.getStageInfo"“
xUnion[TypeAlias[Tuple[builtins.int,builtins.int,builtins.str,builtins.int,builtins.int,builtins.int,builtins.int]],None]…
lTypeAlias[Tuple[builtins.int,builtins.int,builtins.str,builtins.int,builtins.int,builtins.int,builtins.int]]∑
aTuple[builtins.int,builtins.int,builtins.str,builtins.int,builtins.int,builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int
builtins.str"builtins.str
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int"pyspark.status.SparkStageInfo
None*F
self<
pyspark.status.StatusTracker"pyspark.status.StatusTracker*)
stageId
builtins.int"builtins.intr<
	_jtracker&pyspark.status.StatusTracker._jtracker
Any≈
SparkJobInfopyspark.status.SparkJobInfo"builtins.tuple*÷
_replace$pyspark.status.SparkJobInfo._replace"í
pyspark.status.SparkJobInfo._NTm
$Tuple[builtins.int,Any,builtins.str]
builtins.int"builtins.int
Any
builtins.str"builtins.str*û
_selfí
pyspark.status.SparkJobInfo._NTm
$Tuple[builtins.int,Any,builtins.str]
builtins.int"builtins.int
Any
builtins.str"builtins.str*)
jobId
builtins.int"builtins.int *
stageIds
Any **
status
builtins.str"builtins.str *ˇ
__new__#pyspark.status.SparkJobInfo.__new__"í
pyspark.status.SparkJobInfo._NTm
$Tuple[builtins.int,Any,builtins.str]
builtins.int"builtins.int
Any
builtins.str"builtins.str*œ
_clsƒ
%Type[pyspark.status.SparkJobInfo._NT]í
pyspark.status.SparkJobInfo._NTm
$Tuple[builtins.int,Any,builtins.str]
builtins.int"builtins.int
Any
builtins.str"builtins.str"type*'
jobId
builtins.int"builtins.int*
stageIds
Any*(
status
builtins.str"builtins.str*®
_asdict#pyspark.status.SparkJobInfo._asdict"W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*û
_selfí
pyspark.status.SparkJobInfo._NTm
$Tuple[builtins.int,Any,builtins.str]
builtins.int"builtins.int
Any
builtins.str"builtins.str*ä
_make!pyspark.status.SparkJobInfo._make"í
pyspark.status.SparkJobInfo._NTm
$Tuple[builtins.int,Any,builtins.str]
builtins.int"builtins.int
Any
builtins.str"builtins.str*œ
_clsƒ
%Type[pyspark.status.SparkJobInfo._NT]í
pyspark.status.SparkJobInfo._NTm
$Tuple[builtins.int,Any,builtins.str]
builtins.int"builtins.int
Any
builtins.str"builtins.str"type*>
iterable0
typing.Iterable[Any]
Any"typing.Iterable*
new
Any *
len
Any 0:classmethodprH
jobId!pyspark.status.SparkJobInfo.jobId
builtins.int"builtins.intr9
stageIds$pyspark.status.SparkJobInfo.stageIds
AnyrJ
status"pyspark.status.SparkJobInfo.status
builtins.str"builtins.strrH
jobId!pyspark.status.SparkJobInfo.jobId
builtins.int"builtins.intr9
stageIds$pyspark.status.SparkJobInfo.stageIds
AnyrJ
status"pyspark.status.SparkJobInfo.status
builtins.str"builtins.strrº
_fields#pyspark.status.SparkJobInfo._fieldsã
-Tuple[builtins.str,builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.strrë
_field_types(pyspark.status.SparkJobInfo._field_typesW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictró
_field_defaults+pyspark.status.SparkJobInfo._field_defaultsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictrL
_source#pyspark.status.SparkJobInfo._source
builtins.str"builtins.strró
__annotations__+pyspark.status.SparkJobInfo.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict¨.
SparkStageInfopyspark.status.SparkStageInfo"builtins.tuple*‹
_replace&pyspark.status.SparkStageInfo._replace"ﬂ
!pyspark.status.SparkStageInfo._NT∑
aTuple[builtins.int,builtins.int,builtins.str,builtins.int,builtins.int,builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int
builtins.str"builtins.str
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int*Î
_selfﬂ
!pyspark.status.SparkStageInfo._NT∑
aTuple[builtins.int,builtins.int,builtins.str,builtins.int,builtins.int,builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int
builtins.str"builtins.str
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int*+
stageId
builtins.int"builtins.int *4
currentAttemptId
builtins.int"builtins.int *(
name
builtins.str"builtins.str *,
numTasks
builtins.int"builtins.int *2
numActiveTasks
builtins.int"builtins.int *5
numCompletedTasks
builtins.int"builtins.int *2
numFailedTasks
builtins.int"builtins.int *ˇ
__new__%pyspark.status.SparkStageInfo.__new__"ﬂ
!pyspark.status.SparkStageInfo._NT∑
aTuple[builtins.int,builtins.int,builtins.str,builtins.int,builtins.int,builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int
builtins.str"builtins.str
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int*û
_clsì
'Type[pyspark.status.SparkStageInfo._NT]ﬂ
!pyspark.status.SparkStageInfo._NT∑
aTuple[builtins.int,builtins.int,builtins.str,builtins.int,builtins.int,builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int
builtins.str"builtins.str
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int"type*)
stageId
builtins.int"builtins.int*2
currentAttemptId
builtins.int"builtins.int*&
name
builtins.str"builtins.str**
numTasks
builtins.int"builtins.int*0
numActiveTasks
builtins.int"builtins.int*3
numCompletedTasks
builtins.int"builtins.int*0
numFailedTasks
builtins.int"builtins.int*˜
_asdict%pyspark.status.SparkStageInfo._asdict"W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*Î
_selfﬂ
!pyspark.status.SparkStageInfo._NT∑
aTuple[builtins.int,builtins.int,builtins.str,builtins.int,builtins.int,builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int
builtins.str"builtins.str
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int*®
_make#pyspark.status.SparkStageInfo._make"ﬂ
!pyspark.status.SparkStageInfo._NT∑
aTuple[builtins.int,builtins.int,builtins.str,builtins.int,builtins.int,builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int
builtins.str"builtins.str
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int*û
_clsì
'Type[pyspark.status.SparkStageInfo._NT]ﬂ
!pyspark.status.SparkStageInfo._NT∑
aTuple[builtins.int,builtins.int,builtins.str,builtins.int,builtins.int,builtins.int,builtins.int]
builtins.int"builtins.int
builtins.int"builtins.int
builtins.str"builtins.str
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int
builtins.int"builtins.int"type*>
iterable0
typing.Iterable[Any]
Any"typing.Iterable*
new
Any *
len
Any 0:classmethodprN
stageId%pyspark.status.SparkStageInfo.stageId
builtins.int"builtins.intr`
currentAttemptId.pyspark.status.SparkStageInfo.currentAttemptId
builtins.int"builtins.intrH
name"pyspark.status.SparkStageInfo.name
builtins.str"builtins.strrP
numTasks&pyspark.status.SparkStageInfo.numTasks
builtins.int"builtins.intr\
numActiveTasks,pyspark.status.SparkStageInfo.numActiveTasks
builtins.int"builtins.intrb
numCompletedTasks/pyspark.status.SparkStageInfo.numCompletedTasks
builtins.int"builtins.intr\
numFailedTasks,pyspark.status.SparkStageInfo.numFailedTasks
builtins.int"builtins.intrN
stageId%pyspark.status.SparkStageInfo.stageId
builtins.int"builtins.intr`
currentAttemptId.pyspark.status.SparkStageInfo.currentAttemptId
builtins.int"builtins.intrH
name"pyspark.status.SparkStageInfo.name
builtins.str"builtins.strrP
numTasks&pyspark.status.SparkStageInfo.numTasks
builtins.int"builtins.intr\
numActiveTasks,pyspark.status.SparkStageInfo.numActiveTasks
builtins.int"builtins.intrb
numCompletedTasks/pyspark.status.SparkStageInfo.numCompletedTasks
builtins.int"builtins.intr\
numFailedTasks,pyspark.status.SparkStageInfo.numFailedTasks
builtins.int"builtins.intrÍ
_fields%pyspark.status.SparkStageInfo._fields∑
aTuple[builtins.str,builtins.str,builtins.str,builtins.str,builtins.str,builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.strrì
_field_types*pyspark.status.SparkStageInfo._field_typesW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictrô
_field_defaults-pyspark.status.SparkStageInfo._field_defaultsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictrN
_source%pyspark.status.SparkStageInfo._source
builtins.str"builtins.strrô
__annotations__-pyspark.status.SparkStageInfo.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictí
InheritableThreadpyspark.util.InheritableThread"threading.Thread*ä
__init__'pyspark.util.InheritableThread.__init__"
None*J
self@
pyspark.util.InheritableThread"pyspark.util.InheritableThread*W
targetK
CallableType[builtins.function]&
builtins.function"builtins.function*
args
Any*
kwargs
Any*É
start$pyspark.util.InheritableThread.start"
None*J
self@
pyspark.util.InheritableThread"pyspark.util.InheritableThreadr8
_props%pyspark.util.InheritableThread._props
Any–
StorageLevel!pyspark.storagelevel.StorageLevel"builtins.object*Å
__init__*pyspark.storagelevel.StorageLevel.__init__"
None*P
selfF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel*+
useDisk
builtins.bool"builtins.bool*-
	useMemory
builtins.bool"builtins.bool*.

useOffHeap
builtins.bool"builtins.bool*0
deserialized
builtins.bool"builtins.bool*/
replication
builtins.int"builtins.int *û
__repr__*pyspark.storagelevel.StorageLevel.__repr__"
builtins.str"builtins.str*HF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel*ú
__str__)pyspark.storagelevel.StorageLevel.__str__"
builtins.str"builtins.str*HF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel*ß
__eq__(pyspark.storagelevel.StorageLevel.__eq__"
builtins.bool"builtins.bool*HF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel*	
Anyrv
NONE&pyspark.storagelevel.StorageLevel.NONEF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevelrÄ
	DISK_ONLY+pyspark.storagelevel.StorageLevel.DISK_ONLYF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevelrÑ
DISK_ONLY_2-pyspark.storagelevel.StorageLevel.DISK_ONLY_2F
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevelrÑ
DISK_ONLY_3-pyspark.storagelevel.StorageLevel.DISK_ONLY_3F
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevelrÑ
MEMORY_ONLY-pyspark.storagelevel.StorageLevel.MEMORY_ONLYF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevelrà
MEMORY_ONLY_2/pyspark.storagelevel.StorageLevel.MEMORY_ONLY_2F
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevelrå
MEMORY_AND_DISK1pyspark.storagelevel.StorageLevel.MEMORY_AND_DISKF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevelrê
MEMORY_AND_DISK_23pyspark.storagelevel.StorageLevel.MEMORY_AND_DISK_2F
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevelr~
OFF_HEAP*pyspark.storagelevel.StorageLevel.OFF_HEAPF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevelrò
MEMORY_AND_DISK_DESER7pyspark.storagelevel.StorageLevel.MEMORY_AND_DISK_DESERF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevelrT
useDisk)pyspark.storagelevel.StorageLevel.useDisk
builtins.bool"builtins.boolrX
	useMemory+pyspark.storagelevel.StorageLevel.useMemory
builtins.bool"builtins.boolrZ

useOffHeap,pyspark.storagelevel.StorageLevel.useOffHeap
builtins.bool"builtins.boolr^
deserialized.pyspark.storagelevel.StorageLevel.deserialized
builtins.bool"builtins.boolrZ
replication-pyspark.storagelevel.StorageLevel.replication
builtins.int"builtins.intŸ 
Accumulator pyspark.accumulators.Accumulator"builtins.object*…
__init__)pyspark.accumulators.Accumulator.__init__"
None*∏
self≠
8pyspark.accumulators.Accumulator[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator*%
aid
builtins.int"builtins.int*Z
valueO
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object*…
accum_param∑
=pyspark.accumulators.AccumulatorParam[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object"%pyspark.accumulators.AccumulatorParam*

__reduce__+pyspark.accumulators.Accumulator.__reduce__"˘
èTuple[CallableType[builtins.function],Tuple[builtins.int,pyspark.accumulators.T,pyspark.accumulators.AccumulatorParam[pyspark.accumulators.T]]]K
CallableType[builtins.function]&
builtins.function"builtins.functionï
hTuple[builtins.int,pyspark.accumulators.T,pyspark.accumulators.AccumulatorParam[pyspark.accumulators.T]]
builtins.int"builtins.intO
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object∑
=pyspark.accumulators.AccumulatorParam[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object"%pyspark.accumulators.AccumulatorParam*∏
self≠
8pyspark.accumulators.Accumulator[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator*À
add$pyspark.accumulators.Accumulator.add"
None*∏
self≠
8pyspark.accumulators.Accumulator[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator*Y
termO
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object*Î
__iadd__)pyspark.accumulators.Accumulator.__iadd__"≠
8pyspark.accumulators.Accumulator[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator*∞≠
8pyspark.accumulators.Accumulator[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator*QO
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object*Ñ
__str__(pyspark.accumulators.Accumulator.__str__"
builtins.str"builtins.str*∞≠
8pyspark.accumulators.Accumulator[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator*Ü
__repr__)pyspark.accumulators.Accumulator.__repr__"
builtins.str"builtins.str*∞≠
8pyspark.accumulators.Accumulator[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator2‡
value&pyspark.accumulators.Accumulator.valueÀ
value&pyspark.accumulators.Accumulator.value"O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object*∏
self≠
8pyspark.accumulators.Accumulator[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator0:propertyX`‡
value&pyspark.accumulators.Accumulator.value"
None*∏
self≠
8pyspark.accumulators.Accumulator[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator*Z
valueO
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object0:value.setterPrI
aid$pyspark.accumulators.Accumulator.aid
builtins.int"builtins.intrı
accum_param,pyspark.accumulators.Accumulator.accum_param∑
=pyspark.accumulators.AccumulatorParam[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object"%pyspark.accumulators.AccumulatorParamrÇ
_value'pyspark.accumulators.Accumulator._valueO
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.objectr_
_deserialized.pyspark.accumulators.Accumulator._deserialized
builtins.bool"builtins.boolÑ
AccumulatorParam%pyspark.accumulators.AccumulatorParam"builtins.object*§
zero*pyspark.accumulators.AccumulatorParam.zero"O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object*¬
self∑
=pyspark.accumulators.AccumulatorParam[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object"%pyspark.accumulators.AccumulatorParam*Z
valueO
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object*é

addInPlace0pyspark.accumulators.AccumulatorParam.addInPlace"O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object*¬
self∑
=pyspark.accumulators.AccumulatorParam[pyspark.accumulators.T]O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object"%pyspark.accumulators.AccumulatorParam*[
value1O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.object*[
value2O
pyspark.accumulators.T"
builtins.object"builtins.object"builtins.objectP¯
	Broadcastpyspark.broadcast.Broadcast"builtins.object*·
dump pyspark.broadcast.Broadcast.dump"
None*®
selfù
0pyspark.broadcast.Broadcast[pyspark.broadcast.T]L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object"pyspark.broadcast.Broadcast*W
valueL
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object*)
f"
typing.BinaryIO"typing.BinaryIO*›
load_from_path*pyspark.broadcast.Broadcast.load_from_path"L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object*®
selfù
0pyspark.broadcast.Broadcast[pyspark.broadcast.T]L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object"pyspark.broadcast.Broadcast*&
path
builtins.str"builtins.str*œ
load pyspark.broadcast.Broadcast.load"L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object*®
selfù
0pyspark.broadcast.Broadcast[pyspark.broadcast.T]L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object"pyspark.broadcast.Broadcast*,
file"
typing.BinaryIO"typing.BinaryIO*±
value!pyspark.broadcast.Broadcast.value"L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object*®
selfù
0pyspark.broadcast.Broadcast[pyspark.broadcast.T]L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object"pyspark.broadcast.Broadcast0:property`*ó
	unpersist%pyspark.broadcast.Broadcast.unpersist"
None*®
selfù
0pyspark.broadcast.Broadcast[pyspark.broadcast.T]L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object"pyspark.broadcast.Broadcast*.
blocking
builtins.bool"builtins.bool *ì
destroy#pyspark.broadcast.Broadcast.destroy"
None*®
selfù
0pyspark.broadcast.Broadcast[pyspark.broadcast.T]L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object"pyspark.broadcast.Broadcast*.
blocking
builtins.bool"builtins.bool *§

__reduce__&pyspark.broadcast.Broadcast.__reduce__"¬
:Tuple[CallableType[builtins.function],Tuple[builtins.int]]K
CallableType[builtins.function]&
builtins.function"builtins.function5
Tuple[builtins.int]
builtins.int"builtins.int*®
selfù
0pyspark.broadcast.Broadcast[pyspark.broadcast.T]L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object"pyspark.broadcast.Broadcast2≈
__init__$pyspark.broadcast.Broadcast.__init__ˇ
__init__$pyspark.broadcast.Broadcast.__init__"
None*®
selfù
0pyspark.broadcast.Broadcast[pyspark.broadcast.T]L
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object"pyspark.broadcast.Broadcast*D
sc<
pyspark.context.SparkContext"pyspark.context.SparkContext*W
valueL
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.object*k
pickle_registryV
)pyspark.broadcast.BroadcastPickleRegistry")pyspark.broadcast.BroadcastPickleRegistry0:overloadXƒ
__init__$pyspark.broadcast.Broadcast.__init__"
None*R
selfH
 pyspark.broadcast.Broadcast[Any]
Any"pyspark.broadcast.Broadcast*&
path
builtins.str"builtins.str0:overloadX…
__init__$pyspark.broadcast.Broadcast.__init__"
None*R
selfH
 pyspark.broadcast.Broadcast[Any]
Any"pyspark.broadcast.Broadcast*+
	sock_file
builtins.str"builtins.str0:overloadXPrH
_path!pyspark.broadcast.Broadcast._path
builtins.str"builtins.strrú
_scpyspark.broadcast.Broadcast._sct
(Union[pyspark.context.SparkContext,None]<
pyspark.context.SparkContext"pyspark.context.SparkContext
NonerK
_python_broadcast-pyspark.broadcast.Broadcast._python_broadcast
Anyr?
_jbroadcast'pyspark.broadcast.Broadcast._jbroadcast
Anyrﬁ
_pickle_registry,pyspark.broadcast.Broadcast._pickle_registryõ
5Union[pyspark.broadcast.BroadcastPickleRegistry,None]V
)pyspark.broadcast.BroadcastPickleRegistry")pyspark.broadcast.BroadcastPickleRegistry
Nonerz
_value"pyspark.broadcast.Broadcast._valueL
pyspark.broadcast.T"
builtins.object"builtins.object"builtins.objectÚ
MarshalSerializer%pyspark.serializers.MarshalSerializer"$pyspark.serializers.FramedSerializer*G
dumps+pyspark.serializers.MarshalSerializer.dumps*
self*
obj*G
loads+pyspark.serializers.MarshalSerializer.loads*
self*
obj¥ 
TaskContextpyspark.taskcontext.TaskContext"builtins.object*Ù
__new__'pyspark.taskcontext.TaskContext.__new__"B
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext*|
clss
%Type[pyspark.taskcontext.TaskContext]B
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext"type*è
_getOrCreate,pyspark.taskcontext.TaskContext._getOrCreate"B
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext*|
clss
%Type[pyspark.taskcontext.TaskContext]B
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext"type0:classmethodp*∞
_setTaskContext/pyspark.taskcontext.TaskContext._setTaskContext"
None*|
clss
%Type[pyspark.taskcontext.TaskContext]B
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext"type*S
taskContextB
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext0:classmethodp*∏
get#pyspark.taskcontext.TaskContext.get"}
+Union[pyspark.taskcontext.TaskContext,None]B
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext
None*|
clss
%Type[pyspark.taskcontext.TaskContext]B
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext"type0:classmethodp*û
stageId'pyspark.taskcontext.TaskContext.stageId"
builtins.int"builtins.int*L
selfB
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext*¶
partitionId+pyspark.taskcontext.TaskContext.partitionId"
builtins.int"builtins.int*L
selfB
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext*™
attemptNumber-pyspark.taskcontext.TaskContext.attemptNumber"
builtins.int"builtins.int*L
selfB
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext*™
taskAttemptId-pyspark.taskcontext.TaskContext.taskAttemptId"
builtins.int"builtins.int*L
selfB
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext*ˇ
getLocalProperty0pyspark.taskcontext.TaskContext.getLocalProperty"D
Union[builtins.str,None]
builtins.str"builtins.str
None*L
selfB
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext*%
key
builtins.str"builtins.str*ò
cpus$pyspark.taskcontext.TaskContext.cpus"
builtins.int"builtins.int*L
selfB
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext*Ë
	resources)pyspark.taskcontext.TaskContext.resources"·
Lbuiltins.dict[builtins.str,pyspark.resource.information.ResourceInformation]
builtins.str"builtins.strd
0pyspark.resource.information.ResourceInformation"0pyspark.resource.information.ResourceInformation"builtins.dict*L
selfB
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContextrª
_taskContext,pyspark.taskcontext.TaskContext._taskContext}
+Union[pyspark.taskcontext.TaskContext,None]B
pyspark.taskcontext.TaskContext"pyspark.taskcontext.TaskContext
NonerÜ
_attemptNumber.pyspark.taskcontext.TaskContext._attemptNumberD
Union[builtins.int,None]
builtins.int"builtins.int
NonerÇ
_partitionId,pyspark.taskcontext.TaskContext._partitionIdD
Union[builtins.int,None]
builtins.int"builtins.int
Nonerz
_stageId(pyspark.taskcontext.TaskContext._stageIdD
Union[builtins.int,None]
builtins.int"builtins.int
NonerÜ
_taskAttemptId.pyspark.taskcontext.TaskContext._taskAttemptIdD
Union[builtins.int,None]
builtins.int"builtins.int
NonerÄ
_localProperties0pyspark.taskcontext.TaskContext._localPropertiesπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
Nonert
_cpus%pyspark.taskcontext.TaskContext._cpusD
Union[builtins.int,None]
builtins.int"builtins.int
NonerÖ

_resources*pyspark.taskcontext.TaskContext._resources 
XUnion[builtins.dict[builtins.str,pyspark.resource.information.ResourceInformation],None]·
Lbuiltins.dict[builtins.str,pyspark.resource.information.ResourceInformation]
builtins.str"builtins.strd
0pyspark.resource.information.ResourceInformation"0pyspark.resource.information.ResourceInformation"builtins.dict
Noneı
BarrierTaskContext&pyspark.taskcontext.BarrierTaskContext"pyspark.taskcontext.TaskContext*ª
_getOrCreate3pyspark.taskcontext.BarrierTaskContext._getOrCreate"P
&pyspark.taskcontext.BarrierTaskContext"&pyspark.taskcontext.BarrierTaskContext*í
clsà
,Type[pyspark.taskcontext.BarrierTaskContext]P
&pyspark.taskcontext.BarrierTaskContext"&pyspark.taskcontext.BarrierTaskContext"type0:classmethodp*©
get*pyspark.taskcontext.BarrierTaskContext.get"P
&pyspark.taskcontext.BarrierTaskContext"&pyspark.taskcontext.BarrierTaskContext*í
clsà
,Type[pyspark.taskcontext.BarrierTaskContext]P
&pyspark.taskcontext.BarrierTaskContext"&pyspark.taskcontext.BarrierTaskContext"type0:classmethodp*ñ
_initialize2pyspark.taskcontext.BarrierTaskContext._initialize"
None*í
clsà
,Type[pyspark.taskcontext.BarrierTaskContext]P
&pyspark.taskcontext.BarrierTaskContext"&pyspark.taskcontext.BarrierTaskContext"type*y
porto
%Union[builtins.str,builtins.int,None]
builtins.str"builtins.str
builtins.int"builtins.int
None*(
secret
builtins.str"builtins.str0:classmethodp*ü
barrier.pyspark.taskcontext.BarrierTaskContext.barrier"
None*Z
selfP
&pyspark.taskcontext.BarrierTaskContext"&pyspark.taskcontext.BarrierTaskContext*í
	allGather0pyspark.taskcontext.BarrierTaskContext.allGather"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*Z
selfP
&pyspark.taskcontext.BarrierTaskContext"&pyspark.taskcontext.BarrierTaskContext*+
message
builtins.str"builtins.str *±
getTaskInfos3pyspark.taskcontext.BarrierTaskContext.getTaskInfos"è
2builtins.list[pyspark.taskcontext.BarrierTaskInfo]J
#pyspark.taskcontext.BarrierTaskInfo"#pyspark.taskcontext.BarrierTaskInfo"builtins.list*Z
selfP
&pyspark.taskcontext.BarrierTaskContext"&pyspark.taskcontext.BarrierTaskContextr¶
_port,pyspark.taskcontext.BarrierTaskContext._porto
%Union[builtins.str,builtins.int,None]
builtins.str"builtins.str
builtins.int"builtins.int
Noner
_secret.pyspark.taskcontext.BarrierTaskContext._secretD
Union[builtins.str,None]
builtins.str"builtins.str
None„
BarrierTaskInfo#pyspark.taskcontext.BarrierTaskInfo"builtins.object*√
__init__,pyspark.taskcontext.BarrierTaskInfo.__init__"
None*T
selfJ
#pyspark.taskcontext.BarrierTaskInfo"#pyspark.taskcontext.BarrierTaskInfo*)
address
builtins.str"builtins.strrT
address+pyspark.taskcontext.BarrierTaskInfo.address
builtins.str"builtins.str»
Profilerpyspark.profiler.Profiler"builtins.object*¡
__init__"pyspark.profiler.Profiler.__init__"
None*@
self6
pyspark.profiler.Profiler"pyspark.profiler.Profiler*E
ctx<
pyspark.context.SparkContext"pyspark.context.SparkContext*ˆ
profile!pyspark.profiler.Profiler.profile"
Any*@
self6
pyspark.profiler.Profiler"pyspark.profiler.Profiler*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*
args
Any*
kwargs
Any*Ù
statspyspark.profiler.Profiler.stats"á
*Union[pstats.Stats,builtins.dict[Any,Any]]
pstats.Stats"pstats.Stats9
builtins.dict[Any,Any]
Any
Any"builtins.dict*@
self6
pyspark.profiler.Profiler"pyspark.profiler.Profiler*ò
showpyspark.profiler.Profiler.show"
None*@
self6
pyspark.profiler.Profiler"pyspark.profiler.Profiler*$
id
builtins.int"builtins.int*¿
dumppyspark.profiler.Profiler.dump"
None*@
self6
pyspark.profiler.Profiler"pyspark.profiler.Profiler*$
id
builtins.int"builtins.int*&
path
builtins.str"builtins.strœ	
BasicProfilerpyspark.profiler.BasicProfiler"pyspark.profiler.Profiler*–
__init__'pyspark.profiler.BasicProfiler.__init__"
None*J
self@
pyspark.profiler.BasicProfiler"pyspark.profiler.BasicProfiler*E
ctx<
pyspark.context.SparkContext"pyspark.context.SparkContext*Ö
profile&pyspark.profiler.BasicProfiler.profile"
Any*J
self@
pyspark.profiler.BasicProfiler"pyspark.profiler.BasicProfiler*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*
args
Any*
kwargs
Any*ó
stats$pyspark.profiler.BasicProfiler.stats"
pstats.Stats"pstats.Stats*J
self@
pyspark.profiler.BasicProfiler"pyspark.profiler.BasicProfiler*ß
show#pyspark.profiler.BasicProfiler.show"
None*J
self@
pyspark.profiler.BasicProfiler"pyspark.profiler.BasicProfiler*$
id
builtins.int"builtins.int*œ
dump#pyspark.profiler.BasicProfiler.dump"
None*J
self@
pyspark.profiler.BasicProfiler"pyspark.profiler.BasicProfiler*$
id
builtins.int"builtins.int*&
path
builtins.str"builtins.strrë
_accumulator+pyspark.profiler.BasicProfiler._accumulatorT
&pyspark.accumulators.Accumulator[None]
None" pyspark.accumulators.Accumulatorîœ
SparkContextpyspark.context.SparkContext"builtins.object*…
__init__%pyspark.context.SparkContext.__init__"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*R
masterD
Union[builtins.str,None]
builtins.str"builtins.str
None *S
appNameD
Union[builtins.str,None]
builtins.str"builtins.str
None *U
	sparkHomeD
Union[builtins.str,None]
builtins.str"builtins.str
None *ë
pyFilesÅ
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None *¶
environmentí
+Union[builtins.dict[builtins.str,Any],None]W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict
None *-
	batchSize
builtins.int"builtins.int *R

serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer *n
confb
"Union[pyspark.conf.SparkConf,None]0
pyspark.conf.SparkConf"pyspark.conf.SparkConf
None *5
gateway&
Union[Any,None]
Any
None *1
jsc&
Union[Any,None]
Any
None *Ñ
profiler_clsp
$Type[pyspark.profiler.BasicProfiler]@
pyspark.profiler.BasicProfiler"pyspark.profiler.BasicProfiler"type *ë
udf_profiler_clsy
'Type[pyspark.profiler.UDFBasicProfiler]F
!pyspark.profiler.UDFBasicProfiler"!pyspark.profiler.UDFBasicProfiler"type *é
memory_profiler_clss
%Type[pyspark.profiler.MemoryProfiler]B
pyspark.profiler.MemoryProfiler"pyspark.profiler.MemoryProfiler"type *·

_do_init%pyspark.context.SparkContext._do_init"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*P
masterD
Union[builtins.str,None]
builtins.str"builtins.str
None*Q
appNameD
Union[builtins.str,None]
builtins.str"builtins.str
None*S
	sparkHomeD
Union[builtins.str,None]
builtins.str"builtins.str
None*è
pyFilesÅ
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None*§
environmentí
+Union[builtins.dict[builtins.str,Any],None]W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict
None*+
	batchSize
builtins.int"builtins.int*P

serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer*l
confb
"Union[pyspark.conf.SparkConf,None]0
pyspark.conf.SparkConf"pyspark.conf.SparkConf
None*
jsc
Any*Ñ
profiler_clsp
$Type[pyspark.profiler.BasicProfiler]@
pyspark.profiler.BasicProfiler"pyspark.profiler.BasicProfiler"type *ë
udf_profiler_clsy
'Type[pyspark.profiler.UDFBasicProfiler]F
!pyspark.profiler.UDFBasicProfiler"!pyspark.profiler.UDFBasicProfiler"type *é
memory_profiler_clss
%Type[pyspark.profiler.MemoryProfiler]B
pyspark.profiler.MemoryProfiler"pyspark.profiler.MemoryProfiler"type *è
__repr__%pyspark.context.SparkContext.__repr__"
builtins.str"builtins.str*><
pyspark.context.SparkContext"pyspark.context.SparkContext*ù
_repr_html_(pyspark.context.SparkContext._repr_html_"
builtins.str"builtins.str*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*¨
_initialize_context0pyspark.context.SparkContext._initialize_context"
Any*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*
jconf
Any*Ö
_ensure_initialized0pyspark.context.SparkContext._ensure_initialized"
None*s
clsj
"Type[pyspark.context.SparkContext]<
pyspark.context.SparkContext"pyspark.context.SparkContext"type*Ñ
instancet
(Union[pyspark.context.SparkContext,None]<
pyspark.context.SparkContext"pyspark.context.SparkContext
None *5
gateway&
Union[Any,None]
Any
None *n
confb
"Union[pyspark.conf.SparkConf,None]0
pyspark.conf.SparkConf"pyspark.conf.SparkConf
None 0:classmethodp*ì
__getnewargs__+pyspark.context.SparkContext.__getnewargs__"
NoReturn
*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*±
	__enter__&pyspark.context.SparkContext.__enter__"<
pyspark.context.SparkContext"pyspark.context.SparkContext*><
pyspark.context.SparkContext"pyspark.context.SparkContext*‘
__exit__%pyspark.context.SparkContext.__exit__"
None*><
pyspark.context.SparkContext"pyspark.context.SparkContext*ìê
(Union[Type[builtins.BaseException],None]X
Type[builtins.BaseException]0
builtins.BaseException"builtins.BaseException"type
None*db
"Union[builtins.BaseException,None]0
builtins.BaseException"builtins.BaseException
None*[Y
Union[types.TracebackType,None]*
types.TracebackType"types.TracebackType
None*Î
getOrCreate(pyspark.context.SparkContext.getOrCreate"<
pyspark.context.SparkContext"pyspark.context.SparkContext*s
clsj
"Type[pyspark.context.SparkContext]<
pyspark.context.SparkContext"pyspark.context.SparkContext"type*n
confb
"Union[pyspark.conf.SparkConf,None]0
pyspark.conf.SparkConf"pyspark.conf.SparkConf
None 0:classmethodp*µ
setLogLevel(pyspark.context.SparkContext.setLogLevel"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext**
logLevel
builtins.str"builtins.str*£
setSystemProperty.pyspark.context.SparkContext.setSystemProperty"
None*s
clsj
"Type[pyspark.context.SparkContext]<
pyspark.context.SparkContext"pyspark.context.SparkContext"type*%
key
builtins.str"builtins.str*'
value
builtins.str"builtins.str0:classmethodp*£
version$pyspark.context.SparkContext.version"
builtins.str"builtins.str*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*Ø
applicationId*pyspark.context.SparkContext.applicationId"
builtins.str"builtins.str*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*Õ
uiWebUrl%pyspark.context.SparkContext.uiWebUrl"D
Union[builtins.str,None]
builtins.str"builtins.str
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*ß
	startTime&pyspark.context.SparkContext.startTime"
builtins.int"builtins.int*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*π
defaultParallelism/pyspark.context.SparkContext.defaultParallelism"
builtins.int"builtins.int*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*Ω
defaultMinPartitions1pyspark.context.SparkContext.defaultMinPartitions"
builtins.int"builtins.int*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*{
stop!pyspark.context.SparkContext.stop"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*´
emptyRDD%pyspark.context.SparkContext.emptyRDD"0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*æ
range"pyspark.context.SparkContext.range"N
pyspark.rdd.RDD[builtins.int]
builtins.int"builtins.int"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*'
start
builtins.int"builtins.int*O
endD
Union[builtins.int,None]
builtins.int"builtins.int
None *(
step
builtins.int"builtins.int *U
	numSlicesD
Union[builtins.int,None]
builtins.int"builtins.int
None *Ê
parallelize(pyspark.context.SparkContext.parallelize"Å
"pyspark.rdd.RDD[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*â
cÅ
"typing.Iterable[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"typing.Iterable*U
	numSlicesD
Union[builtins.int,None]
builtins.int"builtins.int
None *±
_serialize_to_jvm.pyspark.context.SparkContext._serialize_to_jvm"
Any*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*å
dataÅ
"typing.Iterable[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"typing.Iterable*P

serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializer*\
reader_funcK
CallableType[builtins.function]&
builtins.function"builtins.function*\
server_funcK
CallableType[builtins.function]&
builtins.function"builtins.function*≤

pickleFile'pyspark.context.SparkContext.pickleFile"0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
name
builtins.str"builtins.str*Y
minPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *ˇ
textFile%pyspark.context.SparkContext.textFile"N
pyspark.rdd.RDD[builtins.str]
builtins.str"builtins.str"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
name
builtins.str"builtins.str*Y
minPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *1
use_unicode
builtins.bool"builtins.bool *‰
wholeTextFiles+pyspark.context.SparkContext.wholeTextFiles"¶
1pyspark.rdd.RDD[Tuple[builtins.str,builtins.str]]`
 Tuple[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*Y
minPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *1
use_unicode
builtins.bool"builtins.bool *≥
binaryFiles(pyspark.context.SparkContext.binaryFiles"Æ
3pyspark.rdd.RDD[Tuple[builtins.str,builtins.bytes]]f
"Tuple[builtins.str,builtins.bytes]
builtins.str"builtins.str 
builtins.bytes"builtins.bytes"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*Y
minPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *±
binaryRecords*pyspark.context.SparkContext.binaryRecords"T
pyspark.rdd.RDD[builtins.bytes] 
builtins.bytes"builtins.bytes"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*.
recordLength
builtins.int"builtins.int*“
_dictToJavaMap+pyspark.context.SparkContext._dictToJavaMap"
Any*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*¡
dπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None*≠
sequenceFile)pyspark.context.SparkContext.sequenceFile"ó
;pyspark.rdd.RDD[Tuple[pyspark.context.T,pyspark.context.U]]∆
*Tuple[pyspark.context.T,pyspark.context.U]J
pyspark.context.T"
builtins.object"builtins.object"builtins.objectJ
pyspark.context.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*T
keyClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *V

valueClassD
Union[builtins.str,None]
builtins.str"builtins.str
None *X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *U
	minSplitsD
Union[builtins.int,None]
builtins.int"builtins.int
None *-
	batchSize
builtins.int"builtins.int *á
newAPIHadoopFile-pyspark.context.SparkContext.newAPIHadoopFile"ó
;pyspark.rdd.RDD[Tuple[pyspark.context.T,pyspark.context.U]]∆
*Tuple[pyspark.context.T,pyspark.context.U]J
pyspark.context.T"
builtins.object"builtins.object"builtins.objectJ
pyspark.context.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*2
inputFormatClass
builtins.str"builtins.str**
keyClass
builtins.str"builtins.str*,

valueClass
builtins.str"builtins.str*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *∆
confπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *-
	batchSize
builtins.int"builtins.int *›
newAPIHadoopRDD,pyspark.context.SparkContext.newAPIHadoopRDD"ó
;pyspark.rdd.RDD[Tuple[pyspark.context.T,pyspark.context.U]]∆
*Tuple[pyspark.context.T,pyspark.context.U]J
pyspark.context.T"
builtins.object"builtins.object"builtins.objectJ
pyspark.context.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*2
inputFormatClass
builtins.str"builtins.str**
keyClass
builtins.str"builtins.str*,

valueClass
builtins.str"builtins.str*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *∆
confπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *-
	batchSize
builtins.int"builtins.int *˚

hadoopFile'pyspark.context.SparkContext.hadoopFile"ó
;pyspark.rdd.RDD[Tuple[pyspark.context.T,pyspark.context.U]]∆
*Tuple[pyspark.context.T,pyspark.context.U]J
pyspark.context.T"
builtins.object"builtins.object"builtins.objectJ
pyspark.context.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*2
inputFormatClass
builtins.str"builtins.str**
keyClass
builtins.str"builtins.str*,

valueClass
builtins.str"builtins.str*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *∆
confπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *-
	batchSize
builtins.int"builtins.int *—
	hadoopRDD&pyspark.context.SparkContext.hadoopRDD"ó
;pyspark.rdd.RDD[Tuple[pyspark.context.T,pyspark.context.U]]∆
*Tuple[pyspark.context.T,pyspark.context.U]J
pyspark.context.T"
builtins.object"builtins.object"builtins.objectJ
pyspark.context.U"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*2
inputFormatClass
builtins.str"builtins.str**
keyClass
builtins.str"builtins.str*,

valueClass
builtins.str"builtins.str*X
keyConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *Z
valueConverterD
Union[builtins.str,None]
builtins.str"builtins.str
None *∆
confπ
4Union[builtins.dict[builtins.str,builtins.str],None]u
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict
None *-
	batchSize
builtins.int"builtins.int *«
_checkpointFile,pyspark.context.SparkContext._checkpointFile"0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
name
builtins.str"builtins.str*d
input_deserializerL
$pyspark.serializers.PairDeserializer"$pyspark.serializers.PairDeserializer*À
union"pyspark.context.SparkContext.union"Å
"pyspark.rdd.RDD[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*—
rdds∆
1builtins.list[pyspark.rdd.RDD[pyspark.context.T]]Å
"pyspark.rdd.RDD[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD"builtins.list*Ó
	broadcast&pyspark.context.SparkContext.broadcast"ô
.pyspark.broadcast.Broadcast[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"pyspark.broadcast.Broadcast*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*U
valueJ
pyspark.context.T"
builtins.object"builtins.object"builtins.object*ï
accumulator(pyspark.context.SparkContext.accumulator"£
3pyspark.accumulators.Accumulator[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object" pyspark.accumulators.Accumulator*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*U
valueJ
pyspark.context.T"
builtins.object"builtins.object"builtins.object*ñ
accum_paramÇ
DUnion[pyspark.accumulators.AccumulatorParam[pyspark.context.T],None]≠
8pyspark.accumulators.AccumulatorParam[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"%pyspark.accumulators.AccumulatorParam
None *⁄
addFile$pyspark.context.SparkContext.addFile"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*/
	recursive
builtins.bool"builtins.bool *’
	listFiles&pyspark.context.SparkContext.listFiles"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*≠
	addPyFile&pyspark.context.SparkContext.addPyFile"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*Ø

addArchive'pyspark.context.SparkContext.addArchive"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*€
listArchives)pyspark.context.SparkContext.listArchives"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*æ
setCheckpointDir-pyspark.context.SparkContext.setCheckpointDir"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*)
dirName
builtins.str"builtins.str*œ
getCheckpointDir-pyspark.context.SparkContext.getCheckpointDir"D
Union[builtins.str,None]
builtins.str"builtins.str
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*Ù
_getJavaStorageLevel1pyspark.context.SparkContext._getJavaStorageLevel"
Any*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*X
storageLevelF
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel*ú
setJobGroup(pyspark.context.SparkContext.setJobGroup"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*)
groupId
builtins.str"builtins.str*-
description
builtins.str"builtins.str*7
interruptOnCancel
builtins.bool"builtins.bool *“
setInterruptOnCancel1pyspark.context.SparkContext.setInterruptOnCancel"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*5
interruptOnCancel
builtins.bool"builtins.bool*¨
	addJobTag&pyspark.context.SparkContext.addJobTag"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*%
tag
builtins.str"builtins.str*≤
removeJobTag)pyspark.context.SparkContext.removeJobTag"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*%
tag
builtins.str"builtins.str*«

getJobTags'pyspark.context.SparkContext.getJobTags"H
builtins.set[builtins.str]
builtins.str"builtins.str"builtins.set*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*ã
clearJobTags)pyspark.context.SparkContext.clearJobTags"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*„
setLocalProperty-pyspark.context.SparkContext.setLocalProperty"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*%
key
builtins.str"builtins.str*'
value
builtins.str"builtins.str*ˆ
getLocalProperty-pyspark.context.SparkContext.getLocalProperty"D
Union[builtins.str,None]
builtins.str"builtins.str
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*%
key
builtins.str"builtins.str*æ
setJobDescription.pyspark.context.SparkContext.setJobDescription"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*'
value
builtins.str"builtins.str*ô
	sparkUser&pyspark.context.SparkContext.sparkUser"
builtins.str"builtins.str*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*∫
cancelJobGroup+pyspark.context.SparkContext.cancelJobGroup"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*)
groupId
builtins.str"builtins.str*º
cancelJobsWithTag.pyspark.context.SparkContext.cancelJobsWithTag"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*%
tag
builtins.str"builtins.str*ç
cancelAllJobs*pyspark.context.SparkContext.cancelAllJobs"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*¡
statusTracker*pyspark.context.SparkContext.statusTracker"<
pyspark.status.StatusTracker"pyspark.status.StatusTracker*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*±
runJob#pyspark.context.SparkContext.runJob"}
 builtins.list[pyspark.context.U]J
pyspark.context.U"
builtins.object"builtins.object"builtins.object"builtins.list*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*ã
rddÅ
"pyspark.rdd.RDD[pyspark.context.T]J
pyspark.context.T"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD*^
partitionFuncK
CallableType[builtins.function]&
builtins.function"builtins.function*ö

partitionsá
)Union[typing.Sequence[builtins.int],None]N
typing.Sequence[builtins.int]
builtins.int"builtins.int"typing.Sequence
None *0

allowLocal
builtins.bool"builtins.bool *ç
show_profiles*pyspark.context.SparkContext.show_profiles"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*µ
dump_profiles*pyspark.context.SparkContext.dump_profiles"
None*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*&
path
builtins.str"builtins.str*©
getConf$pyspark.context.SparkContext.getConf"0
pyspark.conf.SparkConf"pyspark.conf.SparkConf*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext*Ì
	resources&pyspark.context.SparkContext.resources"·
Lbuiltins.dict[builtins.str,pyspark.resource.information.ResourceInformation]
builtins.str"builtins.strd
0pyspark.resource.information.ResourceInformation"0pyspark.resource.information.ResourceInformation"builtins.dict*F
self<
pyspark.context.SparkContext"pyspark.context.SparkContext0:property`*_
_assert_on_driver.pyspark.context.SparkContext._assert_on_driver"
None0:staticmethodhrY
_gateway%pyspark.context.SparkContext._gateway&
Union[Any,None]
Any
NonerQ
_jvm!pyspark.context.SparkContext._jvm&
Union[Any,None]
Any
Noner[
_next_accum_id+pyspark.context.SparkContext._next_accum_id
builtins.int"builtins.intr¡
_active_spark_context2pyspark.context.SparkContext._active_spark_contextt
(Union[pyspark.context.SparkContext,None]<
pyspark.context.SparkContext"pyspark.context.SparkContext
NonerQ
_lock"pyspark.context.SparkContext._lock$
threading._RLock"threading._RLockr≈
_python_includes-pyspark.context.SparkContext._python_includesÅ
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
Nonerw

serializer'pyspark.context.SparkContext.serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializerrè
profiler_collector/pyspark.context.SparkContext.profiler_collectorH
"pyspark.profiler.ProfilerCollector""pyspark.profiler.ProfilerCollectorrï
PACKAGE_EXTENSIONS/pyspark.context.SparkContext.PACKAGE_EXTENSIONSN
typing.Iterable[builtins.str]
builtins.str"builtins.str"typing.Iterablerí
	_callsite&pyspark.context.SparkContext._callsite]
Union[Any,Tuple[Any,Any,Any]]
Any1
Tuple[Any,Any,Any]
Any
Any
Anyrê
environment(pyspark.context.SparkContext.environmentW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dictr]
_conf"pyspark.context.SparkContext._conf0
pyspark.conf.SparkConf"pyspark.conf.SparkConfrS

_batchSize'pyspark.context.SparkContext._batchSize
builtins.int"builtins.intrç
_unbatched_serializer2pyspark.context.SparkContext._unbatched_serializer@
pyspark.serializers.Serializer"pyspark.serializers.Serializerrs
master#pyspark.context.SparkContext.masterD
Union[builtins.str,None]
builtins.str"builtins.str
Noneru
appName$pyspark.context.SparkContext.appNameD
Union[builtins.str,None]
builtins.str"builtins.str
Nonery
	sparkHome&pyspark.context.SparkContext.sparkHomeD
Union[builtins.str,None]
builtins.str"builtins.str
Noner2
_jsc!pyspark.context.SparkContext._jsc
Anyró
_accumulatorServer/pyspark.context.SparkContext._accumulatorServerP
&pyspark.accumulators.AccumulatorServer"&pyspark.accumulators.AccumulatorServerrJ
_javaAccumulator-pyspark.context.SparkContext._javaAccumulator
AnyrP
_encryption_enabled0pyspark.context.SparkContext._encryption_enabled
AnyrS

pythonExec'pyspark.context.SparkContext.pythonExec
builtins.str"builtins.strrQ
	pythonVer&pyspark.context.SparkContext.pythonVer
builtins.str"builtins.strrß
_pickled_broadcast_vars4pyspark.context.SparkContext._pickled_broadcast_varsV
)pyspark.broadcast.BroadcastPickleRegistry")pyspark.broadcast.BroadcastPickleRegistryr<
	_temp_dir&pyspark.context.SparkContext._temp_dir
Any”^

SQLContextpyspark.sql.context.SQLContext"builtins.object*¨
__init__'pyspark.sql.context.SQLContext.__init__"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*N
sparkContext<
pyspark.context.SparkContext"pyspark.context.SparkContext*ï
sparkSessionÄ
,Union[pyspark.sql.session.SparkSession,None]D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession
None *9
jsqlContext&
Union[Any,None]
Any
None *ò
	_ssql_ctx(pyspark.sql.context.SQLContext._ssql_ctx"
Any*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`*Õ
getOrCreate*pyspark.sql.context.SQLContext.getOrCreate"@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*y
clsp
$Type[pyspark.sql.context.SQLContext]@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext"type*D
sc<
pyspark.context.SparkContext"pyspark.context.SparkContext0:classmethodp*Ì
_get_or_create-pyspark.sql.context.SQLContext._get_or_create"@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*y
clsp
$Type[pyspark.sql.context.SQLContext]@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext"type*D
sc<
pyspark.context.SparkContext"pyspark.context.SparkContext*
static_conf
Any0:classmethodp*≈

newSession)pyspark.sql.context.SQLContext.newSession"@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*À
setConf&pyspark.sql.context.SQLContext.setConf"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*%
key
builtins.str"builtins.str*ö
valueé
.Union[builtins.bool,builtins.int,builtins.str]
builtins.bool"builtins.bool
builtins.int"builtins.int
builtins.str"builtins.str*§
getConf&pyspark.sql.context.SQLContext.getConf"D
Union[builtins.str,None]
builtins.str"builtins.str
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*%
key
builtins.str"builtins.str*∑
defaultValue¢
6Union[builtins.str,None,pyspark._globals._NoValueType]
builtins.str"builtins.str
None>
pyspark._globals._NoValueType"pyspark._globals._NoValueType *«
udf"pyspark.sql.context.SQLContext.udf"B
pyspark.sql.udf.UDFRegistration"pyspark.sql.udf.UDFRegistration*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`*Õ
udtf#pyspark.sql.context.SQLContext.udtf"F
!pyspark.sql.udtf.UDTFRegistration"!pyspark.sql.udtf.UDTFRegistration*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`*º
range$pyspark.sql.context.SQLContext.range"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*'
start
builtins.int"builtins.int*O
endD
Union[builtins.int,None]
builtins.int"builtins.int
None *(
step
builtins.int"builtins.int *Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *Í
registerFunction/pyspark.sql.context.SQLContext.registerFunction"Z
+pyspark.sql._typing.UserDefinedFunctionLike"+pyspark.sql._typing.UserDefinedFunctionLike*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*&
name
builtins.str"builtins.str*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*Ä

returnTypen
&Union[pyspark.sql.types.DataType,None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
None *˝
registerJavaFunction3pyspark.sql.context.SQLContext.registerJavaFunction"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*&
name
builtins.str"builtins.str*/
javaClassName
builtins.str"builtins.str*Ä

returnTypen
&Union[pyspark.sql.types.DataType,None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
None *·
_inferSchema+pyspark.sql.context.SQLContext._inferSchema"<
pyspark.sql.types.StructType"pyspark.sql.types.StructType*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*9
rdd0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD*_
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None *¢
registerDataFrameAsTable7pyspark.sql.context.SQLContext.registerDataFrameAsTable"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*J
dfB
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*+
	tableName
builtins.str"builtins.str*¿
dropTempTable,pyspark.sql.context.SQLContext.dropTempTable"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*+
	tableName
builtins.str"builtins.str*‹
createExternalTable2pyspark.sql.context.SQLContext.createExternalTable"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*+
	tableName
builtins.str"builtins.str*P
pathD
Union[builtins.str,None]
builtins.str"builtins.str
None *R
sourceD
Union[builtins.str,None]
builtins.str"builtins.str
None *Ç
schemat
(Union[pyspark.sql.types.StructType,None]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
None *)
options
builtins.str"builtins.str*Â
sql"pyspark.sql.context.SQLContext.sql"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext**
sqlQuery
builtins.str"builtins.str*Í
table$pyspark.sql.context.SQLContext.table"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*+
	tableName
builtins.str"builtins.str*ì
tables%pyspark.sql.context.SQLContext.tables"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*R
dbNameD
Union[builtins.str,None]
builtins.str"builtins.str
None *£

tableNames)pyspark.sql.context.SQLContext.tableNames"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*R
dbNameD
Union[builtins.str,None]
builtins.str"builtins.str
None *º

cacheTable)pyspark.sql.context.SQLContext.cacheTable"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*+
	tableName
builtins.str"builtins.str0*¿
uncacheTable+pyspark.sql.context.SQLContext.uncacheTable"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*+
	tableName
builtins.str"builtins.str0*è

clearCache)pyspark.sql.context.SQLContext.clearCache"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0*◊
read#pyspark.sql.context.SQLContext.read"P
&pyspark.sql.readwriter.DataFrameReader"&pyspark.sql.readwriter.DataFrameReader*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`*˘

readStream)pyspark.sql.context.SQLContext.readStream"f
1pyspark.sql.streaming.readwriter.DataStreamReader"1pyspark.sql.streaming.readwriter.DataStreamReader*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`*Û
streams&pyspark.sql.context.SQLContext.streams"f
1pyspark.sql.streaming.query.StreamingQueryManager"1pyspark.sql.streaming.query.StreamingQueryManager*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`2‚
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame—
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*ü
dataî
`Union[pyspark.rdd.RDD[pyspark.sql._typing.RowLike],typing.Iterable[pyspark.sql._typing.RowLike]]ï
,pyspark.rdd.RDD[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDï
,typing.Iterable[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"typing.Iterable*Ï
schema›
?Union[builtins.list[builtins.str],builtins.tuple[builtins.str]]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuple *_
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None 0:overloadX’
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*ü
dataî
`Union[pyspark.rdd.RDD[pyspark.sql._typing.RowLike],typing.Iterable[pyspark.sql._typing.RowLike]]ï
,pyspark.rdd.RDD[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDï
,typing.Iterable[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"typing.Iterable*ù
schemaê
0Union[pyspark.sql.types.StructType,builtins.str]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:overloadXÌ
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*∑
data¨
hUnion[pyspark.rdd.RDD[pyspark.sql._typing.AtomicValue],typing.Iterable[pyspark.sql._typing.AtomicValue]]ù
0pyspark.rdd.RDD[pyspark.sql._typing.AtomicValue]X
pyspark.sql._typing.AtomicValue"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDDù
0typing.Iterable[pyspark.sql._typing.AtomicValue]X
pyspark.sql._typing.AtomicValue"
builtins.object"builtins.object"builtins.object"typing.Iterable*ù
schemaê
0Union[pyspark.sql.types.AtomicType,builtins.str]<
pyspark.sql.types.AtomicType"pyspark.sql.types.AtomicType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:overloadXÜ
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*D
data:
pandas.core.frame.DataFrame"pandas.core.frame.DataFrame*_
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None 0:overloadX˘
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*D
data:
pandas.core.frame.DataFrame"pandas.core.frame.DataFrame*ù
schemaê
0Union[pyspark.sql.types.StructType,builtins.str]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:overloadXr«
_instantiatedContext3pyspark.sql.context.SQLContext._instantiatedContextz
*Union[pyspark.sql.context.SQLContext,None]@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext
Nonerg
_sc"pyspark.sql.context.SQLContext._sc<
pyspark.context.SparkContext"pyspark.context.SparkContextr4
_jsc#pyspark.sql.context.SQLContext._jsc
AnyrS
_jvm#pyspark.sql.context.SQLContext._jvm&
Union[Any,None]
Any
NonerÅ
sparkSession+pyspark.sql.context.SQLContext.sparkSessionD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSessionrD
_jsqlContext+pyspark.sql.context.SQLContext._jsqlContext
Anyÿ
HiveContextpyspark.sql.context.HiveContext"pyspark.sql.context.SQLContext*∞
__init__(pyspark.sql.context.HiveContext.__init__"
None*L
selfB
pyspark.sql.context.HiveContext"pyspark.sql.context.HiveContext*N
sparkContext<
pyspark.context.SparkContext"pyspark.context.SparkContext*ï
sparkSessionÄ
,Union[pyspark.sql.session.SparkSession,None]D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession
None *:
jhiveContext&
Union[Any,None]
Any
None *Ó
_get_or_create.pyspark.sql.context.HiveContext._get_or_create"@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*y
clsp
$Type[pyspark.sql.context.SQLContext]@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext"type*D
sc<
pyspark.context.SparkContext"pyspark.context.SparkContext*
static_conf
Any0:classmethodp*È
_createForTesting1pyspark.sql.context.HiveContext._createForTesting"B
pyspark.sql.context.HiveContext"pyspark.sql.context.HiveContext*|
clss
%Type[pyspark.sql.context.HiveContext]B
pyspark.sql.context.HiveContext"pyspark.sql.context.HiveContext"type*N
sparkContext<
pyspark.context.SparkContext"pyspark.context.SparkContext0:classmethodp*¡
refreshTable,pyspark.sql.context.HiveContext.refreshTable"
None*L
selfB
pyspark.sql.context.HiveContext"pyspark.sql.context.HiveContext*+
	tableName
builtins.str"builtins.strr≥
_static_conf,pyspark.sql.context.HiveContext._static_confu
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict‘
Rowpyspark.sql.types.Row"builtins.tuple*Í
asDictpyspark.sql.types.Row.asDict"W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*8
self.
pyspark.sql.types.Row"pyspark.sql.types.Row*/
	recursive
builtins.bool"builtins.bool *è
__contains__"pyspark.sql.types.Row.__contains__"
builtins.bool"builtins.bool*0.
pyspark.sql.types.Row"pyspark.sql.types.Row*	
Any*ß
__call__pyspark.sql.types.Row.__call__".
pyspark.sql.types.Row"pyspark.sql.types.Row*8
self.
pyspark.sql.types.Row"pyspark.sql.types.Row*
args
Any*v
__getitem__!pyspark.sql.types.Row.__getitem__"
Any*0.
pyspark.sql.types.Row"pyspark.sql.types.Row*	
Any*ã
__getattr__!pyspark.sql.types.Row.__getattr__"
Any*0.
pyspark.sql.types.Row"pyspark.sql.types.Row*
builtins.str"builtins.str*ö
__setattr__!pyspark.sql.types.Row.__setattr__"
None*8
self.
pyspark.sql.types.Row"pyspark.sql.types.Row*
key
Any*
value
Any*„

__reduce__ pyspark.sql.types.Row.__reduce__"y
'Union[builtins.str,builtins.tuple[Any]]
builtins.str"builtins.str.
builtins.tuple[Any]
Any"builtins.tuple*8
self.
pyspark.sql.types.Row"pyspark.sql.types.Row*z
__repr__pyspark.sql.types.Row.__repr__"
builtins.str"builtins.str*0.
pyspark.sql.types.Row"pyspark.sql.types.Row2˜
__new__pyspark.sql.types.Row.__new__Ó
__new__pyspark.sql.types.Row.__new__".
pyspark.sql.types.Row"pyspark.sql.types.Row*^
clsU
Type[pyspark.sql.types.Row].
pyspark.sql.types.Row"pyspark.sql.types.Row"type*&
args
builtins.str"builtins.str0:overloadX€
__new__pyspark.sql.types.Row.__new__".
pyspark.sql.types.Row"pyspark.sql.types.Row*^
clsU
Type[pyspark.sql.types.Row].
pyspark.sql.types.Row"pyspark.sql.types.Row"type*
kwargs
Any0:overloadX‰
inheritable_thread_target&pyspark.util.inheritable_thread_target"K
CallableType[builtins.function]&
builtins.function"builtins.function*R
fK
CallableType[builtins.function]&
builtins.function"builtins.functionÿ
sincepyspark.since"K
CallableType[builtins.function]&
builtins.function"builtins.function*s
versionf
"Union[builtins.str,builtins.float]
builtins.str"builtins.str 
builtins.float"builtins.floaté
	copy_funcpyspark.copy_func"[

pyspark._FK
CallableType[builtins.function]&
builtins.function"builtins.function*b
f[

pyspark._FK
CallableType[builtins.function]&
builtins.function"builtins.function*P
nameD
Union[builtins.str,None]
builtins.str"builtins.str
None *â
sinceversionu
'Union[builtins.str,builtins.float,None]
builtins.str"builtins.str 
builtins.float"builtins.float
None *O
docD
Union[builtins.str,None]
builtins.str"builtins.str
None Ë
keyword_onlypyspark.keyword_only"[

pyspark._FK
CallableType[builtins.function]&
builtins.function"builtins.function*e
func[

pyspark._FK
CallableType[builtins.function]&
builtins.function"builtins.function*h
__path__pyspark.__path__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*É
__annotations__pyspark.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*H
__version__pyspark.version.__version__
builtins.str"builtins.str*e
_NoValuepyspark._globals._NoValue>
pyspark._globals._NoValueType"pyspark._globals._NoValueType*f
__all__pyspark.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list