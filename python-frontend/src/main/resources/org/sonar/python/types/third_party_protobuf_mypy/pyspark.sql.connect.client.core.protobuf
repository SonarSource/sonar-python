
pyspark.sql.connect.client.core«$
ChannelBuilder.pyspark.sql.connect.client.core.ChannelBuilder"builtins.object*{
default_port;pyspark.sql.connect.client.core.ChannelBuilder.default_port"
builtins.int"builtins.int0:staticmethodh*∑
__init__7pyspark.sql.connect.client.core.ChannelBuilder.__init__"
None*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder*%
url
builtins.str"builtins.str*‘
channelOptionsΩ
2Union[builtins.list[Tuple[builtins.str,Any]],None]{
&builtins.list[Tuple[builtins.str,Any]]B
Tuple[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.list
None *œ
_extract_attributesBpyspark.sql.connect.client.core.ChannelBuilder._extract_attributes"
None*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder*ÿ
metadata7pyspark.sql.connect.client.core.ChannelBuilder.metadata"¶
1typing.Iterable[Tuple[builtins.str,builtins.str]]`
 Tuple[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"typing.Iterable*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder*Ÿ
secure5pyspark.sql.connect.client.core.ChannelBuilder.secure"
builtins.bool"builtins.bool*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder0:property`*€
endpoint7pyspark.sql.connect.client.core.ChannelBuilder.endpoint"
builtins.str"builtins.str*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder0:property`*ˇ
_token5pyspark.sql.connect.client.core.ChannelBuilder._token"D
Union[builtins.str,None]
builtins.str"builtins.str
None*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder0:property`*ˇ
userId5pyspark.sql.connect.client.core.ChannelBuilder.userId"D
Union[builtins.str,None]
builtins.str"builtins.str
None*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder0:property`*›
	userAgent8pyspark.sql.connect.client.core.ChannelBuilder.userAgent"
builtins.str"builtins.str*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder0:property`*’
get2pyspark.sql.connect.client.core.ChannelBuilder.get"
Any*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder*%
key
builtins.str"builtins.str*á

session_id9pyspark.sql.connect.client.core.ChannelBuilder.session_id"D
Union[builtins.str,None]
builtins.str"builtins.str
None*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder0:property`*∫
	toChannel8pyspark.sql.connect.client.core.ChannelBuilder.toChannel"
Any*j
self`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilderrk
PARAM_USE_SSL<pyspark.sql.connect.client.core.ChannelBuilder.PARAM_USE_SSL
builtins.str"builtins.strrg
PARAM_TOKEN:pyspark.sql.connect.client.core.ChannelBuilder.PARAM_TOKEN
builtins.str"builtins.strrk
PARAM_USER_ID<pyspark.sql.connect.client.core.ChannelBuilder.PARAM_USER_ID
builtins.str"builtins.strrq
PARAM_USER_AGENT?pyspark.sql.connect.client.core.ChannelBuilder.PARAM_USER_AGENT
builtins.str"builtins.strrq
PARAM_SESSION_ID?pyspark.sql.connect.client.core.ChannelBuilder.PARAM_SESSION_ID
builtins.str"builtins.strru
MAX_MESSAGE_LENGTHApyspark.sql.connect.client.core.ChannelBuilder.MAX_MESSAGE_LENGTH
builtins.int"builtins.intr»
url2pyspark.sql.connect.client.core.ChannelBuilder.urlå
TTuple[builtins.str,builtins.str,builtins.str,builtins.str,builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.str
builtins.str"builtins.strr∂
params5pyspark.sql.connect.client.core.ChannelBuilder.paramsu
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dictr¯
_channel_options?pyspark.sql.connect.client.core.ChannelBuilder._channel_options¢
/builtins.list[Tuple[builtins.str,builtins.int]]`
 Tuple[builtins.str,builtins.int]
builtins.str"builtins.str
builtins.int"builtins.int"builtins.listrY
host3pyspark.sql.connect.client.core.ChannelBuilder.host
builtins.str"builtins.strrY
port3pyspark.sql.connect.client.core.ChannelBuilder.port
builtins.int"builtins.intú
MetricValue+pyspark.sql.connect.client.core.MetricValue"builtins.object*Û
__init__4pyspark.sql.connect.client.core.MetricValue.__init__"
None*d
selfZ
+pyspark.sql.connect.client.core.MetricValue"+pyspark.sql.connect.client.core.MetricValue*&
name
builtins.str"builtins.str*q
valuef
"Union[builtins.int,builtins.float]
builtins.int"builtins.int 
builtins.float"builtins.float*&
type
builtins.str"builtins.str*º
__repr__4pyspark.sql.connect.client.core.MetricValue.__repr__"
builtins.str"builtins.str*\Z
+pyspark.sql.connect.client.core.MetricValue"+pyspark.sql.connect.client.core.MetricValue* 
name0pyspark.sql.connect.client.core.MetricValue.name"
builtins.str"builtins.str*d
selfZ
+pyspark.sql.connect.client.core.MetricValue"+pyspark.sql.connect.client.core.MetricValue0:property`*ñ
value1pyspark.sql.connect.client.core.MetricValue.value"f
"Union[builtins.int,builtins.float]
builtins.int"builtins.int 
builtins.float"builtins.float*d
selfZ
+pyspark.sql.connect.client.core.MetricValue"+pyspark.sql.connect.client.core.MetricValue0:property`*ÿ
metric_type7pyspark.sql.connect.client.core.MetricValue.metric_type"
builtins.str"builtins.str*d
selfZ
+pyspark.sql.connect.client.core.MetricValue"+pyspark.sql.connect.client.core.MetricValue0:property`rX
_name1pyspark.sql.connect.client.core.MetricValue._name
builtins.str"builtins.strrX
_type1pyspark.sql.connect.client.core.MetricValue._type
builtins.str"builtins.strr§
_value2pyspark.sql.connect.client.core.MetricValue._valuef
"Union[builtins.int,builtins.float]
builtins.int"builtins.int 
builtins.float"builtins.floatŒ
PlanMetrics+pyspark.sql.connect.client.core.PlanMetrics"builtins.object*‡
__init__4pyspark.sql.connect.client.core.PlanMetrics.__init__"
None*d
selfZ
+pyspark.sql.connect.client.core.PlanMetrics"+pyspark.sql.connect.client.core.PlanMetrics*&
name
builtins.str"builtins.str*$
id
builtins.int"builtins.int*(
parent
builtins.int"builtins.int*µ
metricsß
:builtins.list[pyspark.sql.connect.client.core.MetricValue]Z
+pyspark.sql.connect.client.core.MetricValue"+pyspark.sql.connect.client.core.MetricValue"builtins.list*º
__repr__4pyspark.sql.connect.client.core.PlanMetrics.__repr__"
builtins.str"builtins.str*\Z
+pyspark.sql.connect.client.core.PlanMetrics"+pyspark.sql.connect.client.core.PlanMetrics* 
name0pyspark.sql.connect.client.core.PlanMetrics.name"
builtins.str"builtins.str*d
selfZ
+pyspark.sql.connect.client.core.PlanMetrics"+pyspark.sql.connect.client.core.PlanMetrics0:property`*–
plan_id3pyspark.sql.connect.client.core.PlanMetrics.plan_id"
builtins.int"builtins.int*d
selfZ
+pyspark.sql.connect.client.core.PlanMetrics"+pyspark.sql.connect.client.core.PlanMetrics0:property`*ﬁ
parent_plan_id:pyspark.sql.connect.client.core.PlanMetrics.parent_plan_id"
builtins.int"builtins.int*d
selfZ
+pyspark.sql.connect.client.core.PlanMetrics"+pyspark.sql.connect.client.core.PlanMetrics0:property`*‹
metrics3pyspark.sql.connect.client.core.PlanMetrics.metrics"ß
:builtins.list[pyspark.sql.connect.client.core.MetricValue]Z
+pyspark.sql.connect.client.core.MetricValue"+pyspark.sql.connect.client.core.MetricValue"builtins.list*d
selfZ
+pyspark.sql.connect.client.core.PlanMetrics"+pyspark.sql.connect.client.core.PlanMetrics0:property`rX
_name1pyspark.sql.connect.client.core.PlanMetrics._name
builtins.str"builtins.strrT
_id/pyspark.sql.connect.client.core.PlanMetrics._id
builtins.int"builtins.intrb

_parent_id6pyspark.sql.connect.client.core.PlanMetrics._parent_id
builtins.int"builtins.intrÍ
_metrics4pyspark.sql.connect.client.core.PlanMetrics._metricsß
:builtins.list[pyspark.sql.connect.client.core.MetricValue]Z
+pyspark.sql.connect.client.core.MetricValue"+pyspark.sql.connect.client.core.MetricValue"builtins.list©
PlanObservedMetrics3pyspark.sql.connect.client.core.PlanObservedMetrics"builtins.object*€
__init__<pyspark.sql.connect.client.core.PlanObservedMetrics.__init__"
None*t
selfj
3pyspark.sql.connect.client.core.PlanObservedMetrics"3pyspark.sql.connect.client.core.PlanObservedMetrics*&
name
builtins.str"builtins.str*Ë
metrics⁄
Kbuiltins.list[pyspark.sql.connect.proto.expressions_pb2.Expression.Literal]|
<pyspark.sql.connect.proto.expressions_pb2.Expression.Literal"<pyspark.sql.connect.proto.expressions_pb2.Expression.Literal"builtins.list*‘
__repr__<pyspark.sql.connect.client.core.PlanObservedMetrics.__repr__"
builtins.str"builtins.str*lj
3pyspark.sql.connect.client.core.PlanObservedMetrics"3pyspark.sql.connect.client.core.PlanObservedMetrics*‚
name8pyspark.sql.connect.client.core.PlanObservedMetrics.name"
builtins.str"builtins.str*t
selfj
3pyspark.sql.connect.client.core.PlanObservedMetrics"3pyspark.sql.connect.client.core.PlanObservedMetrics0:property`*ß
metrics;pyspark.sql.connect.client.core.PlanObservedMetrics.metrics"⁄
Kbuiltins.list[pyspark.sql.connect.proto.expressions_pb2.Expression.Literal]|
<pyspark.sql.connect.proto.expressions_pb2.Expression.Literal"<pyspark.sql.connect.proto.expressions_pb2.Expression.Literal"builtins.list*t
selfj
3pyspark.sql.connect.client.core.PlanObservedMetrics"3pyspark.sql.connect.client.core.PlanObservedMetrics0:property`r`
_name9pyspark.sql.connect.client.core.PlanObservedMetrics._name
builtins.str"builtins.strr•
_metrics<pyspark.sql.connect.client.core.PlanObservedMetrics._metrics⁄
Kbuiltins.list[pyspark.sql.connect.proto.expressions_pb2.Expression.Literal]|
<pyspark.sql.connect.proto.expressions_pb2.Expression.Literal"<pyspark.sql.connect.proto.expressions_pb2.Expression.Literal"builtins.list¥
AnalyzeResult-pyspark.sql.connect.client.core.AnalyzeResult"builtins.object*”

__init__6pyspark.sql.connect.client.core.AnalyzeResult.__init__"
None*h
self^
-pyspark.sql.connect.client.core.AnalyzeResult"-pyspark.sql.connect.client.core.AnalyzeResult*z
scheman
&Union[pyspark.sql.types.DataType,None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
None*X
explain_stringD
Union[builtins.str,None]
builtins.str"builtins.str
None*U
tree_stringD
Union[builtins.str,None]
builtins.str"builtins.str
None*U
is_localG
Union[builtins.bool,None]
builtins.bool"builtins.bool
None*Y
is_streamingG
Union[builtins.bool,None]
builtins.bool"builtins.bool
None*ì
input_filesÅ
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None*W
spark_versionD
Union[builtins.str,None]
builtins.str"builtins.str
None*z
parsedn
&Union[pyspark.sql.types.DataType,None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
None*^
is_same_semanticsG
Union[builtins.bool,None]
builtins.bool"builtins.bool
None*W
semantic_hashD
Union[builtins.int,None]
builtins.int"builtins.int
None*ó
storage_levelÉ
-Union[pyspark.storagelevel.StorageLevel,None]F
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel
None*
	fromProto7pyspark.sql.connect.client.core.AnalyzeResult.fromProto"^
-pyspark.sql.connect.client.core.AnalyzeResult"-pyspark.sql.connect.client.core.AnalyzeResult*ß
clsù
3Type[pyspark.sql.connect.client.core.AnalyzeResult]^
-pyspark.sql.connect.client.core.AnalyzeResult"-pyspark.sql.connect.client.core.AnalyzeResult"type*
pb
Any0:classmethodprÆ
schema4pyspark.sql.connect.client.core.AnalyzeResult.scheman
&Union[pyspark.sql.types.DataType,None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
Nonerî
explain_string<pyspark.sql.connect.client.core.AnalyzeResult.explain_stringD
Union[builtins.str,None]
builtins.str"builtins.str
Noneré
tree_string9pyspark.sql.connect.client.core.AnalyzeResult.tree_stringD
Union[builtins.str,None]
builtins.str"builtins.str
Nonerã
is_local6pyspark.sql.connect.client.core.AnalyzeResult.is_localG
Union[builtins.bool,None]
builtins.bool"builtins.bool
Nonerì
is_streaming:pyspark.sql.connect.client.core.AnalyzeResult.is_streamingG
Union[builtins.bool,None]
builtins.bool"builtins.bool
NonerÃ
input_files9pyspark.sql.connect.client.core.AnalyzeResult.input_filesÅ
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
Nonerí
spark_version;pyspark.sql.connect.client.core.AnalyzeResult.spark_versionD
Union[builtins.str,None]
builtins.str"builtins.str
NonerÆ
parsed4pyspark.sql.connect.client.core.AnalyzeResult.parsedn
&Union[pyspark.sql.types.DataType,None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
Nonerù
is_same_semantics?pyspark.sql.connect.client.core.AnalyzeResult.is_same_semanticsG
Union[builtins.bool,None]
builtins.bool"builtins.bool
Nonerí
semantic_hash;pyspark.sql.connect.client.core.AnalyzeResult.semantic_hashD
Union[builtins.int,None]
builtins.int"builtins.int
Noner“
storage_level;pyspark.sql.connect.client.core.AnalyzeResult.storage_levelÉ
-Union[pyspark.storagelevel.StorageLevel,None]F
!pyspark.storagelevel.StorageLevel"!pyspark.storagelevel.StorageLevel
Noneœ
ConfigResult,pyspark.sql.connect.client.core.ConfigResult"builtins.object*ˇ
__init__5pyspark.sql.connect.client.core.ConfigResult.__init__"
None*f
self\
,pyspark.sql.connect.client.core.ConfigResult",pyspark.sql.connect.client.core.ConfigResult*Ô
pairs„
;builtins.list[Tuple[builtins.str,Union[builtins.str,None]]]î
,Tuple[builtins.str,Union[builtins.str,None]]
builtins.str"builtins.strD
Union[builtins.str,None]
builtins.str"builtins.str
None"builtins.list*X
warningsJ
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*…
	fromProto6pyspark.sql.connect.client.core.ConfigResult.fromProto"\
,pyspark.sql.connect.client.core.ConfigResult",pyspark.sql.connect.client.core.ConfigResult*§
clsö
2Type[pyspark.sql.connect.client.core.ConfigResult]\
,pyspark.sql.connect.client.core.ConfigResult",pyspark.sql.connect.client.core.ConfigResult"type*n
pbf
1pyspark.sql.connect.proto.base_pb2.ConfigResponse"1pyspark.sql.connect.proto.base_pb2.ConfigResponse0:classmethodpr°
pairs2pyspark.sql.connect.client.core.ConfigResult.pairs„
;builtins.list[Tuple[builtins.str,Union[builtins.str,None]]]î
,Tuple[builtins.str,Union[builtins.str,None]]
builtins.str"builtins.strD
Union[builtins.str,None]
builtins.str"builtins.str
None"builtins.listrç
warnings5pyspark.sql.connect.client.core.ConfigResult.warningsJ
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list˛ß
SparkConnectClient2pyspark.sql.connect.client.core.SparkConnectClient"builtins.object*
retry_exceptionBpyspark.sql.connect.client.core.SparkConnectClient.retry_exception"
builtins.bool"builtins.bool*∂
cls¨
8Type[pyspark.sql.connect.client.core.SparkConnectClient]h
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient"type*/
e(
builtins.Exception"builtins.Exception0:classmethodp*∂
__init__;pyspark.sql.connect.client.core.SparkConnectClient.__init__"
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*◊

connection∆
BUnion[builtins.str,pyspark.sql.connect.client.core.ChannelBuilder]
builtins.str"builtins.str`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilder*S
user_idD
Union[builtins.str,None]
builtins.str"builtins.str
None *’
channel_optionsΩ
2Union[builtins.list[Tuple[builtins.str,Any]],None]{
&builtins.list[Tuple[builtins.str,Any]]B
Tuple[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.list
None *ß
retry_policyí
+Union[builtins.dict[builtins.str,Any],None]W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict
None *>
use_reattachable_execute
builtins.bool"builtins.bool *ì
	_retrying<pyspark.sql.connect.client.core.SparkConnectClient._retrying"T
(pyspark.sql.connect.client.core.Retrying"(pyspark.sql.connect.client.core.Retrying*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*Õ
disable_reattachable_executeOpyspark.sql.connect.client.core.SparkConnectClient.disable_reattachable_execute"h
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*À
enable_reattachable_executeNpyspark.sql.connect.client.core.SparkConnectClient.enable_reattachable_execute"h
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*ª
register_udf?pyspark.sql.connect.client.core.SparkConnectClient.register_udf"
builtins.str"builtins.str*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*
function
Any*ä
return_type¯
9TypeAlias[Union[pyspark.sql.types.DataType,builtins.str]]ä
.Union[pyspark.sql.types.DataType,builtins.str]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
builtins.str"builtins.str",pyspark.sql.connect._typing.DataTypeOrString*P
nameD
Union[builtins.str,None]
builtins.str"builtins.str
None *-
	eval_type
builtins.int"builtins.int *3
deterministic
builtins.bool"builtins.bool *ì
register_udtf@pyspark.sql.connect.client.core.SparkConnectClient.register_udtf"
builtins.str"builtins.str*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*
function
Any*ä
return_type¯
9TypeAlias[Union[pyspark.sql.types.DataType,builtins.str]]ä
.Union[pyspark.sql.types.DataType,builtins.str]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
builtins.str"builtins.str",pyspark.sql.connect._typing.DataTypeOrString*&
name
builtins.str"builtins.str*-
	eval_type
builtins.int"builtins.int *3
deterministic
builtins.bool"builtins.bool *â
register_java@pyspark.sql.connect.client.core.SparkConnectClient.register_java"
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*&
name
builtins.str"builtins.str*/
javaClassName
builtins.str"builtins.str*≠
return_typeô
3Union[pyspark.sql.types.DataType,builtins.str,None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
builtins.str"builtins.str
None */
	aggregate
builtins.bool"builtins.bool *Ü
_build_metricsApyspark.sql.connect.client.core.SparkConnectClient._build_metrics"´
<typing.Iterator[pyspark.sql.connect.client.core.PlanMetrics]Z
+pyspark.sql.connect.client.core.PlanMetrics"+pyspark.sql.connect.client.core.PlanMetrics"typing.Iterator*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*é
metricsÄ
>pyspark.sql.connect.proto.base_pb2.ExecutePlanResponse.Metrics">pyspark.sql.connect.proto.base_pb2.ExecutePlanResponse.Metrics*£

_resources=pyspark.sql.connect.client.core.SparkConnectClient._resources"·
Lbuiltins.dict[builtins.str,pyspark.resource.information.ResourceInformation]
builtins.str"builtins.strd
0pyspark.resource.information.ResourceInformation"0pyspark.resource.information.ResourceInformation"builtins.dict*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*≠
_build_observed_metricsJpyspark.sql.connect.client.core.SparkConnectClient._build_observed_metrics"√
Dtyping.Iterator[pyspark.sql.connect.client.core.PlanObservedMetrics]j
3pyspark.sql.connect.client.core.PlanObservedMetrics"3pyspark.sql.connect.client.core.PlanObservedMetrics"typing.Iterator*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*ã
metrics˝
Wtyping.Sequence[pyspark.sql.connect.proto.base_pb2.ExecutePlanResponse.ObservedMetrics]ê
Fpyspark.sql.connect.proto.base_pb2.ExecutePlanResponse.ObservedMetrics"Fpyspark.sql.connect.proto.base_pb2.ExecutePlanResponse.ObservedMetrics"typing.Sequence*Û
to_table_as_iteratorGpyspark.sql.connect.client.core.SparkConnectClient.to_table_as_iterator"ø
8typing.Iterator[Union[pyspark.sql.types.StructType,Any]]r
'Union[pyspark.sql.types.StructType,Any]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
Any"typing.Iterator*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*\
planR
'pyspark.sql.connect.proto.base_pb2.Plan"'pyspark.sql.connect.proto.base_pb2.Plan*“
to_table;pyspark.sql.connect.client.core.SparkConnectClient.to_table"∂
3Tuple[Any,Union[pyspark.sql.types.StructType,None]]
Anyt
(Union[pyspark.sql.types.StructType,None]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*\
planR
'pyspark.sql.connect.proto.base_pb2.Plan"'pyspark.sql.connect.proto.base_pb2.Plan*◊
	to_pandas<pyspark.sql.connect.client.core.SparkConnectClient.to_pandas":
pandas.core.frame.DataFrame"pandas.core.frame.DataFrame*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*\
planR
'pyspark.sql.connect.proto.base_pb2.Plan"'pyspark.sql.connect.proto.base_pb2.Plan*˘
_proto_to_stringCpyspark.sql.connect.client.core.SparkConnectClient._proto_to_string"
builtins.str"builtins.str*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*
p
Any*”
schema9pyspark.sql.connect.client.core.SparkConnectClient.schema"<
pyspark.sql.types.StructType"pyspark.sql.types.StructType*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*\
planR
'pyspark.sql.connect.proto.base_pb2.Plan"'pyspark.sql.connect.proto.base_pb2.Plan*ı
explain_stringApyspark.sql.connect.client.core.SparkConnectClient.explain_string"
builtins.str"builtins.str*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*\
planR
'pyspark.sql.connect.proto.base_pb2.Plan"'pyspark.sql.connect.proto.base_pb2.Plan*0
explain_mode
builtins.str"builtins.str *Ÿ
execute_commandBpyspark.sql.connect.client.core.SparkConnectClient.execute_command"û
NTuple[Union[pandas.core.frame.DataFrame,None],builtins.dict[builtins.str,Any]]q
'Union[pandas.core.frame.DataFrame,None]:
pandas.core.frame.DataFrame"pandas.core.frame.DataFrame
NoneW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*m
command`
.pyspark.sql.connect.proto.commands_pb2.Command".pyspark.sql.connect.proto.commands_pb2.Command*§
same_semanticsApyspark.sql.connect.client.core.SparkConnectClient.same_semantics"
builtins.bool"builtins.bool*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*\
planR
'pyspark.sql.connect.proto.base_pb2.Plan"'pyspark.sql.connect.proto.base_pb2.Plan*]
otherR
'pyspark.sql.connect.proto.base_pb2.Plan"'pyspark.sql.connect.proto.base_pb2.Plan*¡
semantic_hash@pyspark.sql.connect.client.core.SparkConnectClient.semantic_hash"
builtins.int"builtins.int*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*\
planR
'pyspark.sql.connect.proto.base_pb2.Plan"'pyspark.sql.connect.proto.base_pb2.Plan*ø
close8pyspark.sql.connect.client.core.SparkConnectClient.close"
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*Î
	is_closed<pyspark.sql.connect.client.core.SparkConnectClient.is_closed"
builtins.bool"builtins.bool*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient0:property`*ﬂ
host7pyspark.sql.connect.client.core.SparkConnectClient.host"
builtins.str"builtins.str*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient0:property`*â
token8pyspark.sql.connect.client.core.SparkConnectClient.token"D
Union[builtins.str,None]
builtins.str"builtins.str
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient0:property`*·
#_execute_plan_request_with_metadataVpyspark.sql.connect.client.core.SparkConnectClient._execute_plan_request_with_metadata"n
5pyspark.sql.connect.proto.base_pb2.ExecutePlanRequest"5pyspark.sql.connect.proto.base_pb2.ExecutePlanRequest*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*·
#_analyze_plan_request_with_metadataVpyspark.sql.connect.client.core.SparkConnectClient._analyze_plan_request_with_metadata"n
5pyspark.sql.connect.proto.base_pb2.AnalyzePlanRequest"5pyspark.sql.connect.proto.base_pb2.AnalyzePlanRequest*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*⁄
_analyze;pyspark.sql.connect.client.core.SparkConnectClient._analyze"^
-pyspark.sql.connect.client.core.AnalyzeResult"-pyspark.sql.connect.client.core.AnalyzeResult*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*(
method
builtins.str"builtins.str*
kwargs
Any*æ
_execute;pyspark.sql.connect.client.core.SparkConnectClient._execute"
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*w
reqn
5pyspark.sql.connect.proto.base_pb2.ExecutePlanRequest"5pyspark.sql.connect.proto.base_pb2.ExecutePlanRequest*∆
_execute_and_fetch_as_iteratorQpyspark.sql.connect.client.core.SparkConnectClient._execute_and_fetch_as_iterator"„
∏typing.Iterator[Union[Any,pyspark.sql.types.StructType,pyspark.sql.connect.client.core.PlanMetrics,pyspark.sql.connect.client.core.PlanObservedMetrics,builtins.dict[builtins.str,Any]]]î
ßUnion[Any,pyspark.sql.types.StructType,pyspark.sql.connect.client.core.PlanMetrics,pyspark.sql.connect.client.core.PlanObservedMetrics,builtins.dict[builtins.str,Any]]
Any<
pyspark.sql.types.StructType"pyspark.sql.types.StructTypeZ
+pyspark.sql.connect.client.core.PlanMetrics"+pyspark.sql.connect.client.core.PlanMetricsj
3pyspark.sql.connect.client.core.PlanObservedMetrics"3pyspark.sql.connect.client.core.PlanObservedMetricsW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict"typing.Iterator*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*w
reqn
5pyspark.sql.connect.proto.base_pb2.ExecutePlanRequest"5pyspark.sql.connect.proto.base_pb2.ExecutePlanRequest*≈	
_execute_and_fetchEpyspark.sql.connect.client.core.SparkConnectClient._execute_and_fetch"≈
›Tuple[Union[Any,None],Union[pyspark.sql.types.StructType,None],builtins.list[pyspark.sql.connect.client.core.PlanMetrics],builtins.list[pyspark.sql.connect.client.core.PlanObservedMetrics],builtins.dict[builtins.str,Any]]&
Union[Any,None]
Any
Nonet
(Union[pyspark.sql.types.StructType,None]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
Noneß
:builtins.list[pyspark.sql.connect.client.core.PlanMetrics]Z
+pyspark.sql.connect.client.core.PlanMetrics"+pyspark.sql.connect.client.core.PlanMetrics"builtins.listø
Bbuiltins.list[pyspark.sql.connect.client.core.PlanObservedMetrics]j
3pyspark.sql.connect.client.core.PlanObservedMetrics"3pyspark.sql.connect.client.core.PlanObservedMetrics"builtins.listW
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*w
reqn
5pyspark.sql.connect.proto.base_pb2.ExecutePlanRequest"5pyspark.sql.connect.proto.base_pb2.ExecutePlanRequest*3
self_destruct
builtins.bool"builtins.bool *À
_config_request_with_metadataPpyspark.sql.connect.client.core.SparkConnectClient._config_request_with_metadata"d
0pyspark.sql.connect.proto.base_pb2.ConfigRequest"0pyspark.sql.connect.proto.base_pb2.ConfigRequest*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*Ï
get_configs>pyspark.sql.connect.client.core.SparkConnectClient.get_configs"Ä
(builtins.tuple[Union[builtins.str,None]]D
Union[builtins.str,None]
builtins.str"builtins.str
None"builtins.tuple*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*&
keys
builtins.str"builtins.str*Å
get_config_with_defaultsKpyspark.sql.connect.client.core.SparkConnectClient.get_config_with_defaults"Ä
(builtins.tuple[Union[builtins.str,None]]D
Union[builtins.str,None]
builtins.str"builtins.str
None"builtins.tuple*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*†
pairsî
,Tuple[builtins.str,Union[builtins.str,None]]
builtins.str"builtins.strD
Union[builtins.str,None]
builtins.str"builtins.str
None*ü
config9pyspark.sql.connect.client.core.SparkConnectClient.config"\
,pyspark.sql.connect.client.core.ConfigResult",pyspark.sql.connect.client.core.ConfigResult*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*á
	operationx
:pyspark.sql.connect.proto.base_pb2.ConfigRequest.Operation":pyspark.sql.connect.proto.base_pb2.ConfigRequest.Operation*ƒ
_interrupt_requestEpyspark.sql.connect.client.core.SparkConnectClient._interrupt_request"j
3pyspark.sql.connect.proto.base_pb2.InterruptRequest"3pyspark.sql.connect.proto.base_pb2.InterruptRequest*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*0
interrupt_type
builtins.str"builtins.str*U
	id_or_tagD
Union[builtins.str,None]
builtins.str"builtins.str
None *…
interrupt_all@pyspark.sql.connect.client.core.SparkConnectClient.interrupt_all"Å
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*
interrupt_tag@pyspark.sql.connect.client.core.SparkConnectClient.interrupt_tag"Å
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*%
tag
builtins.str"builtins.str*˛
interrupt_operationFpyspark.sql.connect.client.core.SparkConnectClient.interrupt_operation"Å
'Union[builtins.list[builtins.str],None]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*'
op_id
builtins.str"builtins.str*Í
add_tag:pyspark.sql.connect.client.core.SparkConnectClient.add_tag"
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*%
tag
builtins.str"builtins.str*

remove_tag=pyspark.sql.connect.client.core.SparkConnectClient.remove_tag"
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*%
tag
builtins.str"builtins.str*Ö
get_tags;pyspark.sql.connect.client.core.SparkConnectClient.get_tags"H
builtins.set[builtins.str]
builtins.str"builtins.str"builtins.set*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*…

clear_tags=pyspark.sql.connect.client.core.SparkConnectClient.clear_tags"
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*Ü
_throw_if_invalid_tagHpyspark.sql.connect.client.core.SparkConnectClient._throw_if_invalid_tag"
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*%
tag
builtins.str"builtins.str*à
_handle_error@pyspark.sql.connect.client.core.SparkConnectClient._handle_error"
NoReturn
*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*3
error(
builtins.Exception"builtins.Exception*Û
_handle_rpc_errorDpyspark.sql.connect.client.core.SparkConnectClient._handle_rpc_error"
NoReturn
*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*
	rpc_error
Any*˙
add_artifacts@pyspark.sql.connect.client.core.SparkConnectClient.add_artifacts"
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*&
path
builtins.str"builtins.str**
pyfile
builtins.bool"builtins.bool*+
archive
builtins.bool"builtins.bool*(
file
builtins.bool"builtins.bool*∫
copy_from_local_to_fsHpyspark.sql.connect.client.core.SparkConnectClient.copy_from_local_to_fs"
None*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient*,

local_path
builtins.str"builtins.str*+
	dest_path
builtins.str"builtins.str*ë
cache_artifactApyspark.sql.connect.client.core.SparkConnectClient.cache_artifact"
builtins.str"builtins.str*r
selfh
2pyspark.sql.connect.client.core.SparkConnectClient"2pyspark.sql.connect.client.core.SparkConnectClient**
blob 
builtins.bytes"builtins.bytesrs
thread_local?pyspark.sql.connect.client.core.SparkConnectClient.thread_local"
threading.local"threading.localr©
_builder;pyspark.sql.connect.client.core.SparkConnectClient._builder`
.pyspark.sql.connect.client.core.ChannelBuilder".pyspark.sql.connect.client.core.ChannelBuilderrç
_user_id;pyspark.sql.connect.client.core.SparkConnectClient._user_idD
Union[builtins.str,None]
builtins.str"builtins.str
NonerŒ
_retry_policy@pyspark.sql.connect.client.core.SparkConnectClient._retry_policy{
*builtins.dict[builtins.str,builtins.float]
builtins.str"builtins.str 
builtins.float"builtins.float"builtins.dictrk
_session_id>pyspark.sql.connect.client.core.SparkConnectClient._session_id
builtins.str"builtins.strrP
_channel;pyspark.sql.connect.client.core.SparkConnectClient._channel
Anyre
_closed:pyspark.sql.connect.client.core.SparkConnectClient._closed
builtins.bool"builtins.boolr∆
_stub8pyspark.sql.connect.client.core.SparkConnectClient._stubÇ
?pyspark.sql.connect.proto.base_pb2_grpc.SparkConnectServiceStub"?pyspark.sql.connect.proto.base_pb2_grpc.SparkConnectServiceStubr≈
_artifact_managerDpyspark.sql.connect.client.core.SparkConnectClient._artifact_managerj
3pyspark.sql.connect.client.artifact.ArtifactManager"3pyspark.sql.connect.client.artifact.ArtifactManagerrâ
_use_reattachable_executeLpyspark.sql.connect.client.core.SparkConnectClient._use_reattachable_execute
builtins.bool"builtins.bool§

RetryState*pyspark.sql.connect.client.core.RetryState"builtins.object*≠
__init__3pyspark.sql.connect.client.core.RetryState.__init__"
None*b
selfX
*pyspark.sql.connect.client.core.RetryState"*pyspark.sql.connect.client.core.RetryState*Ú
set_exception8pyspark.sql.connect.client.core.RetryState.set_exception"
None*b
selfX
*pyspark.sql.connect.client.core.RetryState"*pyspark.sql.connect.client.core.RetryState*9
exc0
builtins.BaseException"builtins.BaseException*ß
throw0pyspark.sql.connect.client.core.RetryState.throw"
None*b
selfX
*pyspark.sql.connect.client.core.RetryState"*pyspark.sql.connect.client.core.RetryState*≠
set_done3pyspark.sql.connect.client.core.RetryState.set_done"
None*b
selfX
*pyspark.sql.connect.client.core.RetryState"*pyspark.sql.connect.client.core.RetryState*ª
count0pyspark.sql.connect.client.core.RetryState.count"
builtins.int"builtins.int*b
selfX
*pyspark.sql.connect.client.core.RetryState"*pyspark.sql.connect.client.core.RetryState*ª
done/pyspark.sql.connect.client.core.RetryState.done"
builtins.bool"builtins.bool*b
selfX
*pyspark.sql.connect.client.core.RetryState"*pyspark.sql.connect.client.core.RetryStaterß

_exception5pyspark.sql.connect.client.core.RetryState._exceptionb
"Union[builtins.BaseException,None]0
builtins.BaseException"builtins.BaseException
NonerY
_done0pyspark.sql.connect.client.core.RetryState._done
builtins.bool"builtins.boolrY
_count1pyspark.sql.connect.client.core.RetryState._count
builtins.int"builtins.intÎ
AttemptManager.pyspark.sql.connect.client.core.AttemptManager"builtins.object*¸
__init__7pyspark.sql.connect.client.core.AttemptManager.__init__"
None*j
self`
.pyspark.sql.connect.client.core.AttemptManager".pyspark.sql.connect.client.core.AttemptManager*V
checkK
CallableType[builtins.function]&
builtins.function"builtins.function*i
retry_stateX
*pyspark.sql.connect.client.core.RetryState"*pyspark.sql.connect.client.core.RetryState*≥
	__enter__8pyspark.sql.connect.client.core.AttemptManager.__enter__"
None*b`
.pyspark.sql.connect.client.core.AttemptManager".pyspark.sql.connect.client.core.AttemptManager*…
__exit__7pyspark.sql.connect.client.core.AttemptManager.__exit__"G
Union[builtins.bool,None]
builtins.bool"builtins.bool
None*b`
.pyspark.sql.connect.client.core.AttemptManager".pyspark.sql.connect.client.core.AttemptManager*ìê
(Union[Type[builtins.BaseException],None]X
Type[builtins.BaseException]0
builtins.BaseException"builtins.BaseException"type
None*db
"Union[builtins.BaseException,None]0
builtins.BaseException"builtins.BaseException
None*[Y
Union[types.TracebackType,None]*
types.TracebackType"types.TracebackType
None*◊
is_first_try;pyspark.sql.connect.client.core.AttemptManager.is_first_try"
builtins.bool"builtins.bool*j
self`
.pyspark.sql.connect.client.core.AttemptManager".pyspark.sql.connect.client.core.AttemptManagerr•
_retry_state;pyspark.sql.connect.client.core.AttemptManager._retry_stateX
*pyspark.sql.connect.client.core.RetryState"*pyspark.sql.connect.client.core.RetryStaterî

_can_retry9pyspark.sql.connect.client.core.AttemptManager._can_retryK
CallableType[builtins.function]&
builtins.function"builtins.function›
Retrying(pyspark.sql.connect.client.core.Retrying"builtins.object*å
__init__1pyspark.sql.connect.client.core.Retrying.__init__"
None*^
selfT
(pyspark.sql.connect.client.core.Retrying"(pyspark.sql.connect.client.core.Retrying*-
max_retries
builtins.int"builtins.int*1
initial_backoff
builtins.int"builtins.int*-
max_backoff
builtins.int"builtins.int*8
backoff_multiplier 
builtins.float"builtins.float*(
jitter
builtins.int"builtins.int*6
min_jitter_threshold
builtins.int"builtins.int*\
	can_retryK
CallableType[builtins.function]&
builtins.function"builtins.function *X
sleepK
CallableType[builtins.function]&
builtins.function"builtins.function *Ï
__iter__1pyspark.sql.connect.client.core.Retrying.__iter__"‘
Jtyping.Generator[pyspark.sql.connect.client.core.AttemptManager,None,None]`
.pyspark.sql.connect.client.core.AttemptManager".pyspark.sql.connect.client.core.AttemptManager
None
None"typing.Generator*VT
(pyspark.sql.connect.client.core.Retrying"(pyspark.sql.connect.client.core.Retryingré

_can_retry3pyspark.sql.connect.client.core.Retrying._can_retryK
CallableType[builtins.function]&
builtins.function"builtins.functionrc
_max_retries5pyspark.sql.connect.client.core.Retrying._max_retries
builtins.int"builtins.intrk
_initial_backoff9pyspark.sql.connect.client.core.Retrying._initial_backoff
builtins.int"builtins.intrc
_max_backoff5pyspark.sql.connect.client.core.Retrying._max_backoff
builtins.int"builtins.intru
_backoff_multiplier<pyspark.sql.connect.client.core.Retrying._backoff_multiplier 
builtins.float"builtins.floatrY
_jitter0pyspark.sql.connect.client.core.Retrying._jitter
builtins.int"builtins.intru
_min_jitter_threshold>pyspark.sql.connect.client.core.Retrying._min_jitter_threshold
builtins.int"builtins.intrÜ
_sleep/pyspark.sql.connect.client.core.Retrying._sleepK
CallableType[builtins.function]&
builtins.function"builtins.functionj
_configure_logging2pyspark.sql.connect.client.core._configure_logging" 
logging.Logger"logging.LoggerÄ
getLogLevel+pyspark.sql.connect.client.core.getLogLevel"D
Union[builtins.int,None]
builtins.int"builtins.int
None*õ
__annotations__/pyspark.sql.connect.client.core.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*~
__all__'pyspark.sql.connect.client.core.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*
urlliburllib *
pdpandas *1
pa"pyspark.sql.connect.client.core.pa
Any*9
google&pyspark.sql.connect.client.core.google
Any*A

rpc_status*pyspark.sql.connect.client.core.rpc_status
Any*5
grpc$pyspark.sql.connect.client.core.grpc
Any*C
text_format+pyspark.sql.connect.client.core.text_format
Any*O
error_details_pb21pyspark.sql.connect.client.core.error_details_pb2
Any*"
pb2pyspark.sql.connect.proto *5
grpc_lib'pyspark.sql.connect.proto.base_pb2_grpc *$
typespyspark.sql.connect.types *R
logger&pyspark.sql.connect.client.core.logger 
logging.Logger"logging.Logger