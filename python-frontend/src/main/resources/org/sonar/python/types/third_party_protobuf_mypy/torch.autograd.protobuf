
torch.autograd£
detect_anomaly*torch.autograd.anomaly_mode.detect_anomaly"builtins.object*Ç
__init__3torch.autograd.anomaly_mode.detect_anomaly.__init__"
None*b
selfX
*torch.autograd.anomaly_mode.detect_anomaly"*torch.autograd.anomaly_mode.detect_anomaly*
	check_nan
Any *§
	__enter__4torch.autograd.anomaly_mode.detect_anomaly.__enter__"
None*ZX
*torch.autograd.anomaly_mode.detect_anomaly"*torch.autograd.anomaly_mode.detect_anomaly*Í
__exit__3torch.autograd.anomaly_mode.detect_anomaly.__exit__"
None*ZX
*torch.autograd.anomaly_mode.detect_anomaly"*torch.autograd.anomaly_mode.detect_anomaly*&"
builtins.object"builtins.objectrW
prev/torch.autograd.anomaly_mode.detect_anomaly.prev
builtins.bool"builtins.boolrJ
	check_nan4torch.autograd.anomaly_mode.detect_anomaly.check_nan
Anyrk
prev_check_nan9torch.autograd.anomaly_mode.detect_anomaly.prev_check_nan
builtins.bool"builtins.boolÌ
set_detect_anomaly.torch.autograd.anomaly_mode.set_detect_anomaly"builtins.object*”
__init__7torch.autograd.anomaly_mode.set_detect_anomaly.__init__"
None*j
self`
.torch.autograd.anomaly_mode.set_detect_anomaly".torch.autograd.anomaly_mode.set_detect_anomaly*(
mode
builtins.bool"builtins.bool*/
	check_nan
builtins.bool"builtins.bool *³
	__enter__8torch.autograd.anomaly_mode.set_detect_anomaly.__enter__"
None*b`
.torch.autograd.anomaly_mode.set_detect_anomaly".torch.autograd.anomaly_mode.set_detect_anomaly*Ù
__exit__7torch.autograd.anomaly_mode.set_detect_anomaly.__exit__"
None*b`
.torch.autograd.anomaly_mode.set_detect_anomaly".torch.autograd.anomaly_mode.set_detect_anomaly*&"
builtins.object"builtins.objectr[
prev3torch.autograd.anomaly_mode.set_detect_anomaly.prev
builtins.bool"builtins.boolro
prev_check_nan=torch.autograd.anomaly_mode.set_detect_anomaly.prev_check_nan
builtins.bool"builtins.bool¤
Function torch.autograd.function.Function",torch.autograd.function._SingleLevelFunction*U
__init__)torch.autograd.function.Function.__init__*
self*
args*

kwargs*U
__call__)torch.autograd.function.Function.__call__*
self*
args*

kwargs*`
vmap%torch.autograd.function.Function.vmap*
info*
in_dims*
args0:staticmethodh*_
apply&torch.autograd.function.Function.apply*
cls*
args*

kwargs0:classmethodp*l
_compiled_autograd_key7torch.autograd.function.Function._compiled_autograd_key*
ctx0:staticmethodhri
generate_vmap_rule3torch.autograd.function.Function.generate_vmap_rule
builtins.bool"builtins.boolË
NestedIOFunction(torch.autograd.function.NestedIOFunction" torch.autograd.function.Function*X
_do_forward4torch.autograd.function.NestedIOFunction._do_forward*
self*	
input*t
_do_backward5torch.autograd.function.NestedIOFunction._do_backward*
self*
	gradients*
retain_variables*¾
backward1torch.autograd.function.NestedIOFunction.backward"
Any*^
selfT
(torch.autograd.function.NestedIOFunction"(torch.autograd.function.NestedIOFunction*
	gradients
Any*·
forward0torch.autograd.function.NestedIOFunction.forward"
Any*^
selfT
(torch.autograd.function.NestedIOFunction"(torch.autograd.function.NestedIOFunction*
args
Any*Ì
save_for_backward:torch.autograd.function.NestedIOFunction.save_for_backward"
None*^
selfT
(torch.autograd.function.NestedIOFunction"(torch.autograd.function.NestedIOFunction*
args
Any*_
saved_tensors6torch.autograd.function.NestedIOFunction.saved_tensors*
self0:property`*Ó

mark_dirty3torch.autograd.function.NestedIOFunction.mark_dirty"
None*^
selfT
(torch.autograd.function.NestedIOFunction"(torch.autograd.function.NestedIOFunction*
args
Any*
kwargs
Any*í
mark_non_differentiable@torch.autograd.function.NestedIOFunction.mark_non_differentiable"
None*^
selfT
(torch.autograd.function.NestedIOFunction"(torch.autograd.function.NestedIOFunction*
args
Any*
kwargs
Any*Ë
forward_extended9torch.autograd.function.NestedIOFunction.forward_extended"
None*^
selfT
(torch.autograd.function.NestedIOFunction"(torch.autograd.function.NestedIOFunction*
input
Any*Ó
backward_extended:torch.autograd.function.NestedIOFunction.backward_extended"
None*^
selfT
(torch.autograd.function.NestedIOFunction"(torch.autograd.function.NestedIOFunction*
grad_output
AnyrŠ
__call__1torch.autograd.function.NestedIOFunction.__call__K
CallableType[builtins.function]&
builtins.function"builtins.functionrP
_nested_input6torch.autograd.function.NestedIOFunction._nested_input
AnyrV
retain_variables9torch.autograd.function.NestedIOFunction.retain_variables
AnyrR
_nested_output7torch.autograd.function.NestedIOFunction._nested_output
Anyr{
_to_save_nested8torch.autograd.function.NestedIOFunction._to_save_nested.
builtins.tuple[Any]
Any"builtins.tupleÝ
_force_original_view_tracking6torch.autograd.grad_mode._force_original_view_tracking"0torch.utils._contextlib._DecoratorContextManager*û
__init__?torch.autograd.grad_mode._force_original_view_tracking.__init__"
None*z
selfp
6torch.autograd.grad_mode._force_original_view_tracking"6torch.autograd.grad_mode._force_original_view_tracking*(
mode
builtins.bool"builtins.bool*Ë
	__enter__@torch.autograd.grad_mode._force_original_view_tracking.__enter__"
None*rp
6torch.autograd.grad_mode._force_original_view_tracking"6torch.autograd.grad_mode._force_original_view_tracking*ê
__exit__?torch.autograd.grad_mode._force_original_view_tracking.__exit__"
None*rp
6torch.autograd.grad_mode._force_original_view_tracking"6torch.autograd.grad_mode._force_original_view_tracking*	
Any*	
Any*	
Any*O
clone<torch.autograd.grad_mode._force_original_view_tracking.clone*
selfrc
prev;torch.autograd.grad_mode._force_original_view_tracking.prev
builtins.bool"builtins.boolrc
mode;torch.autograd.grad_mode._force_original_view_tracking.mode
builtins.bool"builtins.boolÐ
 _unsafe_preserve_version_counter9torch.autograd.grad_mode._unsafe_preserve_version_counter"0torch.utils._contextlib._DecoratorContextManager*•
__init__Btorch.autograd.grad_mode._unsafe_preserve_version_counter.__init__"
None*€
selfv
9torch.autograd.grad_mode._unsafe_preserve_version_counter"9torch.autograd.grad_mode._unsafe_preserve_version_counter*8
tensor,
torch._tensor.Tensor"torch._tensor.Tensor*Ô
	__enter__Ctorch.autograd.grad_mode._unsafe_preserve_version_counter.__enter__"
None*xv
9torch.autograd.grad_mode._unsafe_preserve_version_counter"9torch.autograd.grad_mode._unsafe_preserve_version_counter*ß
__exit__Btorch.autograd.grad_mode._unsafe_preserve_version_counter.__exit__"
None*xv
9torch.autograd.grad_mode._unsafe_preserve_version_counter"9torch.autograd.grad_mode._unsafe_preserve_version_counter*
Anyrx
tensor@torch.autograd.grad_mode._unsafe_preserve_version_counter.tensor,
torch._tensor.Tensor"torch._tensor.Tensorrt
prev_versionFtorch.autograd.grad_mode._unsafe_preserve_version_counter.prev_version
builtins.int"builtins.intŽ
enable_grad$torch.autograd.grad_mode.enable_grad"7torch.utils._contextlib._NoParamDecoratorContextManager*•
	__enter__.torch.autograd.grad_mode.enable_grad.__enter__"
None*NL
$torch.autograd.grad_mode.enable_grad"$torch.autograd.grad_mode.enable_grad*´
__exit__-torch.autograd.grad_mode.enable_grad.__exit__"
None*NL
$torch.autograd.grad_mode.enable_grad"$torch.autograd.grad_mode.enable_grad*	
Any*	
Any*	
AnyrQ
prev)torch.autograd.grad_mode.enable_grad.prev
builtins.bool"builtins.boolÂ	
inference_mode'torch.autograd.grad_mode.inference_mode"0torch.utils._contextlib._DecoratorContextManager*Ð
__init__0torch.autograd.grad_mode.inference_mode.__init__"
None*\
selfR
'torch.autograd.grad_mode.inference_mode"'torch.autograd.grad_mode.inference_mode**
mode
builtins.bool"builtins.bool *O
__new__/torch.autograd.grad_mode.inference_mode.__new__*
cls*

mode *ž
	__enter__1torch.autograd.grad_mode.inference_mode.__enter__"
None*TR
'torch.autograd.grad_mode.inference_mode"'torch.autograd.grad_mode.inference_mode*½
__exit__0torch.autograd.grad_mode.inference_mode.__exit__"
None*TR
'torch.autograd.grad_mode.inference_mode"'torch.autograd.grad_mode.inference_mode*	
Any*	
Any*	
Any*è
clone-torch.autograd.grad_mode.inference_mode.clone"R
'torch.autograd.grad_mode.inference_mode"'torch.autograd.grad_mode.inference_mode*\
selfR
'torch.autograd.grad_mode.inference_mode"'torch.autograd.grad_mode.inference_moderT
mode,torch.autograd.grad_mode.inference_mode.mode
builtins.bool"builtins.boolrŽ
_inference_mode_context?torch.autograd.grad_mode.inference_mode._inference_mode_context2
torch._C._InferenceMode"torch._C._InferenceModeü
no_grad torch.autograd.grad_mode.no_grad"7torch.utils._contextlib._NoParamDecoratorContextManager*
__init__)torch.autograd.grad_mode.no_grad.__init__"
None*N
selfD
 torch.autograd.grad_mode.no_grad" torch.autograd.grad_mode.no_grad*‰
	__enter__*torch.autograd.grad_mode.no_grad.__enter__"
None*FD
 torch.autograd.grad_mode.no_grad" torch.autograd.grad_mode.no_grad*¨
__exit__)torch.autograd.grad_mode.no_grad.__exit__"
None*FD
 torch.autograd.grad_mode.no_grad" torch.autograd.grad_mode.no_grad*	
Any*	
Any*	
AnyrM
prev%torch.autograd.grad_mode.no_grad.prev
builtins.bool"builtins.boolâ
set_grad_enabled)torch.autograd.grad_mode.set_grad_enabled"0torch.utils._contextlib._DecoratorContextManager*Ô
__init__2torch.autograd.grad_mode.set_grad_enabled.__init__"
None*`
selfV
)torch.autograd.grad_mode.set_grad_enabled")torch.autograd.grad_mode.set_grad_enabled*(
mode
builtins.bool"builtins.bool*‡
__call__2torch.autograd.grad_mode.set_grad_enabled.__call__"j
torch.utils._contextlib.FK
CallableType[builtins.function]&
builtins.function"builtins.function*`
selfV
)torch.autograd.grad_mode.set_grad_enabled")torch.autograd.grad_mode.set_grad_enabled*y
	orig_funcj
torch.utils._contextlib.FK
CallableType[builtins.function]&
builtins.function"builtins.function*¤
	__enter__3torch.autograd.grad_mode.set_grad_enabled.__enter__"
None*XV
)torch.autograd.grad_mode.set_grad_enabled")torch.autograd.grad_mode.set_grad_enabled*Ã
__exit__2torch.autograd.grad_mode.set_grad_enabled.__exit__"
None*XV
)torch.autograd.grad_mode.set_grad_enabled")torch.autograd.grad_mode.set_grad_enabled*	
Any*	
Any*	
Any*ò
clone/torch.autograd.grad_mode.set_grad_enabled.clone"V
)torch.autograd.grad_mode.set_grad_enabled")torch.autograd.grad_mode.set_grad_enabled*`
selfV
)torch.autograd.grad_mode.set_grad_enabled")torch.autograd.grad_mode.set_grad_enabledrV
prev.torch.autograd.grad_mode.set_grad_enabled.prev
builtins.bool"builtins.boolrV
mode.torch.autograd.grad_mode.set_grad_enabled.mode
builtins.bool"builtins.boolŒ

set_multithreading_enabled3torch.autograd.grad_mode.set_multithreading_enabled"0torch.utils._contextlib._DecoratorContextManager*ò
__init__<torch.autograd.grad_mode.set_multithreading_enabled.__init__"
None*t
selfj
3torch.autograd.grad_mode.set_multithreading_enabled"3torch.autograd.grad_mode.set_multithreading_enabled*(
mode
builtins.bool"builtins.bool*Â
	__enter__=torch.autograd.grad_mode.set_multithreading_enabled.__enter__"
None*lj
3torch.autograd.grad_mode.set_multithreading_enabled"3torch.autograd.grad_mode.set_multithreading_enabled*á
__exit__<torch.autograd.grad_mode.set_multithreading_enabled.__exit__"
None*lj
3torch.autograd.grad_mode.set_multithreading_enabled"3torch.autograd.grad_mode.set_multithreading_enabled*	
Any*	
Any*	
Any*¤
clone9torch.autograd.grad_mode.set_multithreading_enabled.clone"j
3torch.autograd.grad_mode.set_multithreading_enabled"3torch.autograd.grad_mode.set_multithreading_enabled*t
selfj
3torch.autograd.grad_mode.set_multithreading_enabled"3torch.autograd.grad_mode.set_multithreading_enabledr`
prev8torch.autograd.grad_mode.set_multithreading_enabled.prev
builtins.bool"builtins.boolr`
mode8torch.autograd.grad_mode.set_multithreading_enabled.mode
builtins.bool"builtins.boolö
Variable torch.autograd.variable.Variable"torch._C._LegacyVariableBase@b$torch.autograd.variable.VariableMetar
_execution_engine2torch.autograd.variable.Variable._execution_engine8
torch._C._ImperativeEngine"torch._C._ImperativeEngineà	
	gradcheck"torch.autograd.gradcheck.gradcheck"
builtins.bool"builtins.bool*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*Ù
inputsÌ
LTypeAlias[Union[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor]]]Û
AUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor]],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence"torch.types._TensorOrTensors*+
eps 
builtins.float"builtins.float *,
atol 
builtins.float"builtins.float *,
rtol 
builtins.float"builtins.float *5
raise_exception
builtins.bool"builtins.bool *2

nondet_tol 
builtins.float"builtins.float *:
check_undefined_grad
builtins.bool"builtins.bool *7
check_grad_dtypes
builtins.bool"builtins.bool *8
check_batched_grad
builtins.bool"builtins.bool *@
check_batched_forward_grad
builtins.bool"builtins.bool *6
check_forward_ad
builtins.bool"builtins.bool *7
check_backward_ad
builtins.bool"builtins.bool */
	fast_mode
builtins.bool"builtins.bool *U
maskedG
Union[builtins.bool,None]
builtins.bool"builtins.bool
None Å
gradgradcheck&torch.autograd.gradcheck.gradgradcheck"
builtins.bool"builtins.bool*U
funcK
CallableType[builtins.function]&
builtins.function"builtins.function*Ù
inputsÌ
LTypeAlias[Union[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor]]]Û
AUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor]],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence"torch.types._TensorOrTensors*ÿ
grad_outputsê
FUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor],None],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence
None *+
eps 
builtins.float"builtins.float *,
atol 
builtins.float"builtins.float *,
rtol 
builtins.float"builtins.float *A
gen_non_contig_grad_outputs
builtins.bool"builtins.bool *5
raise_exception
builtins.bool"builtins.bool *2

nondet_tol 
builtins.float"builtins.float *:
check_undefined_grad
builtins.bool"builtins.bool *7
check_grad_dtypes
builtins.bool"builtins.bool *8
check_batched_grad
builtins.bool"builtins.bool *8
check_fwd_over_rev
builtins.bool"builtins.bool *8
check_rev_over_rev
builtins.bool"builtins.bool */
	fast_mode
builtins.bool"builtins.bool *,
masked
builtins.bool"builtins.bool f
_engine_run_backward)torch.autograd.graph._engine_run_backward*
	t_outputs*
args*

kwargsÝ
_calculate_shapetorch.autograd._calculate_shape" *8
output,
torch._tensor.Tensor"torch._tensor.Tensor*6
grad,
torch._tensor.Tensor"torch._tensor.Tensor*4
is_grads_batched
builtins.bool"builtins.boolâ
_make_gradstorch.autograd._make_grads"ý
;builtins.tuple[TypeAlias[Union[torch._tensor.Tensor,None]]]­
+TypeAlias[Union[torch._tensor.Tensor,None]]\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None"torch.autograd._OptionalTensor"builtins.tuple*s
outputsf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence*‹
gradsÿ
<typing.Sequence[TypeAlias[Union[torch._tensor.Tensor,None]]]­
+TypeAlias[Union[torch._tensor.Tensor,None]]\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None"torch.autograd._OptionalTensor"typing.Sequence*4
is_grads_batched
builtins.bool"builtins.boolî
_tensor_or_tensors_to_tuple*torch.autograd._tensor_or_tensors_to_tuple"ý
;builtins.tuple[TypeAlias[Union[torch._tensor.Tensor,None]]]­
+TypeAlias[Union[torch._tensor.Tensor,None]]\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None"torch.autograd._OptionalTensor"builtins.tuple*ø
tensorsê
FUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor],None],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence
None*(
length
builtins.int"builtins.intç
backwardtorch.autograd.backward"
None*Ú
tensorsÌ
LTypeAlias[Union[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor]]]Û
AUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor]],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence"torch.types._TensorOrTensors*ÿ
grad_tensorsê
FUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor],None],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence
None *[
retain_graphG
Union[builtins.bool,None]
builtins.bool"builtins.bool
None *2
create_graph
builtins.bool"builtins.bool *
grad_variablesê
FUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor],None],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence
None *Ã
inputs´
yUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor],TypeAlias[Tuple[Any,Any]],typing.Sequence[Unknown],None],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequencef
TypeAlias[Tuple[Any,Any]]$
Tuple[Any,Any]
Any
Any"!torch.autograd.graph.GradientEdge-
typing.Sequence[Unknown] "typing.Sequence
None Õ
gradtorch.autograd.grad"d
$builtins.tuple[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.tuple*Ú
outputsÌ
LTypeAlias[Union[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor]]]Û
AUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor]],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence"torch.types._TensorOrTensors*à
inputsÓ
TypeAlias[Union[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor],TypeAlias[Tuple[Any,Any]],typing.Sequence[Unknown]]]¥
tUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor],TypeAlias[Tuple[Any,Any]],typing.Sequence[Unknown]],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequencef
TypeAlias[Tuple[Any,Any]]$
Tuple[Any,Any]
Any
Any"!torch.autograd.graph.GradientEdge-
typing.Sequence[Unknown] "typing.Sequence"&torch.types._TensorOrTensorsOrGradEdge*ÿ
grad_outputsê
FUnion[torch._tensor.Tensor,typing.Sequence[torch._tensor.Tensor],None],
torch._tensor.Tensor"torch._tensor.Tensorf
%typing.Sequence[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"typing.Sequence
None *[
retain_graphG
Union[builtins.bool,None]
builtins.bool"builtins.bool
None *2
create_graph
builtins.bool"builtins.bool *1
only_inputs
builtins.bool"builtins.bool *[
allow_unusedG
Union[builtins.bool,None]
builtins.bool"builtins.bool
None *6
is_grads_batched
builtins.bool"builtins.bool *7
materialize_grads
builtins.bool"builtins.bool ;
_is_checkpoint_valid#torch.autograd._is_checkpoint_valid9
variabletorch.autograd.variable*
args*

kwargsp
$_register_py_tensor_class_for_device3torch.autograd._register_py_tensor_class_for_device*

device*
cls*o
__path__torch.autograd.__path__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*Š
__annotations__torch.autograd.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*)

forward_adtorch.autograd.forward_ad *)

functionaltorch.autograd.functional *
graphtorch.autograd.graph *m
__all__torch.autograd.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*%
profilertorch.autograd.profiler *’
is_multithreading_enabled(torch.autograd.is_multithreading_enabledK
CallableType[builtins.function]&
builtins.function"builtins.function*Œ
is_view_replay_enabled%torch.autograd.is_view_replay_enabledK
CallableType[builtins.function]&
builtins.function"builtins.function