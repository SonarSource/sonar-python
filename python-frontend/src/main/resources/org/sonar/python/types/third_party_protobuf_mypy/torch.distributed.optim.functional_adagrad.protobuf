
*torch.distributed.optim.functional_adagradù
_FunctionalAdagrad=torch.distributed.optim.functional_adagrad._FunctionalAdagrad"builtins.object*Ó
__init__Ftorch.distributed.optim.functional_adagrad._FunctionalAdagrad.__init__"
None*ˆ
self~
=torch.distributed.optim.functional_adagrad._FunctionalAdagrad"=torch.distributed.optim.functional_adagrad._FunctionalAdagrad*n
paramsb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list**
lr 
builtins.float"builtins.float *0
lr_decay 
builtins.float"builtins.float *4
weight_decay 
builtins.float"builtins.float *A
initial_accumulator_value 
builtins.float"builtins.float *<
warmup_lr_multiplier 
builtins.float"builtins.float *8
warmup_num_iters 
builtins.float"builtins.float *+
eps 
builtins.float"builtins.float *3
coalesce_grad
builtins.bool"builtins.bool *-
foreach
builtins.bool"builtins.bool *+
fused
builtins.bool"builtins.bool *.
maximize
builtins.bool"builtins.bool *=
_allow_empty_param_list
builtins.bool"builtins.bool *
stepBtorch.distributed.optim.functional_adagrad._FunctionalAdagrad.step"
Any*ˆ
self~
=torch.distributed.optim.functional_adagrad._FunctionalAdagrad"=torch.distributed.optim.functional_adagrad._FunctionalAdagrad*®
	gradientsž
/builtins.list[Union[torch._tensor.Tensor,None]]\
 Union[torch._tensor.Tensor,None],
torch._tensor.Tensor"torch._tensor.Tensor
None"builtins.list8rÏ
defaultsFtorch.distributed.optim.functional_adagrad._FunctionalAdagrad.defaults{
*builtins.dict[builtins.str,builtins.float]
builtins.str"builtins.str 
builtins.float"builtins.float"builtins.dictr|
coalesce_gradKtorch.distributed.optim.functional_adagrad._FunctionalAdagrad.coalesce_grad
builtins.bool"builtins.boolrp
foreachEtorch.distributed.optim.functional_adagrad._FunctionalAdagrad.foreach
builtins.bool"builtins.boolrl
fusedCtorch.distributed.optim.functional_adagrad._FunctionalAdagrad.fused
builtins.bool"builtins.boolrr
maximizeFtorch.distributed.optim.functional_adagrad._FunctionalAdagrad.maximize
builtins.bool"builtins.boolrU
stateCtorch.distributed.optim.functional_adagrad._FunctionalAdagrad.state
Anyr­
param_groupItorch.distributed.optim.functional_adagrad._FunctionalAdagrad.param_groupÒ
?builtins.dict[builtins.str,builtins.list[torch._tensor.Tensor]]
builtins.str"builtins.strb
#builtins.list[torch._tensor.Tensor],
torch._tensor.Tensor"torch._tensor.Tensor"builtins.list"builtins.dict*¦
__annotations__:torch.distributed.optim.functional_adagrad.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*
Ftorch.optim._functional *‰
__all__2torch.distributed.optim.functional_adagrad.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list