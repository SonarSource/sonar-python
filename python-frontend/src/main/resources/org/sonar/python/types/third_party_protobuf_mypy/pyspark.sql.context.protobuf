
pyspark.sql.contextÓ^

SQLContextpyspark.sql.context.SQLContext"builtins.object*¬
__init__'pyspark.sql.context.SQLContext.__init__"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*N
sparkContext<
pyspark.context.SparkContext"pyspark.context.SparkContext*•
sparkSession€
,Union[pyspark.sql.session.SparkSession,None]D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession
None *9
jsqlContext&
Union[Any,None]
Any
None *˜
	_ssql_ctx(pyspark.sql.context.SQLContext._ssql_ctx"
Any*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`*Í
getOrCreate*pyspark.sql.context.SQLContext.getOrCreate"@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*y
clsp
$Type[pyspark.sql.context.SQLContext]@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext"type*D
sc<
pyspark.context.SparkContext"pyspark.context.SparkContext0:classmethodp*í
_get_or_create-pyspark.sql.context.SQLContext._get_or_create"@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*y
clsp
$Type[pyspark.sql.context.SQLContext]@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext"type*D
sc<
pyspark.context.SparkContext"pyspark.context.SparkContext*
static_conf
Any0:classmethodp*Å

newSession)pyspark.sql.context.SQLContext.newSession"@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*Ë
setConf&pyspark.sql.context.SQLContext.setConf"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*%
key
builtins.str"builtins.str*š
valueŽ
.Union[builtins.bool,builtins.int,builtins.str]
builtins.bool"builtins.bool
builtins.int"builtins.int
builtins.str"builtins.str*¤
getConf&pyspark.sql.context.SQLContext.getConf"D
Union[builtins.str,None]
builtins.str"builtins.str
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*%
key
builtins.str"builtins.str*·
defaultValue¢
6Union[builtins.str,None,pyspark._globals._NoValueType]
builtins.str"builtins.str
None>
pyspark._globals._NoValueType"pyspark._globals._NoValueType *Ç
udf"pyspark.sql.context.SQLContext.udf"B
pyspark.sql.udf.UDFRegistration"pyspark.sql.udf.UDFRegistration*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`*Í
udtf#pyspark.sql.context.SQLContext.udtf"F
!pyspark.sql.udtf.UDTFRegistration"!pyspark.sql.udtf.UDTFRegistration*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`*¼
range$pyspark.sql.context.SQLContext.range"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*'
start
builtins.int"builtins.int*O
endD
Union[builtins.int,None]
builtins.int"builtins.int
None *(
step
builtins.int"builtins.int *Y
numPartitionsD
Union[builtins.int,None]
builtins.int"builtins.int
None *ê
registerFunction/pyspark.sql.context.SQLContext.registerFunction"Z
+pyspark.sql._typing.UserDefinedFunctionLike"+pyspark.sql._typing.UserDefinedFunctionLike*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*&
name
builtins.str"builtins.str*R
fK
CallableType[builtins.function]&
builtins.function"builtins.function*€

returnTypen
&Union[pyspark.sql.types.DataType,None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
None *ý
registerJavaFunction3pyspark.sql.context.SQLContext.registerJavaFunction"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*&
name
builtins.str"builtins.str*/
javaClassName
builtins.str"builtins.str*€

returnTypen
&Union[pyspark.sql.types.DataType,None]8
pyspark.sql.types.DataType"pyspark.sql.types.DataType
None *á
_inferSchema+pyspark.sql.context.SQLContext._inferSchema"<
pyspark.sql.types.StructType"pyspark.sql.types.StructType*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*9
rdd0
pyspark.rdd.RDD[Any]
Any"pyspark.rdd.RDD*_
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None *¢
registerDataFrameAsTable7pyspark.sql.context.SQLContext.registerDataFrameAsTable"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*J
dfB
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*+
	tableName
builtins.str"builtins.str*À
dropTempTable,pyspark.sql.context.SQLContext.dropTempTable"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*+
	tableName
builtins.str"builtins.str*Ü
createExternalTable2pyspark.sql.context.SQLContext.createExternalTable"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*+
	tableName
builtins.str"builtins.str*P
pathD
Union[builtins.str,None]
builtins.str"builtins.str
None *R
sourceD
Union[builtins.str,None]
builtins.str"builtins.str
None *‚
schemat
(Union[pyspark.sql.types.StructType,None]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
None *)
options
builtins.str"builtins.str*å
sql"pyspark.sql.context.SQLContext.sql"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext**
sqlQuery
builtins.str"builtins.str*ê
table$pyspark.sql.context.SQLContext.table"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*+
	tableName
builtins.str"builtins.str*“
tables%pyspark.sql.context.SQLContext.tables"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*R
dbNameD
Union[builtins.str,None]
builtins.str"builtins.str
None *£

tableNames)pyspark.sql.context.SQLContext.tableNames"J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*R
dbNameD
Union[builtins.str,None]
builtins.str"builtins.str
None *¼

cacheTable)pyspark.sql.context.SQLContext.cacheTable"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*+
	tableName
builtins.str"builtins.str0*À
uncacheTable+pyspark.sql.context.SQLContext.uncacheTable"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*+
	tableName
builtins.str"builtins.str0*

clearCache)pyspark.sql.context.SQLContext.clearCache"
None*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0*×
read#pyspark.sql.context.SQLContext.read"P
&pyspark.sql.readwriter.DataFrameReader"&pyspark.sql.readwriter.DataFrameReader*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`*ù

readStream)pyspark.sql.context.SQLContext.readStream"f
1pyspark.sql.streaming.readwriter.DataStreamReader"1pyspark.sql.streaming.readwriter.DataStreamReader*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`*ó
streams&pyspark.sql.context.SQLContext.streams"f
1pyspark.sql.streaming.query.StreamingQueryManager"1pyspark.sql.streaming.query.StreamingQueryManager*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext0:property`2â
createDataFrame.pyspark.sql.context.SQLContext.createDataFrameÑ
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*Ÿ
data”
`Union[pyspark.rdd.RDD[pyspark.sql._typing.RowLike],typing.Iterable[pyspark.sql._typing.RowLike]]•
,pyspark.rdd.RDD[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD•
,typing.Iterable[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"typing.Iterable*ì
schemaÝ
?Union[builtins.list[builtins.str],builtins.tuple[builtins.str]]J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.listL
builtins.tuple[builtins.str]
builtins.str"builtins.str"builtins.tuple *_
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None 0:overloadXÕ
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*Ÿ
data”
`Union[pyspark.rdd.RDD[pyspark.sql._typing.RowLike],typing.Iterable[pyspark.sql._typing.RowLike]]•
,pyspark.rdd.RDD[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD•
,typing.Iterable[pyspark.sql._typing.RowLike]T
pyspark.sql._typing.RowLike"
builtins.object"builtins.object"builtins.object"typing.Iterable*
schema
0Union[pyspark.sql.types.StructType,builtins.str]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:overloadXí
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*·
data¬
hUnion[pyspark.rdd.RDD[pyspark.sql._typing.AtomicValue],typing.Iterable[pyspark.sql._typing.AtomicValue]]
0pyspark.rdd.RDD[pyspark.sql._typing.AtomicValue]X
pyspark.sql._typing.AtomicValue"
builtins.object"builtins.object"builtins.object"pyspark.rdd.RDD
0typing.Iterable[pyspark.sql._typing.AtomicValue]X
pyspark.sql._typing.AtomicValue"
builtins.object"builtins.object"builtins.object"typing.Iterable*
schema
0Union[pyspark.sql.types.AtomicType,builtins.str]<
pyspark.sql.types.AtomicType"pyspark.sql.types.AtomicType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:overloadX†
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*D
data:
pandas.core.frame.DataFrame"pandas.core.frame.DataFrame*_
samplingRatioJ
Union[builtins.float,None] 
builtins.float"builtins.float
None 0:overloadXù
createDataFrame.pyspark.sql.context.SQLContext.createDataFrame"B
pyspark.sql.dataframe.DataFrame"pyspark.sql.dataframe.DataFrame*J
self@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*D
data:
pandas.core.frame.DataFrame"pandas.core.frame.DataFrame*
schema
0Union[pyspark.sql.types.StructType,builtins.str]<
pyspark.sql.types.StructType"pyspark.sql.types.StructType
builtins.str"builtins.str*2
verifySchema
builtins.bool"builtins.bool 0:overloadXrÇ
_instantiatedContext3pyspark.sql.context.SQLContext._instantiatedContextz
*Union[pyspark.sql.context.SQLContext,None]@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext
Nonerg
_sc"pyspark.sql.context.SQLContext._sc<
pyspark.context.SparkContext"pyspark.context.SparkContextr4
_jsc#pyspark.sql.context.SQLContext._jsc
AnyrS
_jvm#pyspark.sql.context.SQLContext._jvm&
Union[Any,None]
Any
Noner
sparkSession+pyspark.sql.context.SQLContext.sparkSessionD
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSessionrD
_jsqlContext+pyspark.sql.context.SQLContext._jsqlContext
AnyØ
HiveContextpyspark.sql.context.HiveContext"pyspark.sql.context.SQLContext*°
__init__(pyspark.sql.context.HiveContext.__init__"
None*L
selfB
pyspark.sql.context.HiveContext"pyspark.sql.context.HiveContext*N
sparkContext<
pyspark.context.SparkContext"pyspark.context.SparkContext*•
sparkSession€
,Union[pyspark.sql.session.SparkSession,None]D
 pyspark.sql.session.SparkSession" pyspark.sql.session.SparkSession
None *:
jhiveContext&
Union[Any,None]
Any
None *î
_get_or_create.pyspark.sql.context.HiveContext._get_or_create"@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext*y
clsp
$Type[pyspark.sql.context.SQLContext]@
pyspark.sql.context.SQLContext"pyspark.sql.context.SQLContext"type*D
sc<
pyspark.context.SparkContext"pyspark.context.SparkContext*
static_conf
Any0:classmethodp*é
_createForTesting1pyspark.sql.context.HiveContext._createForTesting"B
pyspark.sql.context.HiveContext"pyspark.sql.context.HiveContext*|
clss
%Type[pyspark.sql.context.HiveContext]B
pyspark.sql.context.HiveContext"pyspark.sql.context.HiveContext"type*N
sparkContext<
pyspark.context.SparkContext"pyspark.context.SparkContext0:classmethodp*Á
refreshTable,pyspark.sql.context.HiveContext.refreshTable"
None*L
selfB
pyspark.sql.context.HiveContext"pyspark.sql.context.HiveContext*+
	tableName
builtins.str"builtins.strr³
_static_conf,pyspark.sql.context.HiveContext._static_confu
(builtins.dict[builtins.str,builtins.str]
builtins.str"builtins.str
builtins.str"builtins.str"builtins.dict,
_testpyspark.sql.context._test"
None*
__annotations__#pyspark.sql.context.__annotations__W
builtins.dict[builtins.str,Any]
builtins.str"builtins.str
Any"builtins.dict*5

JavaObjectpyspark.sql.context.JavaObject
Any*r
__all__pyspark.sql.context.__all__J
builtins.list[builtins.str]
builtins.str"builtins.str"builtins.list