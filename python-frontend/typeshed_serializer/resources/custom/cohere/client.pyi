from SonarPythonAnalyzerFakeStub import CustomStubBase
from typing import Any, Callable, Iterator, Literal, Optional, Union, Sequence
from concurrent.futures import ThreadPoolExecutor
import httpx


class Client(CustomStubBase):
    def __init__(self,
                 api_key: Optional[Union[str, Callable[[], str]]] = None,
                 *,
                 base_url: Optional[str] = None,
                 environment: ClientEnvironment = ...,
                 client_name: Optional[str] = None,
                 timeout: Optional[float] = None,
                 httpx_client: Optional[httpx.Client] = None,
                 thread_pool_executor: ThreadPoolExecutor = ...,
                 log_warning_experimental_features: bool = True,
                 ) -> None: ...

    def chat(
            self,
            *,
            message: str,
            accepts: Optional[Literal["text/event-stream"]] = None,
            model: Optional[str] = Any,
            preamble: Optional[str] = Any,
            chat_history: Optional[Sequence[Message]] = Any,
            conversation_id: Optional[str] = Any,
            prompt_truncation: Optional[ChatRequestPromptTruncation] = Any,
            connectors: Optional[Sequence[ChatConnector]] = Any,
            search_queries_only: Optional[bool] = Any,
            documents: Optional[Sequence[ChatDocument]] = Any,
            citation_quality: Optional[ChatRequestCitationQuality] = Any,
            temperature: Optional[float] = Any,
            max_tokens: Optional[int] = Any,
            max_input_tokens: Optional[int] = Any,
            k: Optional[int] = Any,
            p: Optional[float] = Any,
            seed: Optional[int] = Any,
            stop_sequences: Optional[Sequence[str]] = Any,
            frequency_penalty: Optional[float] = Any,
            presence_penalty: Optional[float] = Any,
            raw_prompting: Optional[bool] = Any,
            tools: Optional[Sequence[Tool]] = Any,
            tool_results: Optional[Sequence[ToolResult]] = Any,
            force_single_step: Optional[bool] = Any,
            response_format: Optional[ResponseFormat] = Any,
            safety_mode: Optional[Literal["CONTEXTUAL", "STRICT", "NONE"]] | Any = "CONTEXTUAL",
            request_options: Optional[RequestOptions] = None,
    ) -> NonStreamedChatResponse: ...

    def chat_stream(
            self,
            *,
            message: str,
            accepts: Optional[Literal["text/event-stream"]] = None,
            model: Optional[str] = Any,
            preamble: Optional[str] = Any,
            chat_history: Optional[Sequence[Message]] = Any,
            conversation_id: Optional[str] = Any,
            prompt_truncation: Optional[ChatStreamRequestPromptTruncation] = Any,
            connectors: Optional[Sequence[ChatConnector]] = Any,
            search_queries_only: Optional[bool] = Any,
            documents: Optional[Sequence[ChatDocument]] = Any,
            citation_quality: Optional[ChatStreamRequestCitationQuality] = Any,
            temperature: Optional[float] = Any,
            max_tokens: Optional[int] = Any,
            max_input_tokens: Optional[int] = Any,
            k: Optional[int] = Any,
            p: Optional[float] = Any,
            seed: Optional[int] = Any,
            stop_sequences: Optional[Sequence[str]] = Any,
            frequency_penalty: Optional[float] = Any,
            presence_penalty: Optional[float] = Any,
            raw_prompting: Optional[bool] = Any,
            tools: Optional[Sequence[Tool]] = Any,
            tool_results: Optional[Sequence[ToolResult]] = Any,
            force_single_step: Optional[bool] = Any,
            response_format: Optional[ResponseFormat] = Any,
            safety_mode: Optional[Literal["CONTEXTUAL", "STRICT", "NONE"]] = "CONTEXTUAL",
            request_options: Optional[RequestOptions] = None,
    ) -> Iterator[StreamedChatResponse]: ...
